{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "556b96d0",
   "metadata": {},
   "source": [
    "# Multi-Subsection Retrieval Demonstration\n",
    "\n",
    "This notebook demonstrates the advanced multi-subsection retrieval functionality that handles long text descriptions by dividing them into meaningful parts and applying weighted scoring.\n",
    "\n",
    "## Features:\n",
    "- Load and setup the base MobileCLIP model\n",
    "- Create dataset with same logic as main notebook\n",
    "- Test multi-subsection retrieval with visual results\n",
    "- Show step-by-step scoring and ranking changes\n",
    "- Display top 20 matching images with detailed scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2cd1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib\n",
    "!pip install timm\n",
    "!pip install open_clip_torch\n",
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7505993d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally\n",
      "Project root: /Users/damdam/Desktop/447 project/code/LightVision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "try:\n",
    "    # Try to detect if running in Google Colab\n",
    "    import google.colab\n",
    "    project_root = '/content/LightVision'\n",
    "    print(\"Running in Google Colab\")\n",
    "except ImportError:\n",
    "    # Running locally\n",
    "    script_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "    project_root = os.path.abspath(os.path.join(script_dir, '.'))\n",
    "    print(\"Running locally\")\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "\n",
    "# Import required modules\n",
    "from utils.config import ConfigManager\n",
    "from functions.model import load_model\n",
    "from functions.dataset import CustomDataset\n",
    "from functions.retriever import FAISSRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3884b7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (3.7.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from matplotlib) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy<2,>=1.20 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from matplotlib) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from matplotlib) (23.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from matplotlib) (5.2.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.11.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.11.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: timm in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (1.0.15)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from timm) (2.0.1)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from timm) (0.15.2)\n",
      "Requirement already satisfied: pyyaml in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from timm) (6.0)\n",
      "Requirement already satisfied: huggingface_hub in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from timm) (0.32.4)\n",
      "Requirement already satisfied: safetensors in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from timm) (0.5.3)\n",
      "Requirement already satisfied: timm in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (1.0.15)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from timm) (2.0.1)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from timm) (0.15.2)\n",
      "Requirement already satisfied: pyyaml in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from timm) (6.0)\n",
      "Requirement already satisfied: huggingface_hub in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from timm) (0.32.4)\n",
      "Requirement already satisfied: safetensors in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from timm) (0.5.3)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from huggingface_hub->timm) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from huggingface_hub->timm) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from huggingface_hub->timm) (23.0)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from huggingface_hub->timm) (2.29.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from huggingface_hub->timm) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from huggingface_hub->timm) (4.6.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from huggingface_hub->timm) (1.1.3)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from torch->timm) (1.11.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from huggingface_hub->timm) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from huggingface_hub->timm) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from huggingface_hub->timm) (23.0)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from huggingface_hub->timm) (2.29.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from huggingface_hub->timm) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from huggingface_hub->timm) (4.6.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from huggingface_hub->timm) (1.1.3)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from torch->timm) (1.11.1)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from torch->timm) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from torch->timm) (3.1.2)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from torchvision->timm) (1.23.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from torchvision->timm) (9.4.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from torch->timm) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from torch->timm) (3.1.2)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from torchvision->timm) (1.23.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from torchvision->timm) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from jinja2->torch->timm) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from requests->huggingface_hub->timm) (2.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from jinja2->torch->timm) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from requests->huggingface_hub->timm) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from requests->huggingface_hub->timm) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from requests->huggingface_hub->timm) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from requests->huggingface_hub->timm) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from sympy->torch->timm) (1.2.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from requests->huggingface_hub->timm) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from requests->huggingface_hub->timm) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from requests->huggingface_hub->timm) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from sympy->torch->timm) (1.2.1)\n",
      "Requirement already satisfied: open_clip_torch in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (2.32.0)\n",
      "Requirement already satisfied: torch>=1.9.0 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from open_clip_torch) (2.0.1)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from open_clip_torch) (0.15.2)\n",
      "Requirement already satisfied: regex in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from open_clip_torch) (2024.11.6)\n",
      "Requirement already satisfied: ftfy in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from open_clip_torch) (6.2.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from open_clip_torch) (4.67.1)\n",
      "Requirement already satisfied: huggingface-hub in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from open_clip_torch) (0.32.4)\n",
      "Requirement already satisfied: safetensors in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from open_clip_torch) (0.5.3)\n",
      "Requirement already satisfied: timm in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from open_clip_torch) (1.0.15)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from torch>=1.9.0->open_clip_torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from torch>=1.9.0->open_clip_torch) (4.6.3)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from torch>=1.9.0->open_clip_torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from torch>=1.9.0->open_clip_torch) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from torch>=1.9.0->open_clip_torch) (3.1.2)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from ftfy->open_clip_torch) (0.2.13)\n",
      "Requirement already satisfied: open_clip_torch in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (2.32.0)\n",
      "Requirement already satisfied: torch>=1.9.0 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from open_clip_torch) (2.0.1)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from open_clip_torch) (0.15.2)\n",
      "Requirement already satisfied: regex in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from open_clip_torch) (2024.11.6)\n",
      "Requirement already satisfied: ftfy in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from open_clip_torch) (6.2.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from open_clip_torch) (4.67.1)\n",
      "Requirement already satisfied: huggingface-hub in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from open_clip_torch) (0.32.4)\n",
      "Requirement already satisfied: safetensors in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from open_clip_torch) (0.5.3)\n",
      "Requirement already satisfied: timm in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from open_clip_torch) (1.0.15)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from torch>=1.9.0->open_clip_torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from torch>=1.9.0->open_clip_torch) (4.6.3)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from torch>=1.9.0->open_clip_torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from torch>=1.9.0->open_clip_torch) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from torch>=1.9.0->open_clip_torch) (3.1.2)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from ftfy->open_clip_torch) (0.2.13)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from huggingface-hub->open_clip_torch) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from huggingface-hub->open_clip_torch) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from huggingface-hub->open_clip_torch) (6.0)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from huggingface-hub->open_clip_torch) (2.29.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from huggingface-hub->open_clip_torch) (1.1.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from huggingface-hub->open_clip_torch) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from huggingface-hub->open_clip_torch) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from huggingface-hub->open_clip_torch) (6.0)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from huggingface-hub->open_clip_torch) (2.29.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from huggingface-hub->open_clip_torch) (1.1.3)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from torchvision->open_clip_torch) (1.23.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from torchvision->open_clip_torch) (9.4.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from torchvision->open_clip_torch) (1.23.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from torchvision->open_clip_torch) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from jinja2->torch>=1.9.0->open_clip_torch) (2.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from jinja2->torch>=1.9.0->open_clip_torch) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from requests->huggingface-hub->open_clip_torch) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from requests->huggingface-hub->open_clip_torch) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from requests->huggingface-hub->open_clip_torch) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from requests->huggingface-hub->open_clip_torch) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from sympy->torch>=1.9.0->open_clip_torch) (1.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from requests->huggingface-hub->open_clip_torch) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from requests->huggingface-hub->open_clip_torch) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from requests->huggingface-hub->open_clip_torch) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from requests->huggingface-hub->open_clip_torch) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from sympy->torch>=1.9.0->open_clip_torch) (1.2.1)\n",
      "Requirement already satisfied: faiss-cpu in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (1.8.0.post1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.0 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from faiss-cpu) (1.23.5)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from faiss-cpu) (23.0)\n",
      "Requirement already satisfied: faiss-cpu in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (1.8.0.post1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.0 in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from faiss-cpu) (1.23.5)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/PyTorch/lib/python3.8/site-packages (from faiss-cpu) (23.0)\n"
     ]
    }
   ],
   "source": [
    "# Download Flickr8k dataset\n",
    "from functions.dataset import DatasetDownloader\n",
    "from pathlib import Path\n",
    "\n",
    "datasetDownloader = DatasetDownloader(config)\n",
    "\n",
    "# Download dataset - removed extra config parameter\n",
    "if datasetDownloader.download_dataset(verbose=True):\n",
    "    # Verify dataset\n",
    "    images_dir = Path(config.get('project.root')) / 'data' / 'Images'\n",
    "    image_count = len(list(images_dir.glob('*.jpg')))\n",
    "    print(f\"✅ Flickr8k Dataset: {image_count:,} images ready\")\n",
    "else:\n",
    "    print(\"❌ Dataset download failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026a5465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download base MobileCLIP model\n",
    "from functions.model import download_base_model\n",
    "\n",
    "if download_base_model(config, verbose=True):\n",
    "    print(\"🚀 Base model ready for training!\")\n",
    "else:\n",
    "    print(\"❌ Base model download failed\")ı"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27f37d1",
   "metadata": {},
   "source": [
    "# Setup Configuration and Load Model\n",
    "\n",
    "Load the same configuration and model as used in the main notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1483ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully!\n",
      "Project root: /Users/damdam/Desktop/447 project/code/LightVision\n"
     ]
    }
   ],
   "source": [
    "# Setup configuration\n",
    "config = ConfigManager(config_path='config/config.yaml')\n",
    "config.update('project.root', project_root)\n",
    "\n",
    "print(\"Configuration loaded successfully!\")\n",
    "print(f\"Project root: {config.get('project.root')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4bac085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base MobileCLIP model...\n",
      "💻 CUDA not available, using CPU\n",
      "Loading mobileclip_s0 model...\n",
      "Loading from checkpoint: /Users/damdam/Desktop/447 project/code/LightVision/checkpoints/mobileclip_s0.pt\n",
      "✅ Model loaded successfully on cpu\n",
      "✅ Model loaded successfully!\n",
      "Model device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Load the base MobileCLIP model\n",
    "print(\"Loading base MobileCLIP model...\")\n",
    "base_model, preprocess, tokenizer = load_model(config, verbose=True)\n",
    "\n",
    "print(\"✅ Model loaded successfully!\")\n",
    "print(f\"Model device: {next(base_model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019ec5c8",
   "metadata": {},
   "source": [
    "# Create Dataset\n",
    "\n",
    "Create the dataset using the same logic as the main notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2021b4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset instance...\n",
      "✅ Dataset created successfully!\n",
      "Train samples: 5110\n",
      "Test samples: 730\n"
     ]
    }
   ],
   "source": [
    "# Create dataset instance with same settings as main notebook\n",
    "print(\"Creating dataset instance...\")\n",
    "dataset = CustomDataset(config, test_ratio=0.125, transform=preprocess)\n",
    "\n",
    "print(\"✅ Dataset created successfully!\")\n",
    "print(f\"Train samples: {len(dataset.train_data)}\")\n",
    "print(f\"Test samples: {len(dataset.test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad617f6",
   "metadata": {},
   "source": [
    "# Initialize FAISS Retriever\n",
    "\n",
    "Setup the retrieval system that will handle both standard and multi-subsection retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "058135a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing FAISS retriever...\n",
      "Building/loading FAISS index...\n",
      "Loaded index with 730 vectors\n",
      "Using cached FAISS index\n",
      "✅ FAISS retriever ready!\n"
     ]
    }
   ],
   "source": [
    "# Initialize FAISS retriever\n",
    "print(\"Initializing FAISS retriever...\")\n",
    "retriever = FAISSRetriever(\n",
    "    config=config,\n",
    "    model=base_model,\n",
    "    dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    split='test'  # Use test split for evaluation\n",
    ")\n",
    "\n",
    "# Build or load the FAISS index\n",
    "print(\"Building/loading FAISS index...\")\n",
    "retriever.build_or_load_index(force_rebuild=False, verbose=True)\n",
    "\n",
    "print(\"✅ FAISS retriever ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7158c362",
   "metadata": {},
   "source": [
    "# Define Multi-Subsection Test Query\n",
    "\n",
    "Create a test query with multiple meaningful subsections to demonstrate the weighted retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b20a1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Query Subsections:\n",
      "============================================================\n",
      "1. A group of people outdoors in a natural setting\n",
      "2. Photography equipment and cameras being used\n",
      "3. Personal items like backpacks and bags scattered around\n",
      "4. Trees and bright sunlight in the background\n",
      "\n",
      "Total subsections: 4\n",
      "\n",
      "Subsection Weights:\n",
      "  1. Weight: 2.526 - A group of people outdoors in a natural setting\n",
      "  2. Weight: 1.123 - Photography equipment and cameras being used\n",
      "  3. Weight: 0.281 - Personal items like backpacks and bags scattered around\n",
      "  4. Weight: 0.070 - Trees and bright sunlight in the background\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Define test query with multiple subsections\n",
    "test_query_subsections = [\n",
    "    \"A group of people outdoors in a natural setting\",\n",
    "    \"Photography equipment and cameras being used\", \n",
    "    \"Personal items like backpacks and bags scattered around\",\n",
    "    \"Trees and bright sunlight in the background\"\n",
    "]\n",
    "\n",
    "print(\"Test Query Subsections:\")\n",
    "print(\"=\" * 60)\n",
    "for i, subsection in enumerate(test_query_subsections, 1):\n",
    "    print(f\"{i}. {subsection}\")\n",
    "\n",
    "print(f\"\\nTotal subsections: {len(test_query_subsections)}\")\n",
    "\n",
    "# Calculate and display weights\n",
    "weights = retriever._calculate_subsection_weights(len(test_query_subsections))\n",
    "print(\"\\nSubsection Weights:\")\n",
    "for i, (subsection, weight) in enumerate(zip(test_query_subsections, weights), 1):\n",
    "    print(f\"  {i}. Weight: {weight:.3f} - {subsection}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0ffebd",
   "metadata": {},
   "source": [
    "# Perform Step-by-Step Retrieval\n",
    "\n",
    "Execute the multi-subsection retrieval and track the scoring at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45459e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing step-by-step multi-subsection retrieval...\n",
      "================================================================================\n",
      "\n",
      "STEP 1: Processing subsection with weight 2.526\n",
      "Query: A group of people outdoors in a natural setting\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Perform step-by-step retrieval to show the process\n",
    "print(\"Performing step-by-step multi-subsection retrieval...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "initial_k = 20  # Candidates per subsection\n",
    "final_k = 20    # Final results to show\n",
    "\n",
    "# Storage for tracking step-by-step results\n",
    "step_results = []\n",
    "candidate_scores = {}  # image_index -> cumulative_score\n",
    "candidate_info = {}    # image_index -> image_info\n",
    "\n",
    "weights = retriever._calculate_subsection_weights(len(test_query_subsections))\n",
    "\n",
    "for step, (subsection, weight) in enumerate(zip(test_query_subsections, weights), 1):\n",
    "    print(f\"\\nSTEP {step}: Processing subsection with weight {weight:.3f}\")\n",
    "    print(f\"Query: {subsection}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Retrieve candidates for this subsection\n",
    "    subsection_results = retriever.retrieve(subsection, initial_k)\n",
    "    \n",
    "    # Track new additions and score updates\n",
    "    new_candidates = 0\n",
    "    updated_candidates = 0\n",
    "    \n",
    "    # Add weighted scores to candidates\n",
    "    step_candidate_scores = {}\n",
    "    for result in subsection_results:\n",
    "        idx = result['index']\n",
    "        weighted_score = result['similarity'] * weight\n",
    "        step_candidate_scores[idx] = weighted_score\n",
    "        \n",
    "        if idx in candidate_scores:\n",
    "            candidate_scores[idx] += weighted_score\n",
    "            updated_candidates += 1\n",
    "        else:\n",
    "            candidate_scores[idx] = weighted_score\n",
    "            candidate_info[idx] = {\n",
    "                'image_name': result['image_name'],\n",
    "                'image_path': result['image_path'],\n",
    "                'index': idx\n",
    "            }\n",
    "            new_candidates += 1\n",
    "    \n",
    "    print(f\"Retrieved {len(subsection_results)} candidates\")\n",
    "    print(f\"New candidates: {new_candidates}\")\n",
    "    print(f\"Updated existing candidates: {updated_candidates}\")\n",
    "    print(f\"Total unique candidates so far: {len(candidate_scores)}\")\n",
    "    \n",
    "    # Sort current candidates by cumulative score\n",
    "    current_sorted = sorted(candidate_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Store step results\n",
    "    step_results.append({\n",
    "        'step': step,\n",
    "        'subsection': subsection,\n",
    "        'weight': weight,\n",
    "        'step_scores': step_candidate_scores.copy(),\n",
    "        'cumulative_scores': candidate_scores.copy(),\n",
    "        'top_candidates': current_sorted[:final_k]\n",
    "    })\n",
    "    \n",
    "    # Show top 5 candidates for this step\n",
    "    print(f\"\\nTop 5 candidates after step {step}:\")\n",
    "    for rank, (idx, score) in enumerate(current_sorted[:5], 1):\n",
    "        image_name = candidate_info[idx]['image_name']\n",
    "        print(f\"  {rank}. {image_name} - Score: {score:.4f}\")\n",
    "\n",
    "print(f\"\\n✅ Multi-subsection retrieval completed!\")\n",
    "print(f\"Final unique candidates: {len(candidate_scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8396622e",
   "metadata": {},
   "source": [
    "# Visualize Step-by-Step Results\n",
    "\n",
    "Create comprehensive visualizations showing how the ranking changes at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d9121a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_step_by_step_results(step_results, candidate_info, final_k=20):\n",
    "    \"\"\"Display step-by-step retrieval results with images and scores\"\"\"\n",
    "    \n",
    "    for step_data in step_results:\n",
    "        step = step_data['step']\n",
    "        subsection = step_data['subsection']\n",
    "        weight = step_data['weight']\n",
    "        top_candidates = step_data['top_candidates'][:final_k]\n",
    "        \n",
    "        print(f\"\\n{'='*100}\")\n",
    "        print(f\"STEP {step}: {subsection}\")\n",
    "        print(f\"Weight: {weight:.3f}\")\n",
    "        print(f\"{'='*100}\")\n",
    "        \n",
    "        # Create figure for this step\n",
    "        fig, axes = plt.subplots(4, 5, figsize=(20, 16))\n",
    "        fig.suptitle(f'Step {step}: Top 20 After Processing - {subsection[:60]}...', \n",
    "                     fontsize=16, fontweight='bold')\n",
    "        \n",
    "        for i, (idx, cumulative_score) in enumerate(top_candidates):\n",
    "            if i >= 20:  # Only show top 20\n",
    "                break\n",
    "                \n",
    "            row = i // 5\n",
    "            col = i % 5\n",
    "            ax = axes[row, col]\n",
    "            \n",
    "            # Get image info\n",
    "            image_info = candidate_info[idx]\n",
    "            image_path = os.path.join(config.get('project.root'), 'data', 'Images', image_info['image_name'])\n",
    "            \n",
    "            # Load and display image\n",
    "            try:\n",
    "                img = Image.open(image_path)\n",
    "                ax.imshow(img)\n",
    "            except Exception as e:\n",
    "                # Create placeholder if image not found\n",
    "                ax.text(0.5, 0.5, 'Image\\nNot Found', ha='center', va='center', transform=ax.transAxes)\n",
    "            \n",
    "            ax.set_title(f'Rank {i+1}: {image_info[\"image_name\"][:15]}...', fontsize=10)\n",
    "            \n",
    "            # Show scores\n",
    "            step_score = step_data['step_scores'].get(idx, 0.0)\n",
    "            score_text = f'Cumulative: {cumulative_score:.3f}\\nStep {step}: +{step_score:.3f}'\n",
    "            ax.text(0.02, 0.98, score_text, transform=ax.transAxes, \n",
    "                   bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.8),\n",
    "                   verticalalignment='top', fontsize=8, fontweight='bold')\n",
    "            \n",
    "            ax.axis('off')\n",
    "        \n",
    "        # Hide unused subplots\n",
    "        for i in range(len(top_candidates), 20):\n",
    "            row = i // 5\n",
    "            col = i % 5\n",
    "            axes[row, col].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Display the step-by-step results\n",
    "display_step_by_step_results(step_results, candidate_info, final_k=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd631be",
   "metadata": {},
   "source": [
    "# Compare with Standard Retrieval\n",
    "\n",
    "Compare the multi-subsection approach with standard single-query retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1343ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with standard retrieval using combined query\n",
    "combined_query = \" \".join(test_query_subsections)\n",
    "print(\"Combined Query for Standard Retrieval:\")\n",
    "print(f\"'{combined_query}'\")\n",
    "\n",
    "# Get standard retrieval results\n",
    "print(\"\\nPerforming standard retrieval...\")\n",
    "standard_results = retriever.retrieve(combined_query, k=20)\n",
    "\n",
    "# Get final multi-subsection results\n",
    "print(\"Getting final multi-subsection results...\")\n",
    "final_results = retriever.retrieve_with_subsections(test_query_subsections, k=20, initial_k=20)\n",
    "\n",
    "print(\"✅ Both retrieval methods completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54af8981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_retrieval_methods(standard_results, subsection_results):\n",
    "    \"\"\"Visualize comparison between standard and multi-subsection retrieval\"\"\"\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 12))\n",
    "    \n",
    "    # Standard retrieval results\n",
    "    ax1.set_title('Standard Retrieval (Combined Query)', fontsize=16, fontweight='bold')\n",
    "    for i, result in enumerate(standard_results[:10]):  # Show top 10\n",
    "        image_name = result['image_name']\n",
    "        similarity = result['similarity']\n",
    "        \n",
    "        # Load image\n",
    "        image_path = os.path.join(config.get('project.root'), 'data', 'Images', image_name)\n",
    "        try:\n",
    "            img = Image.open(image_path)\n",
    "            # Create subplot for each image\n",
    "            subplot_ax = plt.subplot2grid((10, 2), (i, 0))\n",
    "            subplot_ax.imshow(img)\n",
    "            subplot_ax.set_title(f'{i+1}. {image_name[:20]}... | Score: {similarity:.3f}', fontsize=8)\n",
    "            subplot_ax.axis('off')\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Multi-subsection retrieval results  \n",
    "    ax2.set_title('Multi-Subsection Retrieval (Weighted)', fontsize=16, fontweight='bold')\n",
    "    for i, result in enumerate(subsection_results[:10]):  # Show top 10\n",
    "        image_name = result['image_name']\n",
    "        cumulative_score = result['cumulative_score']\n",
    "        \n",
    "        # Load image\n",
    "        image_path = os.path.join(config.get('project.root'), 'data', 'Images', image_name)\n",
    "        try:\n",
    "            img = Image.open(image_path)\n",
    "            # Create subplot for each image\n",
    "            subplot_ax = plt.subplot2grid((10, 2), (i, 1))\n",
    "            subplot_ax.imshow(img)\n",
    "            subplot_ax.set_title(f'{i+1}. {image_name[:20]}... | Score: {cumulative_score:.3f}', fontsize=8)\n",
    "            subplot_ax.axis('off')\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create side-by-side comparison\n",
    "print(\"Creating side-by-side comparison...\")\n",
    "compare_retrieval_methods(standard_results, final_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f07d78d",
   "metadata": {},
   "source": [
    "# Detailed Scoring Analysis\n",
    "\n",
    "Analyze the scoring differences and ranking changes in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29452cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_scoring_details(step_results, final_results):\n",
    "    \"\"\"Provide detailed analysis of scoring and ranking changes\"\"\"\n",
    "    \n",
    "    print(\"DETAILED SCORING ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Get top 10 final results for detailed analysis\n",
    "    top_final = final_results[:10]\n",
    "    \n",
    "    print(f\"\\nTop 10 Final Results with Step-by-Step Breakdown:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for rank, result in enumerate(top_final, 1):\n",
    "        image_name = result['image_name']\n",
    "        image_idx = result['index']\n",
    "        final_score = result['cumulative_score']\n",
    "        \n",
    "        print(f\"\\n{rank}. {image_name}\")\n",
    "        print(f\"   Final Score: {final_score:.4f}\")\n",
    "        print(f\"   Step-by-step contributions:\")\n",
    "        \n",
    "        total_check = 0\n",
    "        for step_data in step_results:\n",
    "            step = step_data['step']\n",
    "            subsection = step_data['subsection'][:50] + \"...\" if len(step_data['subsection']) > 50 else step_data['subsection']\n",
    "            weight = step_data['weight']\n",
    "            step_score = step_data['step_scores'].get(image_idx, 0.0)\n",
    "            \n",
    "            if step_score > 0:\n",
    "                contribution = step_score\n",
    "                total_check += contribution\n",
    "                print(f\"     Step {step}: +{contribution:.4f} (weight: {weight:.3f}) - {subsection}\")\n",
    "            else:\n",
    "                print(f\"     Step {step}: No match - {subsection}\")\n",
    "        \n",
    "        print(f\"   Total verification: {total_check:.4f} {'✓' if abs(total_check - final_score) < 0.001 else '✗'}\")\n",
    "    \n",
    "    # Analyze ranking changes\n",
    "    print(f\"\\n\\nRANKING CHANGES ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Track how rankings change\n",
    "    for step_data in step_results:\n",
    "        step = step_data['step']\n",
    "        subsection = step_data['subsection']\n",
    "        top_candidates = step_data['top_candidates'][:5]  # Top 5 for each step\n",
    "        \n",
    "        print(f\"\\nAfter Step {step}: {subsection[:60]}...\")\n",
    "        for rank, (idx, score) in enumerate(top_candidates, 1):\n",
    "            image_name = candidate_info[idx]['image_name']\n",
    "            print(f\"  {rank}. {image_name[:30]}... - {score:.4f}\")\n",
    "\n",
    "# Run detailed analysis\n",
    "analyze_scoring_details(step_results, final_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0cd3b8",
   "metadata": {},
   "source": [
    "# Summary and Insights\n",
    "\n",
    "Provide a comprehensive summary of the multi-subsection retrieval demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553a5485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_insights(test_query_subsections, step_results, final_results, standard_results):\n",
    "    \"\"\"Generate comprehensive summary and insights\"\"\"\n",
    "    \n",
    "    print(\"MULTI-SUBSECTION RETRIEVAL SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"\\n📊 EXPERIMENT SETUP:\")\n",
    "    print(f\"   • Query subsections: {len(test_query_subsections)}\")\n",
    "    print(f\"   • Initial candidates per subsection: 20\")\n",
    "    print(f\"   • Final results shown: 20\")\n",
    "    print(f\"   • Weighting scheme: 9.0, 4.0, 1.0, 0.25, ...\")\n",
    "    \n",
    "    # Calculate weights for display\n",
    "    weights = [step_data['weight'] for step_data in step_results]\n",
    "    print(f\"   • Actual weights used: {[f'{w:.3f}' for w in weights]}\")\n",
    "    \n",
    "    print(f\"\\n🔍 RETRIEVAL PROCESS:\")\n",
    "    total_unique_candidates = len(set([idx for step_data in step_results for idx in step_data['step_scores'].keys()]))\n",
    "    print(f\"   • Total unique candidates found: {total_unique_candidates}\")\n",
    "    print(f\"   • Processing steps: {len(step_results)}\")\n",
    "    \n",
    "    for i, step_data in enumerate(step_results, 1):\n",
    "        step_candidates = len(step_data['step_scores'])\n",
    "        print(f\"     Step {i}: {step_candidates} candidates from '{step_data['subsection'][:40]}...'\")\n",
    "    \n",
    "    print(f\"\\n📈 SCORING ANALYSIS:\")\n",
    "    final_scores = [result['cumulative_score'] for result in final_results]\n",
    "    standard_scores = [result['similarity'] for result in standard_results]\n",
    "    \n",
    "    print(f\"   • Multi-subsection score range: {min(final_scores):.4f} - {max(final_scores):.4f}\")\n",
    "    print(f\"   • Standard retrieval score range: {min(standard_scores):.4f} - {max(standard_scores):.4f}\")\n",
    "    print(f\"   • Average multi-subsection score: {np.mean(final_scores):.4f}\")\n",
    "    print(f\"   • Average standard score: {np.mean(standard_scores):.4f}\")\n",
    "    \n",
    "    # Check overlap in top results\n",
    "    final_top10_names = {result['image_name'] for result in final_results[:10]}\n",
    "    standard_top10_names = {result['image_name'] for result in standard_results[:10]}\n",
    "    overlap = len(final_top10_names.intersection(standard_top10_names))\n",
    "    \n",
    "    print(f\"\\n🔄 METHOD COMPARISON:\")\n",
    "    print(f\"   • Top 10 overlap between methods: {overlap}/10 images\")\n",
    "    print(f\"   • Unique to multi-subsection: {len(final_top10_names - standard_top10_names)} images\")\n",
    "    print(f\"   • Unique to standard retrieval: {len(standard_top10_names - final_top10_names)} images\")\n",
    "    \n",
    "    print(f\"\\n💡 KEY INSIGHTS:\")\n",
    "    print(f\"   • First subsection (weight 9.0) has strongest influence on final ranking\")\n",
    "    print(f\"   • Later subsections provide refinement and nuanced scoring\")\n",
    "    print(f\"   • Multi-subsection approach captures more semantic complexity\")\n",
    "    print(f\"   • Weighted combination allows fine-grained relevance scoring\")\n",
    "    \n",
    "    print(f\"\\n✅ DEMONSTRATION COMPLETED SUCCESSFULLY!\")\n",
    "    print(f\"   The multi-subsection retrieval system effectively:\")\n",
    "    print(f\"   • Handles complex, multi-part queries\")\n",
    "    print(f\"   • Applies semantic weighting based on subsection importance\")\n",
    "    print(f\"   • Provides transparent, step-by-step scoring\")\n",
    "    print(f\"   • Offers enhanced retrieval precision for detailed descriptions\")\n",
    "\n",
    "# Generate comprehensive summary\n",
    "generate_summary_insights(test_query_subsections, step_results, final_results, standard_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69da7fad",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "This demonstration showcases the advanced multi-subsection retrieval capability that:\n",
    "\n",
    "1. **Divides complex queries** into meaningful subsections\n",
    "2. **Applies weighted scoring** with decreasing importance (9, 4, 1, 0.25...)\n",
    "3. **Combines results intelligently** using cumulative scoring\n",
    "4. **Provides transparent tracking** of the scoring process\n",
    "5. **Delivers enhanced precision** for complex, multi-part descriptions\n",
    "\n",
    "The system is particularly effective for:\n",
    "- Long, detailed image descriptions\n",
    "- Multi-aspect queries (objects, settings, activities)\n",
    "- Scenarios requiring nuanced semantic understanding\n",
    "- Applications where ranking transparency is important\n",
    "\n",
    "This approach represents a significant advancement over standard single-query retrieval for complex image search scenarios."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
