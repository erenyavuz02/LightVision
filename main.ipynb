{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89daaaba",
   "metadata": {},
   "source": [
    "# Get Pretrained Models\n",
    "Okay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5dae603b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-04-30 18:31:27--  https://docs-assets.developer.apple.com/ml-research/datasets/mobileclip/mobileclip_s0.pt\n",
      "Resolving docs-assets.developer.apple.com (docs-assets.developer.apple.com)... 17.253.73.202, 17.253.73.201\n",
      "Connecting to docs-assets.developer.apple.com (docs-assets.developer.apple.com)|17.253.73.202|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 215934653 (206M) [application/octet-stream]\n",
      "Saving to: ‘checkpoints/mobileclip_s0.pt’\n",
      "\n",
      "mobileclip_s0.pt    100%[===================>] 205,93M  5,49MB/s    in 39s     \n",
      "\n",
      "2025-04-30 18:32:07 (5,34 MB/s) - ‘checkpoints/mobileclip_s0.pt’ saved [215934653/215934653]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p checkpoints/\n",
    "\n",
    "!wget https://docs-assets.developer.apple.com/ml-research/datasets/mobileclip/mobileclip_s0.pt -P checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42eac2b",
   "metadata": {},
   "source": [
    "# Create the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60043773",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install torchvision\n",
    "!pip install timm\n",
    "!pip install open-clip-torch\n",
    "!pip install datasets\n",
    "!pip install clip-benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c56856b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/erenyavuz/Desktop/KU/25 Spring/COMP447/Project/Repo/FlightVision/pngwing.com(1).png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m model, _, preprocess \u001b[38;5;241m=\u001b[39m mobileclip\u001b[38;5;241m.\u001b[39mcreate_model_and_transforms(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmobileclip_s0\u001b[39m\u001b[38;5;124m'\u001b[39m, pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/erenyavuz/Desktop/KU/25 Spring/COMP447/Project/Repo/FlightVision/checkpoints/mobileclip_s0.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m mobileclip\u001b[38;5;241m.\u001b[39mget_tokenizer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmobileclip_s0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m image \u001b[38;5;241m=\u001b[39m preprocess(Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpngwing.com(1).png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      9\u001b[0m text \u001b[38;5;241m=\u001b[39m tokenizer([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma purple dog\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma white dog\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma black dog\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(), torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast():\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/PIL/Image.py:3431\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3428\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(os\u001b[38;5;241m.\u001b[39mfspath(fp))\n\u001b[1;32m   3430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3431\u001b[0m     fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3432\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/erenyavuz/Desktop/KU/25 Spring/COMP447/Project/Repo/FlightVision/pngwing.com(1).png'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import mobileclip\n",
    "\n",
    "model, _, preprocess = mobileclip.create_model_and_transforms('mobileclip_s0', pretrained='/Users/erenyavuz/Desktop/KU/25 Spring/COMP447/Project/Repo/FlightVision/checkpoints/mobileclip_s0.pt')\n",
    "tokenizer = mobileclip.get_tokenizer('mobileclip_s0')\n",
    "\n",
    "image = preprocess(Image.open(\"pngwing.com(1).png\").convert('RGB')).unsqueeze(0)\n",
    "text = tokenizer([\"a purple dog\", \"a white dog\", \"a black dog\"])\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "# Set the print options for PyTorch to avoid scientific notation and limit decimal places\n",
    "torch.set_printoptions(sci_mode=False, precision=4)\n",
    "\n",
    "print(\"Label probs:\", text_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97b92180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearnablePositionalEmbedding(num_embeddings=77, embedding_dim=512, padding_idx=None)\n",
      "Modified Positional Embedding: Parameter containing:\n",
      "tensor([[[[     0.0000,      0.0000,      0.0000,  ...,      0.0000,\n",
      "                0.0000,      0.0000],\n",
      "          [     0.0041,      0.0016,     -0.0007,  ...,      0.0007,\n",
      "                0.0036,     -0.0076],\n",
      "          [     0.0077,      0.0026,      0.0012,  ...,      0.0001,\n",
      "                0.0013,     -0.0043],\n",
      "          ...,\n",
      "          [     0.0075,      0.0051,      0.0000,  ...,      0.0011,\n",
      "                0.0001,     -0.0003],\n",
      "          [     0.0031,      0.0068,     -0.0008,  ...,      0.0024,\n",
      "               -0.0002,     -0.0025],\n",
      "          [    -0.0013,      0.0086,     -0.0016,  ...,      0.0036,\n",
      "               -0.0004,     -0.0046]]]])\n"
     ]
    }
   ],
   "source": [
    "print(model.get_positional_embedding() )\n",
    "\n",
    "def get_positional_embedding(self, lambda2: int = 4):\n",
    "    \"\"\"\n",
    "    Get modified positional embedding for text encoder based on the given formula.\n",
    "    \"\"\"\n",
    "    pos_embed = self.text_encoder.get_positional_embedding().pos_embed.pos_embed\n",
    "    if pos_embed is None:\n",
    "        raise ValueError(\"Positional embedding not found in text encoder.\")\n",
    "\n",
    "    max_pos, embed_dim = pos_embed.shape[2], pos_embed.shape[3]\n",
    "    modified_pos_embed = torch.zeros((1, 1, max_pos, embed_dim), device=pos_embed.device)\n",
    "\n",
    "    for pos in range(max_pos):\n",
    "        if pos <= 20:\n",
    "            modified_pos_embed[:, :, pos, :] = pos_embed[:, :, pos, :]\n",
    "        else:\n",
    "            lower_idx = pos // lambda2\n",
    "            upper_idx = min(lower_idx + 1, max_pos - 1)  # Ensure upper_idx is within bounds\n",
    "            alpha = (pos % lambda2) / lambda2\n",
    "            modified_pos_embed[:, :, pos, :] = (1 - alpha) * pos_embed[:, :, lower_idx, :] + alpha * pos_embed[:, :, upper_idx, :]\n",
    "    # turn the torch tensor into nn parameter\n",
    "    modified_pos_embed = torch.nn.Parameter(modified_pos_embed, requires_grad=False)\n",
    "    return modified_pos_embed\n",
    "\n",
    "# Example usage\n",
    "lambda2 = 4\n",
    "new_pos_embed = get_positional_embedding(model, lambda2)\n",
    "print(\"Modified Positional Embedding:\", new_pos_embed)\n",
    "\n",
    "# set the models pos embedding to the new one\n",
    "model.text_encoder.get_positional_embedding().pos_embed.pos_embed = new_pos_embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1d0bbec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label probs: tensor([[0.7638, 0.0161, 0.2201]])\n"
     ]
    }
   ],
   "source": [
    "image = preprocess(Image.open(\"pngwing.com(1).png\").convert('RGB')).unsqueeze(0)\n",
    "text = tokenizer([\"a purple dog\", \"a white dog\", \"a black dog\"])\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "# Set the print options for PyTorch to avoid scientific notation and limit decimal places\n",
    "torch.set_printoptions(sci_mode=False, precision=4)\n",
    "\n",
    "print(\"Label probs:\", text_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284b0522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the coco dataset\n",
    "# Download a smaller dataset, such as CIFAR-10\n",
    "!mkdir -p cifar10\n",
    "!wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz -P cifar10/\n",
    "!tar -xvzf cifar10/cifar-10-python.tar.gz -C cifar10/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7353ab8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  train the model using contrastive loss\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from torchvision import datasets\n",
    "from custom_dataset import CustomDataset\n",
    "\n",
    "# Define the transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "cifar10_dataset = datasets.CIFAR10(root='cifar10', train=True, download=True)\n",
    "# Define a custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, caption = self.dataset[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, caption\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "train_dataset = CustomDataset(cifar10_dataset, transform=transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=1)\n",
    "# Define the contrastive loss function\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, image_features, text_features):\n",
    "        # Compute the cosine similarity\n",
    "        logits = torch.matmul(image_features, text_features.T) / self.temperature\n",
    "        labels = torch.arange(logits.size(0)).to(logits.device)\n",
    "        loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "        return loss\n",
    "# Initialize the model, optimizer, and loss function\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = ContrastiveLoss(temperature=0.07)\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, captions in tqdm(train_dataloader):\n",
    "        \n",
    "        images = images.to(device)\n",
    "        captions = tokenizer(captions).to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            image_features = model.encode_image(images)\n",
    "            text_features = model.encode_text(captions)\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            loss = criterion(image_features, text_features)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_dataloader):.4f}\")\n",
    "    # Save the model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dc3e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "train_dataset = CustomDataset(cifar10_dataset, transform=preprocess)\n",
    "dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "\n",
    "# Replace `MobileCLIPModel()` with your Mobile CLIP implementation.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# 3. Contrastive Loss (InfoNCE)\n",
    "def contrastive_loss(image_embeds: torch.Tensor, text_embeds: torch.Tensor, temperature: float = 0.07) -> torch.Tensor:\n",
    "    # Normalize embeddings\n",
    "    image_embeds = F.normalize(image_embeds, dim=1)\n",
    "    text_embeds = F.normalize(text_embeds, dim=1)\n",
    "    # Compute logits\n",
    "    logits = image_embeds @ text_embeds.t() / temperature\n",
    "    labels = torch.arange(logits.size(0), device=logits.device)\n",
    "    loss_i2t = F.cross_entropy(logits, labels)\n",
    "    loss_t2i = F.cross_entropy(logits.t(), labels)\n",
    "    return (loss_i2t + loss_t2i) / 2\n",
    "\n",
    "# 4. Optimizer and training setup\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "epochs = 10\n",
    "\n",
    "# 5. Training loop\n",
    "def train():\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        for images, captions in tqdm(dataloader):\n",
    "            images = images.to(device)\n",
    "            captions = tokenizer(captions).to(device)\n",
    "            # Forward pass: user-provided model should return (image_embeds, text_embeds)\n",
    "            image_embeds = model.encode_image(images)\n",
    "            text_embeds = model.encode_text(captions)\n",
    "\n",
    "            # Compute contrastive loss\n",
    "            loss = contrastive_loss(image_embeds, text_embeds)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader.dataset)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e95ec6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30951f86870347c582b2fdb7552f3ad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62a1bf7037bd46cd916a9c19f9e61b8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/506 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6174cabc29ba4280b83640f055a1c0d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b23f5a312804b8ea249765e5c0557da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34e3b1d210a14ef0924e55e9564dfca0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "790122ce7a9a44f2ac9b6a6cf7b7f1bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.56k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a844e2ad098742c689f2a17bac2c8fa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b40ef3792b945aeb43c1ffdd4cfc5f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "The image to be converted to a PIL image contains values outside the range [0, 1], got [-2.1179039478302, 2.393557548522949] which cannot be converted to uint8.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m captions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images[:\u001b[38;5;241m100\u001b[39m]:  \u001b[38;5;66;03m# Increase this number as needed\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m processor(images\u001b[38;5;241m=\u001b[39mimage, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     31\u001b[0m         out \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/blip/processing_blip.py:110\u001b[0m, in \u001b[0;36mBlipProcessor.__call__\u001b[0;34m(self, images, text, audio, videos, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m     text_encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(text, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moutput_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 110\u001b[0m     encoding_image_processor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_processor(images, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moutput_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m text_encoding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m         encoding_image_processor\u001b[38;5;241m.\u001b[39mupdate(text_encoding)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/image_processing_utils.py:42\u001b[0m, in \u001b[0;36mBaseImageProcessor.__call__\u001b[0;34m(self, images, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchFeature:\n\u001b[1;32m     41\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(images, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/generic.py:866\u001b[0m, in \u001b[0;36mfilter_out_non_signature_kwargs.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         cls_prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    859\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    860\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following named arguments are not valid for `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcls_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    861\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and were ignored: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minvalid_kwargs_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m    863\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    864\u001b[0m     )\n\u001b[0;32m--> 866\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mvalid_kwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/blip/image_processing_blip.py:271\u001b[0m, in \u001b[0;36mBlipImageProcessor.preprocess\u001b[0;34m(self, images, do_resize, size, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, return_tensors, do_convert_rgb, data_format, input_data_format)\u001b[0m\n\u001b[1;32m    267\u001b[0m     input_data_format \u001b[38;5;241m=\u001b[39m infer_channel_dimension_format(images[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_resize:\n\u001b[1;32m    270\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 271\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresize(image\u001b[38;5;241m=\u001b[39mimage, size\u001b[38;5;241m=\u001b[39msize, resample\u001b[38;5;241m=\u001b[39mresample, input_data_format\u001b[38;5;241m=\u001b[39minput_data_format)\n\u001b[1;32m    272\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images\n\u001b[1;32m    273\u001b[0m     ]\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_rescale:\n\u001b[1;32m    276\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrescale(image\u001b[38;5;241m=\u001b[39mimage, scale\u001b[38;5;241m=\u001b[39mrescale_factor, input_data_format\u001b[38;5;241m=\u001b[39minput_data_format)\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images\n\u001b[1;32m    279\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/blip/image_processing_blip.py:150\u001b[0m, in \u001b[0;36mBlipImageProcessor.resize\u001b[0;34m(self, image, size, resample, data_format, input_data_format, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `size` dictionary must contain the keys `height` and `width`. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    149\u001b[0m output_size \u001b[38;5;241m=\u001b[39m (size[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheight\u001b[39m\u001b[38;5;124m\"\u001b[39m], size[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resize(\n\u001b[1;32m    151\u001b[0m     image,\n\u001b[1;32m    152\u001b[0m     size\u001b[38;5;241m=\u001b[39moutput_size,\n\u001b[1;32m    153\u001b[0m     resample\u001b[38;5;241m=\u001b[39mresample,\n\u001b[1;32m    154\u001b[0m     data_format\u001b[38;5;241m=\u001b[39mdata_format,\n\u001b[1;32m    155\u001b[0m     input_data_format\u001b[38;5;241m=\u001b[39minput_data_format,\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    157\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/image_transforms.py:368\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(image, size, resample, reducing_gap, data_format, return_numpy, input_data_format)\u001b[0m\n\u001b[1;32m    366\u001b[0m do_rescale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(image, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n\u001b[0;32m--> 368\u001b[0m     do_rescale \u001b[38;5;241m=\u001b[39m _rescale_for_pil_conversion(image)\n\u001b[1;32m    369\u001b[0m     image \u001b[38;5;241m=\u001b[39m to_pil_image(image, do_rescale\u001b[38;5;241m=\u001b[39mdo_rescale, input_data_format\u001b[38;5;241m=\u001b[39minput_data_format)\n\u001b[1;32m    370\u001b[0m height, width \u001b[38;5;241m=\u001b[39m size\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/image_transforms.py:150\u001b[0m, in \u001b[0;36m_rescale_for_pil_conversion\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m    148\u001b[0m     do_rescale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe image to be converted to a PIL image contains values outside the range [0, 1], \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] which cannot be converted to uint8.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    153\u001b[0m     )\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m do_rescale\n",
      "\u001b[0;31mValueError\u001b[0m: The image to be converted to a PIL image contains values outside the range [0, 1], got [-2.1179039478302, 2.393557548522949] which cannot be converted to uint8."
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Define the transformation pipeline\n",
    "# Remove transforms.ToPILImage() since CIFAR-10 images are already PIL images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to match model input size\n",
    "    transforms.ToTensor(),         # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize\n",
    "])\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "cifar10_dataset = datasets.CIFAR10(root='cifar10', train=True, download=True)\n",
    "\n",
    "# Apply the transformation pipeline to the images\n",
    "images = [transform(img) for img, _ in cifar10_dataset]\n",
    "\n",
    "# Load BLIP model and processor\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model.eval()\n",
    "\n",
    "# Generate captions for a subset (e.g., first 100 images for demo)\n",
    "captions = []\n",
    "for image in images[:100]:  # Increase this number as needed\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**inputs)\n",
    "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "    captions.append(caption)\n",
    "\n",
    "# Custom dataset combining image and generated caption\n",
    "class CIFAR10WithCaptions(Dataset):\n",
    "    def __init__(self, images, captions, transform=None):\n",
    "        self.images = images\n",
    "        self.captions = captions\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        caption = self.captions[idx]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, caption\n",
    "\n",
    "# Example usage\n",
    "transform_tensor = transforms.ToTensor()\n",
    "custom_dataset = CIFAR10WithCaptions(images[:100], captions, transform=transform_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ddd0b8a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CIFAR10WithCaptions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m      2\u001b[0m transform_tensor \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mToTensor()\n\u001b[0;32m----> 3\u001b[0m custom_dataset \u001b[38;5;241m=\u001b[39m CIFAR10WithCaptions(images[:\u001b[38;5;241m100\u001b[39m], captions, transform\u001b[38;5;241m=\u001b[39mtransform_tensor)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CIFAR10WithCaptions' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "transform_tensor = transforms.ToTensor()\n",
    "custom_dataset = CIFAR10WithCaptions(images[:100], captions, transform=transform_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b411e312",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'custom_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Get the first image and caption\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m img, caption \u001b[38;5;241m=\u001b[39m custom_dataset[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Convert the tensor image to a PIL image for plotting\u001b[39;00m\n\u001b[1;32m      8\u001b[0m img \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mto_pil_image(img)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'custom_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "# Get the first image and caption\n",
    "img, caption = custom_dataset[0]\n",
    "\n",
    "# Convert the tensor image to a PIL image for plotting\n",
    "img = F.to_pil_image(img)\n",
    "\n",
    "# Plot the image\n",
    "plt.imshow(img)\n",
    "plt.title(f\"Caption: {caption}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5183c22d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
