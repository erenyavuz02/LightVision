{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89daaaba",
   "metadata": {},
   "source": [
    "# Get Pretrained Models\n",
    "Okay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5dae603b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-04-30 18:31:27--  https://docs-assets.developer.apple.com/ml-research/datasets/mobileclip/mobileclip_s0.pt\n",
      "Resolving docs-assets.developer.apple.com (docs-assets.developer.apple.com)... 17.253.73.202, 17.253.73.201\n",
      "Connecting to docs-assets.developer.apple.com (docs-assets.developer.apple.com)|17.253.73.202|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 215934653 (206M) [application/octet-stream]\n",
      "Saving to: ‘checkpoints/mobileclip_s0.pt’\n",
      "\n",
      "mobileclip_s0.pt    100%[===================>] 205,93M  5,49MB/s    in 39s     \n",
      "\n",
      "2025-04-30 18:32:07 (5,34 MB/s) - ‘checkpoints/mobileclip_s0.pt’ saved [215934653/215934653]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p checkpoints/\n",
    "\n",
    "!wget https://docs-assets.developer.apple.com/ml-research/datasets/mobileclip/mobileclip_s0.pt -P checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42eac2b",
   "metadata": {},
   "source": [
    "# dowload libraries if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60043773",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install torchvision\n",
    "!pip install timm\n",
    "!pip install open-clip-torch\n",
    "!pip install datasets\n",
    "!pip install clip-benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ffff17",
   "metadata": {},
   "source": [
    "# Libaries, Parameters and Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c56856b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label probs: tensor([[    0.9790,     0.0004,     0.0205]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import mobileclip\n",
    "import random\n",
    "\n",
    "# --- Configuration ---\n",
    "# Using relative paths for better portability\n",
    "BASE_DATA_DESTINATION = os.path.join(os.getcwd(), \"data\")\n",
    "FLICKR8K_IMAGES_FOLDER_NAME = \"Images\"\n",
    "CAPTIONS_JSON_FILENAME = \"all_captions.json\"\n",
    "CHECKPOINT_DIR = os.path.join(os.getcwd(), \"checkpoints\")\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 1\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the MobileCLIP model\n",
    "model_path = os.path.join(CHECKPOINT_DIR, 'mobileclip_s0.pt')\n",
    "model, _, preprocess = mobileclip.create_model_and_transforms(\n",
    "    'mobileclip_s0', \n",
    "    pretrained=model_path\n",
    ")\n",
    "model.to(DEVICE)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = mobileclip.get_tokenizer('mobileclip_s0')\n",
    "\n",
    "\n",
    "image = preprocess(Image.open(\"/Users/erenyavuz/Desktop/KU/25 Spring/COMP447/Project/Repo/FlightVision/pngwing.com.png\").convert('RGB')).unsqueeze(0)\n",
    "text = tokenizer([\"a brown dog\", \"a white dog\", \"a purple dog\"])\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "# Set the print options for PyTorch to avoid scientific notation and limit decimal places\n",
    "torch.set_printoptions(sci_mode=False, precision=4)\n",
    "\n",
    "print(\"Label probs:\", text_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86efc59",
   "metadata": {},
   "source": [
    "# Changing the Positional Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97b92180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearnablePositionalEmbedding(num_embeddings=77, embedding_dim=512, padding_idx=None)\n",
      "Modified Positional Embedding: Parameter containing:\n",
      "tensor([[[[     0.0000,      0.0000,      0.0000,  ...,      0.0000,\n",
      "                0.0000,      0.0000],\n",
      "          [     0.0041,      0.0016,     -0.0007,  ...,      0.0007,\n",
      "                0.0036,     -0.0076],\n",
      "          [     0.0077,      0.0026,      0.0012,  ...,      0.0001,\n",
      "                0.0013,     -0.0043],\n",
      "          ...,\n",
      "          [     0.0075,      0.0051,      0.0000,  ...,      0.0011,\n",
      "                0.0001,     -0.0003],\n",
      "          [     0.0031,      0.0068,     -0.0008,  ...,      0.0024,\n",
      "               -0.0002,     -0.0025],\n",
      "          [    -0.0013,      0.0086,     -0.0016,  ...,      0.0036,\n",
      "               -0.0004,     -0.0046]]]])\n"
     ]
    }
   ],
   "source": [
    "print(model.get_positional_embedding() )\n",
    "\n",
    "def get_positional_embedding(self, lambda2: int = 4):\n",
    "    \"\"\"\n",
    "    Get modified positional embedding for text encoder based on the given formula.\n",
    "    \"\"\"\n",
    "    pos_embed = self.text_encoder.get_positional_embedding().pos_embed.pos_embed\n",
    "    if pos_embed is None:\n",
    "        raise ValueError(\"Positional embedding not found in text encoder.\")\n",
    "\n",
    "    max_pos, embed_dim = pos_embed.shape[2], pos_embed.shape[3]\n",
    "    modified_pos_embed = torch.zeros((1, 1, max_pos, embed_dim), device=pos_embed.device)\n",
    "\n",
    "    for pos in range(max_pos):\n",
    "        if pos <= 20:\n",
    "            modified_pos_embed[:, :, pos, :] = pos_embed[:, :, pos, :]\n",
    "        else:\n",
    "            lower_idx = pos // lambda2\n",
    "            upper_idx = min(lower_idx + 1, max_pos - 1)  # Ensure upper_idx is within bounds\n",
    "            alpha = (pos % lambda2) / lambda2\n",
    "            modified_pos_embed[:, :, pos, :] = (1 - alpha) * pos_embed[:, :, lower_idx, :] + alpha * pos_embed[:, :, upper_idx, :]\n",
    "    # turn the torch tensor into nn parameter\n",
    "    modified_pos_embed = torch.nn.Parameter(modified_pos_embed, requires_grad=False)\n",
    "    return modified_pos_embed\n",
    "\n",
    "# Example usage\n",
    "lambda2 = 4\n",
    "new_pos_embed = get_positional_embedding(model, lambda2)\n",
    "print(\"Modified Positional Embedding:\", new_pos_embed)\n",
    "\n",
    "# set the models pos embedding to the new one\n",
    "model.text_encoder.get_positional_embedding().pos_embed.pos_embed = new_pos_embed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7444910a",
   "metadata": {},
   "source": [
    "# Testing the model after changing the positional embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1d0bbec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label probs: tensor([[0.4086, 0.3122, 0.2792]])\n"
     ]
    }
   ],
   "source": [
    "image = preprocess(Image.open(\"/Users/erenyavuz/Desktop/KU/25 Spring/COMP447/Project/Repo/FlightVision/pngwing.com.png\").convert('RGB')).unsqueeze(0)\n",
    "text = tokenizer([\"a brown dog\", \"a white dog\", \"a purple dog\"])\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "# Set the print options for PyTorch to avoid scientific notation and limit decimal places\n",
    "torch.set_printoptions(sci_mode=False, precision=4)\n",
    "\n",
    "print(\"Label probs:\", text_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03590a4c",
   "metadata": {},
   "source": [
    "# Downloading the captioned images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "284b0522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up data directories in: /Users/erenyavuz/Desktop/KU/25 Spring/COMP447/Project/Repo/FlightVision/data\n",
      "Images already exist at /Users/erenyavuz/Desktop/KU/25 Spring/COMP447/Project/Repo/FlightVision/data/Images.\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "import os\n",
    "\n",
    "# Using relative paths for better portability\n",
    "# This creates a 'data' directory in the project folder\n",
    "BASE_DATA_DESTINATION = os.path.join(os.getcwd(), \"data\")\n",
    "KAGGLE_FLICKR8K_URL = \"https://www.kaggle.com/api/v1/datasets/download/adityajn105/flickr8k\"\n",
    "FLICKR8K_ZIP_FILENAME = \"flickr8k.zip\"\n",
    "FLICKR8K_IMAGES_FOLDER_NAME = \"Images\"\n",
    "CAPTIONS_CSV_FILENAME = \"captions.csv\"\n",
    "OUTPUT_FOLDER_NAME = \"output\"\n",
    "\n",
    "import zipfile\n",
    "import requests\n",
    "import io\n",
    "\n",
    "def download_file(url: str, destination_path: str):\n",
    "    print(f\"Downloading from {url} to {destination_path}...\")\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()\n",
    "        with open(destination_path, \"wb\") as file:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                if chunk:\n",
    "                    file.write(chunk)\n",
    "        print(\"Download complete.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        raise\n",
    "\n",
    "def extract_zip_file(zip_path: str, destination_folder: str):\n",
    "    print(f\"Extracting {zip_path} to {destination_folder}...\")\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(destination_folder)\n",
    "        print(\"Extraction complete.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Extraction error: {e}\")\n",
    "        raise\n",
    "\n",
    "def setup_data_directory(base_data_path: str):\n",
    "    images_path = os.path.join(base_data_path, FLICKR8K_IMAGES_FOLDER_NAME)\n",
    "    output_path = os.path.join(base_data_path, OUTPUT_FOLDER_NAME)\n",
    "    os.makedirs(base_data_path, exist_ok=True)\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    return base_data_path, images_path, output_path\n",
    "\n",
    "\n",
    "print(f\"Setting up data directories in: {BASE_DATA_DESTINATION}\")\n",
    "base_dir, images_dir, output_dir = setup_data_directory(BASE_DATA_DESTINATION)\n",
    "\n",
    "zip_file_path = os.path.join(base_dir, FLICKR8K_ZIP_FILENAME)\n",
    "\n",
    "if not os.path.exists(images_dir):\n",
    "    print(\"Images not found. Attempting download...\")\n",
    "    try:\n",
    "        download_file(KAGGLE_FLICKR8K_URL, zip_file_path)\n",
    "        extract_zip_file(zip_file_path, base_dir)\n",
    "        os.remove(zip_file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to set up dataset: {e}\")\n",
    "        raise FileNotFoundError(f\"Please manually download and extract to: {base_dir}\")\n",
    "else:\n",
    "    print(f\"Images already exist at {images_dir}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc27e2b8",
   "metadata": {},
   "source": [
    "# Train the model using the downloaded images and custom captions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba672ab3",
   "metadata": {},
   "source": [
    "Importing the required libraries\n",
    "setting parameters and lookups\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b7d2ac",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "22fa04af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset for Flickr8k with the specific JSON caption format\n",
    "class Flickr8kCaptionedDataset(Dataset):\n",
    "    def __init__(self, image_dir, captions_file, preprocess_fn, pull_from_json=True):\n",
    "        self.image_dir = image_dir\n",
    "        self.preprocess_fn = preprocess_fn\n",
    "        \n",
    "        self.num_samples = 0\n",
    "        # Create list of samples\n",
    "        self.samples = []\n",
    "        \n",
    "        if pull_from_json:\n",
    "            # Load captions from JSON file\n",
    "            with open(captions_file, 'r') as f:\n",
    "                self.captions_data = json.load(f)\n",
    "            \n",
    "            # Process JSON with format {\"image.jpg\": {\"long_caption\": \"...\", \"short_caption\": \"...\"}, ...}\n",
    "            for image_name, captions in self.captions_data.items():\n",
    "                if \"long_caption\" in captions and \"short_caption\" in captions:\n",
    "                    #if image is not in the image directory, skip\n",
    "                    image_path = os.path.join(self.image_dir, image_name)\n",
    "                    if not os.path.exists(image_path):\n",
    "                        print(f\"Image {image_path} not found, skipping.\")\n",
    "                        continue\n",
    "                    # Add both caption types for each image\n",
    "                    self.samples.append((image_name, captions[\"short_caption\"], captions[\"long_caption\"]))\n",
    "        else:\n",
    "            # Use the default Flickr8k captions file\n",
    "            captions_file = \"data/captions.txt\"\n",
    "            with open(captions_file, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                \n",
    "            # Process the standard Flickr8k format \n",
    "            # Typically each line has format: \"image_name#caption\" or \"image_name,caption\"\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    # Try to split by common delimiters\n",
    "                    if '#' in line:\n",
    "                        parts = line.split('#', 1)\n",
    "                    else:\n",
    "                        parts = line.split(',', 1)\n",
    "                        \n",
    "                    if len(parts) == 2:\n",
    "                        image_name, caption = parts\n",
    "                        print(f\"Image name: {image_name.strip()}, Caption: {caption.strip()}\")\n",
    "                        # Add this check before appending to self.samples\n",
    "                        image_path = os.path.join(self.image_dir, image_name.strip())\n",
    "                        if not os.path.exists(image_path):\n",
    "                            continue  # Skip this caption if image doesn't exist\n",
    "                        self.samples.append((image_name.strip(), caption.strip(), \"standard\"))\n",
    "                        self.num_samples += 1\n",
    "                        \n",
    "        print(f\"Loaded {len(self.samples)} samples from {captions_file}.\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_name, caption, caption_type = self.samples[idx]\n",
    "        image_path = os.path.join(self.image_dir, image_name)\n",
    "        \n",
    "        # Load and preprocess the image\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            image = self.preprocess_fn(image)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {image_path}: {e}\")\n",
    "            # Return a random valid sample instead\n",
    "            return self.__getitem__(random.randint(0, len(self) - 1))\n",
    "        \n",
    "        return image, caption, caption_type\n",
    "    \n",
    "    def __reduce__(self):\n",
    "        return (self.__class__, (self.image_dir, self.captions_file, self.preprocess_fn, True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca6364d",
   "metadata": {},
   "source": [
    "# PCA of LongCLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cb52f8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rewrite PCA to avoid inf\n",
    "def PCA(input_tensor, PCA_dim):\n",
    "    # 计算均值\n",
    "    mean = torch.mean(input_tensor, dim=0)\n",
    "    # 去均值\n",
    "    X_centered = input_tensor - mean.unsqueeze(0)\n",
    "    X_centered = X_centered.float()\n",
    "\n",
    "    # 使用SVD而不是eig来计算主成分\n",
    "    U, S, Vt = torch.linalg.svd(X_centered, full_matrices=False)\n",
    "    principal_components = Vt.T[:, :PCA_dim]\n",
    "    \n",
    "    # 转换到新的维度\n",
    "    X_transformed = torch.mm(X_centered, principal_components)\n",
    "    # 恢复到原始空间\n",
    "    X_reversed = torch.mm(X_transformed, principal_components.T)\n",
    "    X_reversed += mean\n",
    "\n",
    "    return X_reversed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb56133f",
   "metadata": {},
   "source": [
    "## Loss functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c1e2117a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contrastive Loss Function\n",
    "def single_loss(image_embeds, text_embeds, temperature=0.07):\n",
    "    # Normalize embeddings\n",
    "    image_embeds = F.normalize(image_embeds, dim=1)\n",
    "    text_embeds = F.normalize(text_embeds, dim=1)\n",
    "    \n",
    "    # Compute similarity matrix\n",
    "    logits = torch.matmul(image_embeds, text_embeds.T) / temperature\n",
    "    \n",
    "    # Labels are the positions of the positive pairs\n",
    "    labels = torch.arange(logits.size(0), device=logits.device)\n",
    "    \n",
    "    # Compute loss in both directions (image->text and text->image)\n",
    "    loss_i2t = F.cross_entropy(logits, labels)\n",
    "    loss_t2i = F.cross_entropy(logits.T, labels)\n",
    "    \n",
    "    return (loss_i2t + loss_t2i) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c9ed8cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_clip_loss(image_embedding, long_embedding, short_embedding):\n",
    "    image_features_long = image_embedding\n",
    "    text_features_long = long_embedding\n",
    "    text_features_short = short_embedding\n",
    "    \n",
    "    # Normalize features\n",
    "    image_features_long = image_features_long / image_features_long.norm(dim=1, keepdim=True)\n",
    "    text_features_long = text_features_long / text_features_long.norm(dim=1, keepdim=True)\n",
    "    text_features_short = text_features_short / text_features_short.norm(dim=1, keepdim=True)\n",
    "    \n",
    "    # Apply PCA to get compressed image features\n",
    "    image_features_short = PCA(image_features_long, 32)\n",
    "    image_features_short = image_features_short / image_features_short.norm(dim=1, keepdim=True)\n",
    "    \n",
    "    # Since we're not using distributed training, simplify this part\n",
    "    image_feat_all_long = image_features_long\n",
    "    image_features_all_short = image_features_short\n",
    "    text_feat_all_long = text_features_long\n",
    "    text_feat_all_short = text_features_short\n",
    "    \n",
    "    # Calculate similarity matrices\n",
    "    sim_i2tl = torch.matmul(image_features_long, text_feat_all_long.T)\n",
    "    sim_tl2i = torch.matmul(image_feat_all_long, text_features_long.T)\n",
    "    sim_tl2i = sim_tl2i.T\n",
    "    \n",
    "    sim_i2ts = torch.matmul(image_features_short, text_feat_all_short.T)\n",
    "    sim_ts2i = torch.matmul(image_features_all_short, text_features_short.T)\n",
    "    sim_ts2i = sim_ts2i.T\n",
    "    \n",
    "    # Apply temperature scaling\n",
    "    logit_scale = model.logit_scale if hasattr(model, 'logit_scale') else 1.0\n",
    "    \n",
    "    if isinstance(logit_scale, torch.nn.Parameter):\n",
    "        sim_i2tl = logit_scale.exp() * sim_i2tl\n",
    "        sim_tl2i = logit_scale.exp() * sim_tl2i\n",
    "        sim_i2ts = logit_scale.exp() * sim_i2ts\n",
    "        sim_ts2i = logit_scale.exp() * sim_ts2i\n",
    "    \n",
    "    # Create targets for loss calculation\n",
    "    bs = image_embedding.size(0)\n",
    "    targets = torch.arange(bs, device=image_embedding.device)\n",
    "    \n",
    "    # Calculate losses\n",
    "    loss_itcl = (\n",
    "        F.cross_entropy(sim_i2tl, targets, label_smoothing=0.1) \n",
    "        + F.cross_entropy(sim_tl2i, targets, label_smoothing=0.1)\n",
    "    ) / 2\n",
    "    \n",
    "    loss_itcs = (\n",
    "        F.cross_entropy(sim_i2ts, targets, label_smoothing=0.1)\n",
    "        + F.cross_entropy(sim_ts2i, targets, label_smoothing=0.1)\n",
    "    ) / 2\n",
    "    \n",
    "    # single loss by combining the two\n",
    "    total_loss = (loss_itcl + loss_itcs) / 2\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0e2873",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7353ab8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1074 samples from /Users/erenyavuz/Desktop/KU/25 Spring/COMP447/Project/Repo/FlightVision/data/all_captions.json.\n"
     ]
    }
   ],
   "source": [
    "# Set up dataset and dataloader\n",
    "images_dir = os.path.join(BASE_DATA_DESTINATION, FLICKR8K_IMAGES_FOLDER_NAME)\n",
    "captions_file = os.path.join(BASE_DATA_DESTINATION, CAPTIONS_JSON_FILENAME)\n",
    "\n",
    "pull_from_json = True\n",
    "\n",
    "# Check if files exist\n",
    "if not os.path.exists(images_dir):\n",
    "    raise FileNotFoundError(f\"Images directory not found: {images_dir}\")\n",
    "if not os.path.exists(captions_file):\n",
    "    print(f\"Captions file not found: {captions_file}\")\n",
    "    pull_from_json = False\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = Flickr8kCaptionedDataset(images_dir, captions_file, preprocess, pull_from_json=pull_from_json)\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=0,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "\n",
    "\n",
    "# Training loop\n",
    "def train():\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        total_loss = 0.0\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "        \n",
    "        for batch_idx, (images, captions, long_captions) in enumerate(progress_bar):\n",
    "            images = images.to(DEVICE)\n",
    "            \n",
    "            # Tokenize the captions\n",
    "            tokenized_captions = tokenizer(captions).to(DEVICE)\n",
    "            \n",
    "            # Forward pass with mixed precision\n",
    "            with torch.cuda.amp.autocast():\n",
    "                image_features = model.encode_image(images)\n",
    "                text_features = model.encode_text(tokenized_captions)\n",
    "                \n",
    "                # Compute contrastive loss\n",
    "                if long_captions is not None:\n",
    "                    long_captions = tokenizer(long_captions).to(DEVICE)\n",
    "                    long_text_features = model.encode_text(long_captions)\n",
    "                    loss = long_clip_loss(image_features, text_features, long_text_features)\n",
    "                else:\n",
    "                    # Use single loss if long captions are not available\n",
    "                    loss = single_loss(image_features, text_features)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update progress bar\n",
    "            total_loss += loss.item()\n",
    "            avg_loss = total_loss / (batch_idx + 1)\n",
    "            progress_bar.set_postfix(loss=f\"{avg_loss:.4f}\")\n",
    "            if batch_idx >= MAX_TRAINING_STEPS:\n",
    "                break\n",
    "        # Print average loss for the epoch\n",
    "            \n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"mobileclip_finetuned_epoch{epoch+1}.pt\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e5d0e0",
   "metadata": {},
   "source": [
    "## Running the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cd246bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device: cpu\n",
      "Dataset size: 1074 samples\n",
      "Batch size: 32\n",
      "Learning rate: 0.0001\n",
      "Number of epochs: 1\n",
      "Max training steps: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 33/33 [06:40<00:00, 12.15s/it, loss=1.5122]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Loss: 1.5122\n",
      "Checkpoint saved: /Users/erenyavuz/Desktop/KU/25 Spring/COMP447/Project/Repo/FlightVision/checkpoints/mobileclip_finetuned_epoch1.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (image_encoder): MCi(\n",
       "    (model): FastViT(\n",
       "      (patch_embed): Sequential(\n",
       "        (0): MobileOneBlock(\n",
       "          (se): Identity()\n",
       "          (activation): GELU(approximate='none')\n",
       "          (reparam_conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "        (1): MobileOneBlock(\n",
       "          (se): Identity()\n",
       "          (activation): GELU(approximate='none')\n",
       "          (reparam_conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64)\n",
       "        )\n",
       "        (2): MobileOneBlock(\n",
       "          (se): Identity()\n",
       "          (activation): GELU(approximate='none')\n",
       "          (reparam_conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (network): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): RepMixerBlock(\n",
       "            (token_mixer): RepMixer(\n",
       "              (reparam_conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "            )\n",
       "            (convffn): ConvFFN(\n",
       "              (conv): Sequential(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=64, bias=False)\n",
       "                (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (fc1): Conv2d(64, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (1): RepMixerBlock(\n",
       "            (token_mixer): RepMixer(\n",
       "              (reparam_conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "            )\n",
       "            (convffn): ConvFFN(\n",
       "              (conv): Sequential(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=64, bias=False)\n",
       "                (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (fc1): Conv2d(64, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "        )\n",
       "        (1): PatchEmbed(\n",
       "          (proj): Sequential(\n",
       "            (0): ReparamLargeKernelConv(\n",
       "              (activation): GELU(approximate='none')\n",
       "              (se): Identity()\n",
       "              (lkb_reparam): Conv2d(64, 128, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=64)\n",
       "            )\n",
       "            (1): MobileOneBlock(\n",
       "              (se): Identity()\n",
       "              (activation): GELU(approximate='none')\n",
       "              (reparam_conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): RepMixerBlock(\n",
       "            (token_mixer): RepMixer(\n",
       "              (reparam_conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "            )\n",
       "            (convffn): ConvFFN(\n",
       "              (conv): Sequential(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128, bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (fc1): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (1): RepMixerBlock(\n",
       "            (token_mixer): RepMixer(\n",
       "              (reparam_conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "            )\n",
       "            (convffn): ConvFFN(\n",
       "              (conv): Sequential(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128, bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (fc1): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (2): RepMixerBlock(\n",
       "            (token_mixer): RepMixer(\n",
       "              (reparam_conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "            )\n",
       "            (convffn): ConvFFN(\n",
       "              (conv): Sequential(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128, bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (fc1): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (3): RepMixerBlock(\n",
       "            (token_mixer): RepMixer(\n",
       "              (reparam_conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "            )\n",
       "            (convffn): ConvFFN(\n",
       "              (conv): Sequential(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128, bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (fc1): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (4): RepMixerBlock(\n",
       "            (token_mixer): RepMixer(\n",
       "              (reparam_conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "            )\n",
       "            (convffn): ConvFFN(\n",
       "              (conv): Sequential(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128, bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (fc1): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (5): RepMixerBlock(\n",
       "            (token_mixer): RepMixer(\n",
       "              (reparam_conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "            )\n",
       "            (convffn): ConvFFN(\n",
       "              (conv): Sequential(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128, bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (fc1): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "        )\n",
       "        (3): PatchEmbed(\n",
       "          (proj): Sequential(\n",
       "            (0): ReparamLargeKernelConv(\n",
       "              (activation): GELU(approximate='none')\n",
       "              (se): SEModule(\n",
       "                (fc1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (bn): Identity()\n",
       "                (act): ReLU(inplace=True)\n",
       "                (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (lkb_reparam): Conv2d(128, 256, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=128)\n",
       "            )\n",
       "            (1): MobileOneBlock(\n",
       "              (se): Identity()\n",
       "              (activation): GELU(approximate='none')\n",
       "              (reparam_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): Sequential(\n",
       "          (0): RepMixerBlock(\n",
       "            (token_mixer): RepMixer(\n",
       "              (reparam_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "            )\n",
       "            (convffn): ConvFFN(\n",
       "              (conv): Sequential(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256, bias=False)\n",
       "                (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (fc1): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (1): RepMixerBlock(\n",
       "            (token_mixer): RepMixer(\n",
       "              (reparam_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "            )\n",
       "            (convffn): ConvFFN(\n",
       "              (conv): Sequential(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256, bias=False)\n",
       "                (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (fc1): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (2): RepMixerBlock(\n",
       "            (token_mixer): RepMixer(\n",
       "              (reparam_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "            )\n",
       "            (convffn): ConvFFN(\n",
       "              (conv): Sequential(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256, bias=False)\n",
       "                (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (fc1): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (3): RepMixerBlock(\n",
       "            (token_mixer): RepMixer(\n",
       "              (reparam_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "            )\n",
       "            (convffn): ConvFFN(\n",
       "              (conv): Sequential(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256, bias=False)\n",
       "                (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (fc1): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (4): RepMixerBlock(\n",
       "            (token_mixer): RepMixer(\n",
       "              (reparam_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "            )\n",
       "            (convffn): ConvFFN(\n",
       "              (conv): Sequential(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256, bias=False)\n",
       "                (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (fc1): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (5): RepMixerBlock(\n",
       "            (token_mixer): RepMixer(\n",
       "              (reparam_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "            )\n",
       "            (convffn): ConvFFN(\n",
       "              (conv): Sequential(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256, bias=False)\n",
       "                (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (fc1): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (6): RepMixerBlock(\n",
       "            (token_mixer): RepMixer(\n",
       "              (reparam_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "            )\n",
       "            (convffn): ConvFFN(\n",
       "              (conv): Sequential(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256, bias=False)\n",
       "                (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (fc1): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (7): RepMixerBlock(\n",
       "            (token_mixer): RepMixer(\n",
       "              (reparam_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "            )\n",
       "            (convffn): ConvFFN(\n",
       "              (conv): Sequential(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256, bias=False)\n",
       "                (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (fc1): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (8): RepMixerBlock(\n",
       "            (token_mixer): RepMixer(\n",
       "              (reparam_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "            )\n",
       "            (convffn): ConvFFN(\n",
       "              (conv): Sequential(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256, bias=False)\n",
       "                (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (fc1): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (9): RepMixerBlock(\n",
       "            (token_mixer): RepMixer(\n",
       "              (reparam_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "            )\n",
       "            (convffn): ConvFFN(\n",
       "              (conv): Sequential(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256, bias=False)\n",
       "                (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (fc1): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "        )\n",
       "        (5): PatchEmbed(\n",
       "          (proj): Sequential(\n",
       "            (0): ReparamLargeKernelConv(\n",
       "              (activation): GELU(approximate='none')\n",
       "              (se): SEModule(\n",
       "                (fc1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (bn): Identity()\n",
       "                (act): ReLU(inplace=True)\n",
       "                (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (lkb_reparam): Conv2d(256, 512, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=256)\n",
       "            )\n",
       "            (1): MobileOneBlock(\n",
       "              (se): Identity()\n",
       "              (activation): GELU(approximate='none')\n",
       "              (reparam_conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): RepCPE(\n",
       "          (reparam_conv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "        )\n",
       "        (7): Sequential(\n",
       "          (0): AttentionBlock(\n",
       "            (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (token_mixer): MHSA(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (convffn): ConvFFN(\n",
       "              (conv): Sequential(\n",
       "                (conv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512, bias=False)\n",
       "                (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (fc1): Conv2d(512, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Conv2d(1536, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (1): AttentionBlock(\n",
       "            (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (token_mixer): MHSA(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (convffn): ConvFFN(\n",
       "              (conv): Sequential(\n",
       "                (conv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512, bias=False)\n",
       "                (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (fc1): Conv2d(512, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Conv2d(1536, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (conv_exp): MobileOneBlock(\n",
       "        (se): SEBlock(\n",
       "          (reduce): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (expand): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (activation): GELU(approximate='none')\n",
       "        (reparam_conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "      )\n",
       "      (head): GlobalPool2D(\n",
       "        (pool): GlobalPool()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (text_encoder): TextTransformer(\n",
       "    (embedding_layer): Embedding(49408, 512)\n",
       "    (positional_embedding): LearnablePositionalEmbedding(num_embeddings=77, embedding_dim=512, padding_idx=None)\n",
       "    (embedding_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (transformer): ModuleList(\n",
       "      (0): RepMixerBlock(\n",
       "        (token_mixer): RepMixer(\n",
       "          (reparam_conv): Conv2d(512, 512, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5), groups=512)\n",
       "        )\n",
       "        (convffn): ConvFFN(\n",
       "          (conv): Sequential(\n",
       "            (conv): Conv2d(512, 512, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5), groups=512, bias=False)\n",
       "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (fc1): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (1-4): 4 x TransformerEncoder(embed_dim=512, ffn_dim=2048, dropout=0.0, ffn_dropout=0.0, stochastic_dropout=0.0, attn_fn=MultiHeadAttention, act_fn=GELU, norm_fn=layer_norm_fp32)\n",
       "      (5): RepMixerBlock(\n",
       "        (token_mixer): RepMixer(\n",
       "          (reparam_conv): Conv2d(512, 512, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5), groups=512)\n",
       "        )\n",
       "        (convffn): ConvFFN(\n",
       "          (conv): Sequential(\n",
       "            (conv): Conv2d(512, 512, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5), groups=512, bias=False)\n",
       "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (fc1): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNormFP32((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run training\n",
    "print(f\"Training on device: {DEVICE}\")\n",
    "print(f\"Dataset size: {len(dataset)} samples\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Number of epochs: {NUM_EPOCHS}\")\n",
    "\n",
    "MAX_TRAINING_STEPS = 1000\n",
    "\n",
    "\n",
    "print(f\"Max training steps: {MAX_TRAINING_STEPS}\")\n",
    "\n",
    "train()\n",
    "\n",
    "# Evaluate model after training\n",
    "model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49b57a8",
   "metadata": {},
   "source": [
    "Load the trained checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af73e6a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If not trained, load the trained checkpoint\n",
    "checkpoint_path = os.path.join(CHECKPOINT_DIR, 'mobileclip_finetuned_epoch1.pt')\n",
    "checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b7e2b7",
   "metadata": {},
   "source": [
    "# Evaluate the model with short captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2832abfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label probabilities after training: tensor([[0.0219, 0.9768, 0.0012]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Example evaluation\n",
    "test_image_path = \"/Users/erenyavuz/Desktop/KU/25 Spring/COMP447/Project/Repo/FlightVision/pngwing.com.png\"\n",
    "test_texts = [\"a brown\", \"a white dog\", \"a black dog\"]\n",
    "\n",
    "test_image = preprocess(Image.open(test_image_path).convert('RGB')).unsqueeze(0).to(DEVICE)\n",
    "test_text = tokenizer(test_texts).to(DEVICE)\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    image_features = model.encode_image(test_image)\n",
    "    text_features = model.encode_text(test_text)\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "torch.set_printoptions(sci_mode=False, precision=4)\n",
    "print(\"Label probabilities after training:\", text_probs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ad6790",
   "metadata": {},
   "source": [
    "# Evalute the model with long captions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
