{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89daaaba",
   "metadata": {},
   "source": [
    "# Get Pretrained Models\n",
    "Okay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5dae603b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-04-30 18:31:27--  https://docs-assets.developer.apple.com/ml-research/datasets/mobileclip/mobileclip_s0.pt\n",
      "Resolving docs-assets.developer.apple.com (docs-assets.developer.apple.com)... 17.253.73.202, 17.253.73.201\n",
      "Connecting to docs-assets.developer.apple.com (docs-assets.developer.apple.com)|17.253.73.202|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 215934653 (206M) [application/octet-stream]\n",
      "Saving to: ‘checkpoints/mobileclip_s0.pt’\n",
      "\n",
      "mobileclip_s0.pt    100%[===================>] 205,93M  5,49MB/s    in 39s     \n",
      "\n",
      "2025-04-30 18:32:07 (5,34 MB/s) - ‘checkpoints/mobileclip_s0.pt’ saved [215934653/215934653]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p checkpoints/\n",
    "\n",
    "!wget https://docs-assets.developer.apple.com/ml-research/datasets/mobileclip/mobileclip_s0.pt -P checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42eac2b",
   "metadata": {},
   "source": [
    "# Create the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60043773",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install torchvision\n",
    "!pip install timm\n",
    "!pip install open-clip-torch\n",
    "!pip install datasets\n",
    "!pip install clip-benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ffff17",
   "metadata": {},
   "source": [
    "# Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c56856b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/erenyavuz/Desktop/KU/25 Spring/COMP447/Project/Repo/FlightVision/pngwing.com(1).png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m model, _, preprocess \u001b[38;5;241m=\u001b[39m mobileclip\u001b[38;5;241m.\u001b[39mcreate_model_and_transforms(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmobileclip_s0\u001b[39m\u001b[38;5;124m'\u001b[39m, pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/erenyavuz/Desktop/KU/25 Spring/COMP447/Project/Repo/FlightVision/checkpoints/mobileclip_s0.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m mobileclip\u001b[38;5;241m.\u001b[39mget_tokenizer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmobileclip_s0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m image \u001b[38;5;241m=\u001b[39m preprocess(Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpngwing.com(1).png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      9\u001b[0m text \u001b[38;5;241m=\u001b[39m tokenizer([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma purple dog\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma white dog\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma black dog\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(), torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast():\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/PIL/Image.py:3431\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3428\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(os\u001b[38;5;241m.\u001b[39mfspath(fp))\n\u001b[1;32m   3430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3431\u001b[0m     fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3432\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/erenyavuz/Desktop/KU/25 Spring/COMP447/Project/Repo/FlightVision/pngwing.com(1).png'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import mobileclip\n",
    "\n",
    "model, _, preprocess = mobileclip.create_model_and_transforms('mobileclip_s0', pretrained='/Users/erenyavuz/Desktop/KU/25 Spring/COMP447/Project/Repo/FlightVision/checkpoints/mobileclip_s0.pt')\n",
    "tokenizer = mobileclip.get_tokenizer('mobileclip_s0')\n",
    "\n",
    "image = preprocess(Image.open(\"pngwing.com(1).png\").convert('RGB')).unsqueeze(0)\n",
    "text = tokenizer([\"a purple dog\", \"a white dog\", \"a black dog\"])\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "# Set the print options for PyTorch to avoid scientific notation and limit decimal places\n",
    "torch.set_printoptions(sci_mode=False, precision=4)\n",
    "\n",
    "print(\"Label probs:\", text_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86efc59",
   "metadata": {},
   "source": [
    "# Changing the Positional Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97b92180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LearnablePositionalEmbedding(num_embeddings=77, embedding_dim=512, padding_idx=None)\n",
      "Modified Positional Embedding: Parameter containing:\n",
      "tensor([[[[     0.0000,      0.0000,      0.0000,  ...,      0.0000,\n",
      "                0.0000,      0.0000],\n",
      "          [     0.0041,      0.0016,     -0.0007,  ...,      0.0007,\n",
      "                0.0036,     -0.0076],\n",
      "          [     0.0077,      0.0026,      0.0012,  ...,      0.0001,\n",
      "                0.0013,     -0.0043],\n",
      "          ...,\n",
      "          [     0.0075,      0.0051,      0.0000,  ...,      0.0011,\n",
      "                0.0001,     -0.0003],\n",
      "          [     0.0031,      0.0068,     -0.0008,  ...,      0.0024,\n",
      "               -0.0002,     -0.0025],\n",
      "          [    -0.0013,      0.0086,     -0.0016,  ...,      0.0036,\n",
      "               -0.0004,     -0.0046]]]])\n"
     ]
    }
   ],
   "source": [
    "print(model.get_positional_embedding() )\n",
    "\n",
    "def get_positional_embedding(self, lambda2: int = 4):\n",
    "    \"\"\"\n",
    "    Get modified positional embedding for text encoder based on the given formula.\n",
    "    \"\"\"\n",
    "    pos_embed = self.text_encoder.get_positional_embedding().pos_embed.pos_embed\n",
    "    if pos_embed is None:\n",
    "        raise ValueError(\"Positional embedding not found in text encoder.\")\n",
    "\n",
    "    max_pos, embed_dim = pos_embed.shape[2], pos_embed.shape[3]\n",
    "    modified_pos_embed = torch.zeros((1, 1, max_pos, embed_dim), device=pos_embed.device)\n",
    "\n",
    "    for pos in range(max_pos):\n",
    "        if pos <= 20:\n",
    "            modified_pos_embed[:, :, pos, :] = pos_embed[:, :, pos, :]\n",
    "        else:\n",
    "            lower_idx = pos // lambda2\n",
    "            upper_idx = min(lower_idx + 1, max_pos - 1)  # Ensure upper_idx is within bounds\n",
    "            alpha = (pos % lambda2) / lambda2\n",
    "            modified_pos_embed[:, :, pos, :] = (1 - alpha) * pos_embed[:, :, lower_idx, :] + alpha * pos_embed[:, :, upper_idx, :]\n",
    "    # turn the torch tensor into nn parameter\n",
    "    modified_pos_embed = torch.nn.Parameter(modified_pos_embed, requires_grad=False)\n",
    "    return modified_pos_embed\n",
    "\n",
    "# Example usage\n",
    "lambda2 = 4\n",
    "new_pos_embed = get_positional_embedding(model, lambda2)\n",
    "print(\"Modified Positional Embedding:\", new_pos_embed)\n",
    "\n",
    "# set the models pos embedding to the new one\n",
    "model.text_encoder.get_positional_embedding().pos_embed.pos_embed = new_pos_embed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7444910a",
   "metadata": {},
   "source": [
    "# Testing the model after changing the positional embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1d0bbec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label probs: tensor([[0.7638, 0.0161, 0.2201]])\n"
     ]
    }
   ],
   "source": [
    "image = preprocess(Image.open(\"pngwing.com(1).png\").convert('RGB')).unsqueeze(0)\n",
    "text = tokenizer([\"a purple dog\", \"a white dog\", \"a black dog\"])\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "# Set the print options for PyTorch to avoid scientific notation and limit decimal places\n",
    "torch.set_printoptions(sci_mode=False, precision=4)\n",
    "\n",
    "print(\"Label probs:\", text_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03590a4c",
   "metadata": {},
   "source": [
    "# Downloading the captioned images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "284b0522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up data directories in: /Users/erenyavuz/Desktop/KU/25 Spring/COMP447/Project/Repo/FlightVision/data\n",
      "Images not found. Attempting download...\n",
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/adityajn105/flickr8k to /Users/erenyavuz/Desktop/KU/25 Spring/COMP447/Project/Repo/FlightVision/data/flickr8k.zip...\n",
      "Download complete.\n",
      "Extracting /Users/erenyavuz/Desktop/KU/25 Spring/COMP447/Project/Repo/FlightVision/data/flickr8k.zip to /Users/erenyavuz/Desktop/KU/25 Spring/COMP447/Project/Repo/FlightVision/data...\n",
      "Extraction complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "import os\n",
    "\n",
    "# Using relative paths for better portability\n",
    "# This creates a 'data' directory in the project folder\n",
    "BASE_DATA_DESTINATION = os.path.join(os.getcwd(), \"data\")\n",
    "KAGGLE_FLICKR8K_URL = \"https://www.kaggle.com/api/v1/datasets/download/adityajn105/flickr8k\"\n",
    "FLICKR8K_ZIP_FILENAME = \"flickr8k.zip\"\n",
    "FLICKR8K_IMAGES_FOLDER_NAME = \"Images\"\n",
    "CAPTIONS_CSV_FILENAME = \"captions.csv\"\n",
    "OUTPUT_FOLDER_NAME = \"output\"\n",
    "\n",
    "import zipfile\n",
    "import requests\n",
    "import io\n",
    "\n",
    "def download_file(url: str, destination_path: str):\n",
    "    print(f\"Downloading from {url} to {destination_path}...\")\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()\n",
    "        with open(destination_path, \"wb\") as file:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                if chunk:\n",
    "                    file.write(chunk)\n",
    "        print(\"Download complete.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        raise\n",
    "\n",
    "def extract_zip_file(zip_path: str, destination_folder: str):\n",
    "    print(f\"Extracting {zip_path} to {destination_folder}...\")\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(destination_folder)\n",
    "        print(\"Extraction complete.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Extraction error: {e}\")\n",
    "        raise\n",
    "\n",
    "def setup_data_directory(base_data_path: str):\n",
    "    images_path = os.path.join(base_data_path, FLICKR8K_IMAGES_FOLDER_NAME)\n",
    "    output_path = os.path.join(base_data_path, OUTPUT_FOLDER_NAME)\n",
    "    os.makedirs(base_data_path, exist_ok=True)\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    return base_data_path, images_path, output_path\n",
    "\n",
    "\n",
    "print(f\"Setting up data directories in: {BASE_DATA_DESTINATION}\")\n",
    "base_dir, images_dir, output_dir = setup_data_directory(BASE_DATA_DESTINATION)\n",
    "\n",
    "zip_file_path = os.path.join(base_dir, FLICKR8K_ZIP_FILENAME)\n",
    "\n",
    "if not os.path.exists(images_dir):\n",
    "    print(\"Images not found. Attempting download...\")\n",
    "    try:\n",
    "        download_file(KAGGLE_FLICKR8K_URL, zip_file_path)\n",
    "        extract_zip_file(zip_file_path, base_dir)\n",
    "        os.remove(zip_file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to set up dataset: {e}\")\n",
    "        raise FileNotFoundError(f\"Please manually download and extract to: {base_dir}\")\n",
    "else:\n",
    "    print(f\"Images already exist at {images_dir}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc27e2b8",
   "metadata": {},
   "source": [
    "# Train the model using the downloaded images and custom captions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba672ab3",
   "metadata": {},
   "source": [
    "Importing the required libraries\n",
    "setting parameters and lookups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "006b697a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (image_encoder): MCi(\n",
       "    (model): FastViT(\n",
       "      (patch_embed): Sequential(\n",
       "        (0): MobileOneBlock(\n",
       "          (se): Identity()\n",
       "          (activation): GELU(approximate='none')\n",
       "          (reparam_conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "        (1): MobileOneBlock(\n",
       "          (se): Identity()\n",
       "          (activation): GELU(approximate='none')\n",
       "          (reparam_conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64)\n",
       "        )\n",
       "        (2): MobileOneBlock(\n",
       "          (se): Identity()\n",
       "          (activation): GELU(approximate='none')\n",
       "          (reparam_conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (network): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): RepMixerBlock(\n",
       "            (token_mixer): RepMixer(\n",
       "              (reparam_conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "            )\n",
       "            (convffn): ConvFFN(\n",
       "              (conv): Sequential(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=64, bias=False)\n",
       "                (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (fc1): Conv2d(64, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (1): RepMixerBlock(\n",
       "            (token_mixer): RepMixer(\n",
       "              (reparam_conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "            )\n",
       "            (convffn): ConvFFN(\n",
       "              (conv): Sequential(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=64, bias=False)\n",
       "                (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (fc1): Conv2d(64, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "        )\n",
       "        (1): PatchEmbed(\n",
       "          (proj): Sequential(\n",
       "            (0): ReparamLargeKernelConv(\n",
       "              (activation): GELU(approximate='none')\n",
       "              (se): Identity()\n",
       "              (lkb_reparam): Conv2d(64, 128, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=64)\n",
       "            )\n",
       "            (1): MobileOneBlock(\n",
       "              (se): Identity()\n",
       "              (activation): GELU(approximate='none')\n",
       "              (reparam_conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): RepMixerBlock(\n",
       "            (token_mixer): RepMixer(\n",
       "              (reparam_conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "            )\n",
       "            (convffn): ConvFFN(\n",
       "              (conv): Sequential(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128, bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (fc1): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (1): RepMixerBlock(\n",
       "            (token_mixer): RepMixer(\n",
       "              (reparam_conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "            )\n",
       "            (convffn): ConvFFN(\n",
       "              (conv): Sequential(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128, bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (fc1): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (2): RepMixerBlock(\n",
       "            (token_mixer): RepMixer(\n",
       "              (reparam_conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "            )\n",
       "            (convffn): ConvFFN(\n",
       "              (conv): Sequential(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128, bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (fc1): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (3): RepMixerBlock(\n",
       "            (token_mixer): RepMixer(\n",
       "              (reparam_conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "            )\n",
       "            (convffn): ConvFFN(\n",
       "              (conv): Sequential(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128, bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (fc1): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (4): RepMixerBlock(\n",
       "            (token_mixer): RepMixer(\n",
       "              (reparam_conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "            )\n",
       "            (convffn): ConvFFN(\n",
       "              (conv): Sequential(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128, bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (fc1): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (5): RepMixerBlock(\n",
       "            (token_mixer): RepMixer(\n",
       "              (reparam_conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "            )\n",
       "            (convffn): ConvFFN(\n",
       "              (conv): Sequential(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128, bias=False)\n",
       "                (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (fc1): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "        )\n",
       "        (3): PatchEmbed(\n",
       "          (proj): Sequential(\n",
       "            (0): ReparamLargeKernelConv(\n",
       "              (activation): GELU(approximate='none')\n",
       "              (se): SEModule(\n",
       "                (fc1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (bn): Identity()\n",
       "                (act): ReLU(inplace=True)\n",
       "                (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (lkb_reparam): Conv2d(128, 256, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=128)\n",
       "            )\n",
       "            (1): MobileOneBlock(\n",
       "              (se): Identity()\n",
       "              (activation): GELU(approximate='none')\n",
       "              (reparam_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): Sequential(\n",
       "          (0): RepMixerBlock(\n",
       "            (token_mixer): RepMixer(\n",
       "              (reparam_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "            )\n",
       "            (convffn): ConvFFN(\n",
       "              (conv): Sequential(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256, bias=False)\n",
       "                (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (fc1): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (1): RepMixerBlock(\n",
       "            (token_mixer): RepMixer(\n",
       "              (reparam_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "            )\n",
       "            (convffn): ConvFFN(\n",
       "              (conv): Sequential(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256, bias=False)\n",
       "                (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (fc1): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (2): RepMixerBlock(\n",
       "            (token_mixer): RepMixer(\n",
       "              (reparam_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "            )\n",
       "            (convffn): ConvFFN(\n",
       "              (conv): Sequential(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256, bias=False)\n",
       "                (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (fc1): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (3): RepMixerBlock(\n",
       "            (token_mixer): RepMixer(\n",
       "              (reparam_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "            )\n",
       "            (convffn): ConvFFN(\n",
       "              (conv): Sequential(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256, bias=False)\n",
       "                (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (fc1): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (4): RepMixerBlock(\n",
       "            (token_mixer): RepMixer(\n",
       "              (reparam_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "            )\n",
       "            (convffn): ConvFFN(\n",
       "              (conv): Sequential(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256, bias=False)\n",
       "                (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (fc1): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (5): RepMixerBlock(\n",
       "            (token_mixer): RepMixer(\n",
       "              (reparam_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "            )\n",
       "            (convffn): ConvFFN(\n",
       "              (conv): Sequential(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256, bias=False)\n",
       "                (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (fc1): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (6): RepMixerBlock(\n",
       "            (token_mixer): RepMixer(\n",
       "              (reparam_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "            )\n",
       "            (convffn): ConvFFN(\n",
       "              (conv): Sequential(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256, bias=False)\n",
       "                (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (fc1): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (7): RepMixerBlock(\n",
       "            (token_mixer): RepMixer(\n",
       "              (reparam_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "            )\n",
       "            (convffn): ConvFFN(\n",
       "              (conv): Sequential(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256, bias=False)\n",
       "                (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (fc1): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (8): RepMixerBlock(\n",
       "            (token_mixer): RepMixer(\n",
       "              (reparam_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "            )\n",
       "            (convffn): ConvFFN(\n",
       "              (conv): Sequential(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256, bias=False)\n",
       "                (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (fc1): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (9): RepMixerBlock(\n",
       "            (token_mixer): RepMixer(\n",
       "              (reparam_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "            )\n",
       "            (convffn): ConvFFN(\n",
       "              (conv): Sequential(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256, bias=False)\n",
       "                (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (fc1): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "        )\n",
       "        (5): PatchEmbed(\n",
       "          (proj): Sequential(\n",
       "            (0): ReparamLargeKernelConv(\n",
       "              (activation): GELU(approximate='none')\n",
       "              (se): SEModule(\n",
       "                (fc1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (bn): Identity()\n",
       "                (act): ReLU(inplace=True)\n",
       "                (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (lkb_reparam): Conv2d(256, 512, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=256)\n",
       "            )\n",
       "            (1): MobileOneBlock(\n",
       "              (se): Identity()\n",
       "              (activation): GELU(approximate='none')\n",
       "              (reparam_conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): RepCPE(\n",
       "          (reparam_conv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "        )\n",
       "        (7): Sequential(\n",
       "          (0): AttentionBlock(\n",
       "            (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (token_mixer): MHSA(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (convffn): ConvFFN(\n",
       "              (conv): Sequential(\n",
       "                (conv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512, bias=False)\n",
       "                (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (fc1): Conv2d(512, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Conv2d(1536, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (1): AttentionBlock(\n",
       "            (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (token_mixer): MHSA(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (convffn): ConvFFN(\n",
       "              (conv): Sequential(\n",
       "                (conv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512, bias=False)\n",
       "                (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (fc1): Conv2d(512, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Conv2d(1536, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (conv_exp): MobileOneBlock(\n",
       "        (se): SEBlock(\n",
       "          (reduce): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (expand): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (activation): GELU(approximate='none')\n",
       "        (reparam_conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "      )\n",
       "      (head): GlobalPool2D(\n",
       "        (pool): GlobalPool()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (text_encoder): TextTransformer(\n",
       "    (embedding_layer): Embedding(49408, 512)\n",
       "    (positional_embedding): LearnablePositionalEmbedding(num_embeddings=77, embedding_dim=512, padding_idx=None)\n",
       "    (embedding_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (transformer): ModuleList(\n",
       "      (0): RepMixerBlock(\n",
       "        (token_mixer): RepMixer(\n",
       "          (reparam_conv): Conv2d(512, 512, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5), groups=512)\n",
       "        )\n",
       "        (convffn): ConvFFN(\n",
       "          (conv): Sequential(\n",
       "            (conv): Conv2d(512, 512, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5), groups=512, bias=False)\n",
       "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (fc1): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (1-4): 4 x TransformerEncoder(embed_dim=512, ffn_dim=2048, dropout=0.0, ffn_dropout=0.0, stochastic_dropout=0.0, attn_fn=MultiHeadAttention, act_fn=GELU, norm_fn=layer_norm_fp32)\n",
       "      (5): RepMixerBlock(\n",
       "        (token_mixer): RepMixer(\n",
       "          (reparam_conv): Conv2d(512, 512, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5), groups=512)\n",
       "        )\n",
       "        (convffn): ConvFFN(\n",
       "          (conv): Sequential(\n",
       "            (conv): Conv2d(512, 512, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5), groups=512, bias=False)\n",
       "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (fc1): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNormFP32((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import mobileclip\n",
    "import random\n",
    "\n",
    "# --- Configuration ---\n",
    "# Using relative paths for better portability\n",
    "BASE_DATA_DESTINATION = os.path.join(os.getcwd(), \"data\")\n",
    "FLICKR8K_IMAGES_FOLDER_NAME = \"Images\"\n",
    "CAPTIONS_JSON_FILENAME = \"captions.json\"\n",
    "CHECKPOINT_DIR = os.path.join(os.getcwd(), \"checkpoints\")\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 10\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the MobileCLIP model\n",
    "model_path = os.path.join(CHECKPOINT_DIR, 'mobileclip_s0.pt')\n",
    "model, _, preprocess = mobileclip.create_model_and_transforms(\n",
    "    'mobileclip_s0', \n",
    "    pretrained=model_path\n",
    ")\n",
    "tokenizer = mobileclip.get_tokenizer('mobileclip_s0')\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b7d2ac",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22fa04af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset for Flickr8k with the specific JSON caption format\n",
    "class Flickr8kCaptionedDataset(Dataset):\n",
    "    def __init__(self, image_dir, captions_file, preprocess_fn, pull_from_json=True):\n",
    "        self.image_dir = image_dir\n",
    "        self.preprocess_fn = preprocess_fn\n",
    "        \n",
    "    \n",
    "        # Create list of samples\n",
    "        self.samples = []\n",
    "        \n",
    "        if pull_from_json:\n",
    "            # Load captions from JSON file\n",
    "            with open(captions_file, 'r') as f:\n",
    "                self.captions_data = json.load(f)\n",
    "            \n",
    "            # Process JSON with format {\"image.jpg\": {\"long_caption\": \"...\", \"short_caption\": \"...\"}, ...}\n",
    "            for image_name, captions in self.captions_data.items():\n",
    "                if \"long_caption\" in captions and \"short_caption\" in captions:\n",
    "                    # Add both caption types for each image\n",
    "                    self.samples.append((image_name, captions[\"long_caption\"], \"long\"))\n",
    "                    self.samples.append((image_name, captions[\"short_caption\"], \"short\"))\n",
    "        else:\n",
    "            # Use the default Flickr8k captions file\n",
    "            captions_file = \"data/captions.txt\"\n",
    "            with open(captions_file, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                \n",
    "            # Process the standard Flickr8k format \n",
    "            # Typically each line has format: \"image_name#caption\" or \"image_name,caption\"\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    # Try to split by common delimiters\n",
    "                    if '#' in line:\n",
    "                        parts = line.split('#', 1)\n",
    "                    else:\n",
    "                        parts = line.split(',', 1)\n",
    "                        \n",
    "                    if len(parts) == 2:\n",
    "                        image_name, caption = parts\n",
    "                        # Since we don't have long/short distinction, mark as \"standard\"\n",
    "                        self.samples.append((image_name.strip(), caption.strip(), \"standard\"))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_name, caption, caption_type = self.samples[idx]\n",
    "        image_path = os.path.join(self.image_dir, image_name)\n",
    "        \n",
    "        # Load and preprocess the image\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            image = self.preprocess_fn(image)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {image_path}: {e}\")\n",
    "            # Return a random valid sample instead\n",
    "            return self.__getitem__(random.randint(0, len(self) - 1))\n",
    "        \n",
    "        return image, caption, caption_type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb56133f",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1e2117a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contrastive Loss Function\n",
    "def contrastive_loss(image_embeds, text_embeds, temperature=0.07):\n",
    "    # Normalize embeddings\n",
    "    image_embeds = F.normalize(image_embeds, dim=1)\n",
    "    text_embeds = F.normalize(text_embeds, dim=1)\n",
    "    \n",
    "    # Compute similarity matrix\n",
    "    logits = torch.matmul(image_embeds, text_embeds.T) / temperature\n",
    "    \n",
    "    # Labels are the positions of the positive pairs\n",
    "    labels = torch.arange(logits.size(0), device=logits.device)\n",
    "    \n",
    "    # Compute loss in both directions (image->text and text->image)\n",
    "    loss_i2t = F.cross_entropy(logits, labels)\n",
    "    loss_t2i = F.cross_entropy(logits.T, labels)\n",
    "    \n",
    "    return (loss_i2t + loss_t2i) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0e2873",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7353ab8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captions file not found: /Users/erenyavuz/Desktop/KU/25 Spring/COMP447/Project/Repo/FlightVision/data/captions.json\n"
     ]
    }
   ],
   "source": [
    "# Set up dataset and dataloader\n",
    "images_dir = os.path.join(BASE_DATA_DESTINATION, FLICKR8K_IMAGES_FOLDER_NAME)\n",
    "captions_file = os.path.join(BASE_DATA_DESTINATION, CAPTIONS_JSON_FILENAME)\n",
    "\n",
    "pull_from_json = True\n",
    "\n",
    "# Check if files exist\n",
    "if not os.path.exists(images_dir):\n",
    "    raise FileNotFoundError(f\"Images directory not found: {images_dir}\")\n",
    "if not os.path.exists(captions_file):\n",
    "    print(f\"Captions file not found: {captions_file}\")\n",
    "    pull_from_json = False\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = Flickr8kCaptionedDataset(images_dir, captions_file, preprocess, pull_from_json=pull_from_json)\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=0,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Training loop\n",
    "def train():\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        total_loss = 0.0\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "        \n",
    "        for batch_idx, (images, captions, caption_types) in enumerate(progress_bar):\n",
    "            images = images.to(DEVICE)\n",
    "            \n",
    "            # Tokenize the captions\n",
    "            tokenized_captions = tokenizer(captions).to(DEVICE)\n",
    "            \n",
    "            # Forward pass with mixed precision\n",
    "            with torch.cuda.amp.autocast():\n",
    "                image_features = model.encode_image(images)\n",
    "                text_features = model.encode_text(tokenized_captions)\n",
    "                \n",
    "                # Compute contrastive loss\n",
    "                loss = contrastive_loss(image_features, text_features)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update progress bar\n",
    "            total_loss += loss.item()\n",
    "            avg_loss = total_loss / (batch_idx + 1)\n",
    "            progress_bar.set_postfix(loss=f\"{avg_loss:.4f}\")\n",
    "            \n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"mobileclip_finetuned_epoch{epoch+1}.pt\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e5d0e0",
   "metadata": {},
   "source": [
    "## Running the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd246bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device: cpu\n",
      "Dataset size: 40456 samples\n",
      "Batch size: 32\n",
      "Learning rate: 0.0001\n",
      "Number of epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|          | 14/1264 [02:28<3:28:56, 10.03s/it, loss=0.7555]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading image /Users/erenyavuz/Desktop/KU/25 Spring/COMP447/Project/Repo/FlightVision/data/Images/2924483864_cfdb900a13.jpg,White dog with yellow and black: [Errno 2] No such file or directory: '/Users/erenyavuz/Desktop/KU/25 Spring/COMP447/Project/Repo/FlightVision/data/Images/2924483864_cfdb900a13.jpg,White dog with yellow and black'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▏         | 31/1264 [05:30<3:52:37, 11.32s/it, loss=0.6701]"
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "print(f\"Training on device: {DEVICE}\")\n",
    "print(f\"Dataset size: {len(dataset)} samples\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Number of epochs: {NUM_EPOCHS}\")\n",
    "\n",
    "train()\n",
    "\n",
    "# Evaluate model after training\n",
    "model.eval()\n",
    "\n",
    "# Example evaluation\n",
    "test_image_path = \"pngwing.com(1).png\"\n",
    "test_texts = [\"a purple dog\", \"a white dog\", \"a black dog\"]\n",
    "\n",
    "test_image = preprocess(Image.open(test_image_path).convert('RGB')).unsqueeze(0).to(DEVICE)\n",
    "test_text = tokenizer(test_texts).to(DEVICE)\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    image_features = model.encode_image(test_image)\n",
    "    text_features = model.encode_text(test_text)\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "torch.set_printoptions(sci_mode=False, precision=4)\n",
    "print(\"Label probabilities after training:\", text_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dc3e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "train_dataset = CustomDataset(cifar10_dataset, transform=preprocess)\n",
    "dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "\n",
    "# Replace `MobileCLIPModel()` with your Mobile CLIP implementation.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# 3. Contrastive Loss (InfoNCE)\n",
    "def contrastive_loss(image_embeds: torch.Tensor, text_embeds: torch.Tensor, temperature: float = 0.07) -> torch.Tensor:\n",
    "    # Normalize embeddings\n",
    "    image_embeds = F.normalize(image_embeds, dim=1)\n",
    "    text_embeds = F.normalize(text_embeds, dim=1)\n",
    "    # Compute logits\n",
    "    logits = image_embeds @ text_embeds.t() / temperature\n",
    "    labels = torch.arange(logits.size(0), device=logits.device)\n",
    "    loss_i2t = F.cross_entropy(logits, labels)\n",
    "    loss_t2i = F.cross_entropy(logits.t(), labels)\n",
    "    return (loss_i2t + loss_t2i) / 2\n",
    "\n",
    "# 4. Optimizer and training setup\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "epochs = 10\n",
    "\n",
    "# 5. Training loop\n",
    "def train():\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        for images, captions in tqdm(dataloader):\n",
    "            images = images.to(device)\n",
    "            captions = tokenizer(captions).to(device)\n",
    "            # Forward pass: user-provided model should return (image_embeds, text_embeds)\n",
    "            image_embeds = model.encode_image(images)\n",
    "            text_embeds = model.encode_text(captions)\n",
    "\n",
    "            # Compute contrastive loss\n",
    "            loss = contrastive_loss(image_embeds, text_embeds)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader.dataset)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e95ec6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30951f86870347c582b2fdb7552f3ad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62a1bf7037bd46cd916a9c19f9e61b8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/506 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6174cabc29ba4280b83640f055a1c0d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b23f5a312804b8ea249765e5c0557da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34e3b1d210a14ef0924e55e9564dfca0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "790122ce7a9a44f2ac9b6a6cf7b7f1bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.56k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a844e2ad098742c689f2a17bac2c8fa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b40ef3792b945aeb43c1ffdd4cfc5f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "The image to be converted to a PIL image contains values outside the range [0, 1], got [-2.1179039478302, 2.393557548522949] which cannot be converted to uint8.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m captions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images[:\u001b[38;5;241m100\u001b[39m]:  \u001b[38;5;66;03m# Increase this number as needed\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m processor(images\u001b[38;5;241m=\u001b[39mimage, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     31\u001b[0m         out \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/blip/processing_blip.py:110\u001b[0m, in \u001b[0;36mBlipProcessor.__call__\u001b[0;34m(self, images, text, audio, videos, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m     text_encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(text, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moutput_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 110\u001b[0m     encoding_image_processor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_processor(images, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moutput_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m text_encoding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m         encoding_image_processor\u001b[38;5;241m.\u001b[39mupdate(text_encoding)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/image_processing_utils.py:42\u001b[0m, in \u001b[0;36mBaseImageProcessor.__call__\u001b[0;34m(self, images, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchFeature:\n\u001b[1;32m     41\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(images, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/generic.py:866\u001b[0m, in \u001b[0;36mfilter_out_non_signature_kwargs.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         cls_prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    859\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    860\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following named arguments are not valid for `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcls_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    861\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and were ignored: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minvalid_kwargs_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m    863\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    864\u001b[0m     )\n\u001b[0;32m--> 866\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mvalid_kwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/blip/image_processing_blip.py:271\u001b[0m, in \u001b[0;36mBlipImageProcessor.preprocess\u001b[0;34m(self, images, do_resize, size, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, return_tensors, do_convert_rgb, data_format, input_data_format)\u001b[0m\n\u001b[1;32m    267\u001b[0m     input_data_format \u001b[38;5;241m=\u001b[39m infer_channel_dimension_format(images[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_resize:\n\u001b[1;32m    270\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 271\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresize(image\u001b[38;5;241m=\u001b[39mimage, size\u001b[38;5;241m=\u001b[39msize, resample\u001b[38;5;241m=\u001b[39mresample, input_data_format\u001b[38;5;241m=\u001b[39minput_data_format)\n\u001b[1;32m    272\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images\n\u001b[1;32m    273\u001b[0m     ]\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_rescale:\n\u001b[1;32m    276\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrescale(image\u001b[38;5;241m=\u001b[39mimage, scale\u001b[38;5;241m=\u001b[39mrescale_factor, input_data_format\u001b[38;5;241m=\u001b[39minput_data_format)\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images\n\u001b[1;32m    279\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/blip/image_processing_blip.py:150\u001b[0m, in \u001b[0;36mBlipImageProcessor.resize\u001b[0;34m(self, image, size, resample, data_format, input_data_format, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `size` dictionary must contain the keys `height` and `width`. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    149\u001b[0m output_size \u001b[38;5;241m=\u001b[39m (size[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheight\u001b[39m\u001b[38;5;124m\"\u001b[39m], size[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resize(\n\u001b[1;32m    151\u001b[0m     image,\n\u001b[1;32m    152\u001b[0m     size\u001b[38;5;241m=\u001b[39moutput_size,\n\u001b[1;32m    153\u001b[0m     resample\u001b[38;5;241m=\u001b[39mresample,\n\u001b[1;32m    154\u001b[0m     data_format\u001b[38;5;241m=\u001b[39mdata_format,\n\u001b[1;32m    155\u001b[0m     input_data_format\u001b[38;5;241m=\u001b[39minput_data_format,\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    157\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/image_transforms.py:368\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(image, size, resample, reducing_gap, data_format, return_numpy, input_data_format)\u001b[0m\n\u001b[1;32m    366\u001b[0m do_rescale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(image, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n\u001b[0;32m--> 368\u001b[0m     do_rescale \u001b[38;5;241m=\u001b[39m _rescale_for_pil_conversion(image)\n\u001b[1;32m    369\u001b[0m     image \u001b[38;5;241m=\u001b[39m to_pil_image(image, do_rescale\u001b[38;5;241m=\u001b[39mdo_rescale, input_data_format\u001b[38;5;241m=\u001b[39minput_data_format)\n\u001b[1;32m    370\u001b[0m height, width \u001b[38;5;241m=\u001b[39m size\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/image_transforms.py:150\u001b[0m, in \u001b[0;36m_rescale_for_pil_conversion\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m    148\u001b[0m     do_rescale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe image to be converted to a PIL image contains values outside the range [0, 1], \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] which cannot be converted to uint8.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    153\u001b[0m     )\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m do_rescale\n",
      "\u001b[0;31mValueError\u001b[0m: The image to be converted to a PIL image contains values outside the range [0, 1], got [-2.1179039478302, 2.393557548522949] which cannot be converted to uint8."
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Define the transformation pipeline\n",
    "# Remove transforms.ToPILImage() since CIFAR-10 images are already PIL images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to match model input size\n",
    "    transforms.ToTensor(),         # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize\n",
    "])\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "cifar10_dataset = datasets.CIFAR10(root='cifar10', train=True, download=True)\n",
    "\n",
    "# Apply the transformation pipeline to the images\n",
    "images = [transform(img) for img, _ in cifar10_dataset]\n",
    "\n",
    "# Load BLIP model and processor\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model.eval()\n",
    "\n",
    "# Generate captions for a subset (e.g., first 100 images for demo)\n",
    "captions = []\n",
    "for image in images[:100]:  # Increase this number as needed\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**inputs)\n",
    "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "    captions.append(caption)\n",
    "\n",
    "# Custom dataset combining image and generated caption\n",
    "class CIFAR10WithCaptions(Dataset):\n",
    "    def __init__(self, images, captions, transform=None):\n",
    "        self.images = images\n",
    "        self.captions = captions\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        caption = self.captions[idx]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, caption\n",
    "\n",
    "# Example usage\n",
    "transform_tensor = transforms.ToTensor()\n",
    "custom_dataset = CIFAR10WithCaptions(images[:100], captions, transform=transform_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd0b8a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CIFAR10WithCaptions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m      2\u001b[0m transform_tensor \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mToTensor()\n\u001b[0;32m----> 3\u001b[0m custom_dataset \u001b[38;5;241m=\u001b[39m CIFAR10WithCaptions(images[:\u001b[38;5;241m100\u001b[39m], captions, transform\u001b[38;5;241m=\u001b[39mtransform_tensor)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CIFAR10WithCaptions' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "transform_tensor = transforms.ToTensor()\n",
    "custom_dataset = CIFAR10WithCaptions(images[:100], captions, transform=transform_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b411e312",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'custom_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Get the first image and caption\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m img, caption \u001b[38;5;241m=\u001b[39m custom_dataset[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Convert the tensor image to a PIL image for plotting\u001b[39;00m\n\u001b[1;32m      8\u001b[0m img \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mto_pil_image(img)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'custom_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "# Get the first image and caption\n",
    "img, caption = custom_dataset[0]\n",
    "\n",
    "# Convert the tensor image to a PIL image for plotting\n",
    "img = F.to_pil_image(img)\n",
    "\n",
    "# Plot the image\n",
    "plt.imshow(img)\n",
    "plt.title(f\"Caption: {caption}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5183c22d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
