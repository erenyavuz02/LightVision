{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae579b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LIGHTVISION: MOBILE-FRIENDLY IMAGE RETRIEVAL SYSTEM\n",
      "============================================================\n",
      "Setting up environment and checking dependencies...\n",
      "Device: cpu\n",
      "Base data directory: /Users/erenyavuz/Desktop/KU/25 Spring/COMP447/Project/Repo/FlightVision/data\n",
      "Checkpoint directory: /Users/erenyavuz/Desktop/KU/25 Spring/COMP447/Project/Repo/FlightVision/checkpoints\n",
      "Environment setup complete!\n"
     ]
    }
   ],
   "source": [
    "# LightVision: Long-Text-Aware Offline Lightweight Text-to-Image Retrieval\n",
    "# Refactored and Organized Notebook\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 1: Project Setup and Environment Check\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LIGHTVISION: MOBILE-FRIENDLY IMAGE RETRIEVAL SYSTEM\")\n",
    "print(\"=\"*60)\n",
    "print(\"Setting up environment and checking dependencies...\")\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Check CUDA availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "\n",
    "# Project directories\n",
    "BASE_DATA_DESTINATION = os.path.join(os.getcwd(), \"data\")\n",
    "FLICKR8K_IMAGES_FOLDER_NAME = \"Images\"\n",
    "CAPTIONS_JSON_FILENAME = \"all_captions.json\"\n",
    "CHECKPOINT_DIR = os.path.join(os.getcwd(), \"checkpoints\")\n",
    "DEVICE = device\n",
    "\n",
    "print(f\"Base data directory: {BASE_DATA_DESTINATION}\")\n",
    "print(f\"Checkpoint directory: {CHECKPOINT_DIR}\")\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8b67427",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All dependencies imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 2: Install and Import Dependencies\n",
    "# ============================================================================\n",
    "\n",
    "# Install required packages (uncomment if needed)\n",
    "# !pip install torch torchvision timm open-clip-torch datasets clip-benchmark\n",
    "\n",
    "# Core imports\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Add LightVision to path\n",
    "sys.path.append('/content/LightVision')\n",
    "\n",
    "# MobileCLIP imports\n",
    "import mobileclip\n",
    "\n",
    "print(\"All dependencies imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18888776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 3: Configuration and Parameters\n",
    "# ============================================================================\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Centralized configuration class\"\"\"\n",
    "    \n",
    "    # Model configuration\n",
    "    MODEL_NAME = 'mobileclip_s0'\n",
    "    DEVICE = DEVICE\n",
    "    EMBEDDING_DIM = 512\n",
    "    \n",
    "    # Training configuration\n",
    "    BATCH_SIZE = 128\n",
    "    LEARNING_RATE = 1e-4\n",
    "    NUM_EPOCHS = 1\n",
    "    NUM_WORKERS = 0\n",
    "    \n",
    "    # Data configuration\n",
    "    BASE_DATA_DIR = BASE_DATA_DESTINATION\n",
    "    IMAGES_DIR = os.path.join(BASE_DATA_DIR, FLICKR8K_IMAGES_FOLDER_NAME)\n",
    "    CAPTIONS_FILE = os.path.join(BASE_DATA_DIR, CAPTIONS_JSON_FILENAME)\n",
    "    NEW_CAPTIONS_FILE = os.path.join(BASE_DATA_DIR, \"new_file.json\")\n",
    "    \n",
    "    # Checkpoint configuration\n",
    "    CHECKPOINT_DIR = CHECKPOINT_DIR\n",
    "    BASE_MODEL_PATH = os.path.join(CHECKPOINT_DIR, 'mobileclip_s0.pt')\n",
    "    FINETUNED_MODEL_PATH = os.path.join(CHECKPOINT_DIR, 'mobileclip_finetuned_epoch1_last.pt')\n",
    "    \n",
    "    # Positional embedding configuration\n",
    "    LAMBDA2 = 4  # Interpolation parameter\n",
    "    \n",
    "    # Loss configuration\n",
    "    TEMPERATURE = 0.07\n",
    "    PCA_DIM = 32\n",
    "\n",
    "# Print configuration\n",
    "config = Config()\n",
    "print(\"Configuration loaded:\")\n",
    "for attr in dir(config):\n",
    "    if not attr.startswith('_'):\n",
    "        print(f\"  {attr}: {getattr(config, attr)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d47f0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 4: Download Base Model and Setup Directories\n",
    "# ============================================================================\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs(config.CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(config.BASE_DATA_DIR, exist_ok=True)\n",
    "\n",
    "# Download base model if not exists\n",
    "if not os.path.exists(config.BASE_MODEL_PATH):\n",
    "    print(\"Downloading base MobileCLIP model...\")\n",
    "    !wget https://docs-assets.developer.apple.com/ml-research/datasets/mobileclip/mobileclip_s0.pt -P {config.CHECKPOINT_DIR}\n",
    "    print(\"Base model downloaded successfully!\")\n",
    "else:\n",
    "    print(\"Base model already exists.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484cc5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 5: Model Loading and Setup Functions\n",
    "# ============================================================================\n",
    "\n",
    "def load_base_model(model_name=config.MODEL_NAME, checkpoint_path=None):\n",
    "    \"\"\"Load the base MobileCLIP model\"\"\"\n",
    "    print(f\"Loading {model_name} model...\")\n",
    "    \n",
    "    if checkpoint_path and os.path.exists(checkpoint_path):\n",
    "        print(f\"Loading from checkpoint: {checkpoint_path}\")\n",
    "        model, _, preprocess = mobileclip.create_model_and_transforms(\n",
    "            model_name, pretrained=checkpoint_path\n",
    "        )\n",
    "    else:\n",
    "        print(\"Loading base pretrained model...\")\n",
    "        model, _, preprocess = mobileclip.create_model_and_transforms(\n",
    "            model_name, pretrained=config.BASE_MODEL_PATH\n",
    "        )\n",
    "    \n",
    "    model.to(config.DEVICE)\n",
    "    tokenizer = mobileclip.get_tokenizer(model_name)\n",
    "    \n",
    "    print(f\"Model loaded successfully on {config.DEVICE}\")\n",
    "    return model, preprocess, tokenizer\n",
    "\n",
    "def apply_positional_embedding_interpolation(model, lambda2=config.LAMBDA2):\n",
    "    \"\"\"Apply Knowledge-Preserving Stretching for positional embeddings\"\"\"\n",
    "    print(f\"Applying positional embedding interpolation with λ={lambda2}\")\n",
    "    \n",
    "    pos_embed = model.text_encoder.get_positional_embedding().pos_embed.pos_embed\n",
    "    if pos_embed is None:\n",
    "        raise ValueError(\"Positional embedding not found in text encoder.\")\n",
    "    \n",
    "    max_pos, embed_dim = pos_embed.shape[2], pos_embed.shape[3]\n",
    "    modified_pos_embed = torch.zeros((1, 1, max_pos, embed_dim), device=pos_embed.device)\n",
    "    \n",
    "    for pos in range(max_pos):\n",
    "        if pos <= 20:\n",
    "            # Preserve first 20 positions (most informative)\n",
    "            modified_pos_embed[:, :, pos, :] = pos_embed[:, :, pos, :]\n",
    "        else:\n",
    "            # Interpolate remaining positions\n",
    "            lower_idx = pos // lambda2\n",
    "            upper_idx = min(lower_idx + 1, max_pos - 1)\n",
    "            alpha = (pos % lambda2) / lambda2\n",
    "            modified_pos_embed[:, :, pos, :] = (\n",
    "                (1 - alpha) * pos_embed[:, :, lower_idx, :] + \n",
    "                alpha * pos_embed[:, :, upper_idx, :]\n",
    "            )\n",
    "    \n",
    "    # Update model with new positional embeddings\n",
    "    modified_pos_embed = torch.nn.Parameter(modified_pos_embed, requires_grad=False)\n",
    "    model.text_encoder.get_positional_embedding().pos_embed.pos_embed = modified_pos_embed\n",
    "    \n",
    "    print(\"Positional embedding interpolation applied successfully\")\n",
    "    return model\n",
    "\n",
    "def print_model_info(model):\n",
    "    \"\"\"Print model information\"\"\"\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"\\nModel Information:\")\n",
    "    print(f\"  Device: {config.DEVICE}\")\n",
    "    print(f\"  Total parameters: {total_params:,}\")\n",
    "    print(f\"  Trainable parameters: {trainable_params:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f584cecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 6: Load and Configure Base Model\n",
    "# ============================================================================\n",
    "\n",
    "# Load base model\n",
    "model, preprocess, tokenizer = load_base_model()\n",
    "\n",
    "# Apply positional embedding interpolation\n",
    "model = apply_positional_embedding_interpolation(model)\n",
    "\n",
    "# Print model information\n",
    "print_model_info(model)\n",
    "\n",
    "# Set model to evaluation mode initially\n",
    "model.eval()\n",
    "\n",
    "print(\"Base model loaded and configured successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77db75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 7: Test Base Model Performance\n",
    "# ============================================================================\n",
    "\n",
    "def test_model_inference(model, preprocess, tokenizer, test_texts=None, test_image_path=None):\n",
    "    \"\"\"Test model inference capabilities\"\"\"\n",
    "    print(\"Testing model inference...\")\n",
    "    \n",
    "    if test_texts is None:\n",
    "        test_texts = [\n",
    "            \"The lemon on the left is yellow and the eggplant on the right is purple.\",\n",
    "            \"The lemon on the left is purple and the eggplant on the right is yellow.\",\n",
    "            \"The lemon on the right is yellow and the eggplant on the left is purple.\",\n",
    "            \"The lemon on the right is purple and the eggplant on the left is yellow\"\n",
    "        ]\n",
    "    \n",
    "    # Test text encoding\n",
    "    text_tokens = tokenizer(test_texts).to(config.DEVICE)\n",
    "    \n",
    "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "        text_features = model.encode_text(text_tokens)\n",
    "        text_features = F.normalize(text_features, dim=-1)\n",
    "        \n",
    "    print(f\"Text encoding successful: {text_features.shape}\")\n",
    "    \n",
    "    # Test image encoding if image provided\n",
    "    if test_image_path and os.path.exists(test_image_path):\n",
    "        image = preprocess(Image.open(test_image_path).convert('RGB')).unsqueeze(0)\n",
    "        image = image.to(config.DEVICE)\n",
    "        \n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            image_features = model.encode_image(image.half())\n",
    "            image_features = F.normalize(image_features, dim=-1)\n",
    "            \n",
    "            # Calculate similarities\n",
    "            text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "        \n",
    "        print(f\"Image encoding successful: {image_features.shape}\")\n",
    "        print(f\"Text probabilities: {text_probs}\")\n",
    "        \n",
    "        return text_probs\n",
    "    \n",
    "    return text_features\n",
    "\n",
    "# Test the base model\n",
    "print(\"Testing base model performance...\")\n",
    "test_model_inference(model, preprocess, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8d540b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# UPDATED CELL 8: Data Loading and Dataset Classes with Train/Test Split\n",
    "# ============================================================================\n",
    "\n",
    "class Flickr8kCaptionedDataset(Dataset):\n",
    "    \"\"\"Dataset class for Flickr8k with custom captions\"\"\"\n",
    "    \n",
    "    def __init__(self, image_dir, captions_file, preprocess_fn, pull_from_json=True):\n",
    "        self.image_dir = image_dir\n",
    "        self.preprocess_fn = preprocess_fn\n",
    "        self.samples = []\n",
    "        \n",
    "        print(f\"Loading dataset from {captions_file}...\")\n",
    "        \n",
    "        if pull_from_json:\n",
    "            self._load_from_json(captions_file)\n",
    "        else:\n",
    "            self._load_from_txt(captions_file)\n",
    "        \n",
    "        print(f\"Loaded {len(self.samples)} samples\")\n",
    "    \n",
    "    def _load_from_json(self, captions_file):\n",
    "        \"\"\"Load captions from JSON file\"\"\"\n",
    "        with open(captions_file, 'r') as f:\n",
    "            captions_data = json.load(f)\n",
    "        \n",
    "        for image_name, captions in captions_data.items():\n",
    "            image_path = os.path.join(self.image_dir, image_name)\n",
    "            if not os.path.exists(image_path):\n",
    "                continue\n",
    "            \n",
    "            if isinstance(captions, dict):\n",
    "                # Handle both LLaVA format and standard format\n",
    "                if \"long_caption\" in captions and \"short_caption\" in captions:\n",
    "                    self.samples.append((\n",
    "                        image_name, \n",
    "                        captions[\"short_caption\"], \n",
    "                        captions[\"long_caption\"]\n",
    "                    ))\n",
    "                elif \"long_detailed\" in captions and \"short_caption\" in captions:\n",
    "                    # Handle LLaVA format directly\n",
    "                    self.samples.append((\n",
    "                        image_name, \n",
    "                        captions[\"short_caption\"], \n",
    "                        captions[\"long_detailed\"]\n",
    "                    ))\n",
    "            else:\n",
    "                self.samples.append((image_name, captions, \"standard\"))\n",
    "    \n",
    "    def _load_from_txt(self, captions_file):\n",
    "        \"\"\"Load captions from text file\"\"\"\n",
    "        with open(captions_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            if '#' in line:\n",
    "                parts = line.split('#', 1)\n",
    "            else:\n",
    "                parts = line.split(',', 1)\n",
    "            \n",
    "            if len(parts) == 2:\n",
    "                image_name, caption = parts\n",
    "                image_path = os.path.join(self.image_dir, image_name.strip())\n",
    "                if os.path.exists(image_path):\n",
    "                    self.samples.append((image_name.strip(), caption.strip(), \"standard\"))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_name, caption, caption_type = self.samples[idx]\n",
    "        image_path = os.path.join(self.image_dir, image_name)\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            image = self.preprocess_fn(image)\n",
    "            return image, caption, caption_type\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {image_path}: {e}\")\n",
    "            return self.__getitem__(random.randint(0, len(self) - 1))\n",
    "\n",
    "def create_train_test_split(llava_captions_file, images_dir, output_dir, test_ratio=0.125):\n",
    "    \"\"\"\n",
    "    Create train/test split from LLaVA captions\n",
    "    \n",
    "    Args:\n",
    "        llava_captions_file: Path to LLaVA generated captions\n",
    "        images_dir: Directory containing images\n",
    "        output_dir: Output directory for split files\n",
    "        test_ratio: Ratio for test set (0.125 = 1k out of 8k)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with file paths\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    print(\"CREATING TRAIN/TEST SPLIT\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Load LLaVA captions\n",
    "    print(f\"Loading captions from: {llava_captions_file}\")\n",
    "    with open(llava_captions_file, 'r') as f:\n",
    "        llava_data = json.load(f)\n",
    "    \n",
    "    print(f\"Found {len(llava_data)} image-caption pairs\")\n",
    "    \n",
    "    # Clean and filter existing images\n",
    "    cleaned_data = {}\n",
    "    for image_name, captions in llava_data.items():\n",
    "        image_path = os.path.join(images_dir, image_name)\n",
    "        if not os.path.exists(image_path):\n",
    "            continue\n",
    "        \n",
    "        # Standardize format\n",
    "        if isinstance(captions, dict):\n",
    "            # Convert LLaVA format to standard format\n",
    "            if 'short_caption' in captions and 'long_detailed' in captions:\n",
    "                cleaned_data[image_name] = {\n",
    "                    'short_caption': captions['short_caption'].strip(),\n",
    "                    'long_caption': captions['long_detailed'].strip()  # Rename for consistency\n",
    "                }\n",
    "            elif 'short_caption' in captions and 'long_caption' in captions:\n",
    "                cleaned_data[image_name] = {\n",
    "                    'short_caption': captions['short_caption'].strip(),\n",
    "                    'long_caption': captions['long_caption'].strip()\n",
    "                }\n",
    "    \n",
    "    print(f\"Cleaned data: {len(cleaned_data)} valid image-caption pairs\")\n",
    "    \n",
    "    # Analyze caption statistics\n",
    "    short_lengths = [len(caps['short_caption'].split()) for caps in cleaned_data.values()]\n",
    "    long_lengths = [len(caps['long_caption'].split()) for caps in cleaned_data.values()]\n",
    "    \n",
    "    print(f\"Caption statistics:\")\n",
    "    print(f\"  Short captions: avg {sum(short_lengths)/len(short_lengths):.1f} words (range: {min(short_lengths)}-{max(short_lengths)})\")\n",
    "    print(f\"  Long captions: avg {sum(long_lengths)/len(long_lengths):.1f} words (range: {min(long_lengths)}-{max(long_lengths)})\")\n",
    "    \n",
    "    # Create random split\n",
    "    random.seed(42)  # For reproducible splits\n",
    "    all_images = list(cleaned_data.keys())\n",
    "    random.shuffle(all_images)\n",
    "    \n",
    "    # Calculate split sizes\n",
    "    total_images = len(all_images)\n",
    "    test_size = int(total_images * test_ratio)\n",
    "    train_size = total_images - test_size\n",
    "    \n",
    "    print(f\"\\nSplit configuration:\")\n",
    "    print(f\"  Total images: {total_images}\")\n",
    "    print(f\"  Training: {train_size} images ({(train_size/total_images)*100:.1f}%)\")\n",
    "    print(f\"  Testing: {test_size} images ({(test_size/total_images)*100:.1f}%)\")\n",
    "    \n",
    "    # Create splits\n",
    "    test_images = all_images[:test_size]\n",
    "    train_images = all_images[test_size:]\n",
    "    \n",
    "    train_data = {img: cleaned_data[img] for img in train_images}\n",
    "    test_data = {img: cleaned_data[img] for img in test_images}\n",
    "    \n",
    "    # Save split files\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    train_file = os.path.join(output_dir, \"train_captions.json\")\n",
    "    test_file = os.path.join(output_dir, \"test_captions.json\")\n",
    "    \n",
    "    with open(train_file, 'w') as f:\n",
    "        json.dump(train_data, f, indent=2)\n",
    "    \n",
    "    with open(test_file, 'w') as f:\n",
    "        json.dump(test_data, f, indent=2)\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        'split_info': {\n",
    "            'seed': 42,\n",
    "            'total_images': total_images,\n",
    "            'train_size': train_size,\n",
    "            'test_size': test_size,\n",
    "            'test_ratio': test_ratio\n",
    "        },\n",
    "        'caption_stats': {\n",
    "            'short_caption_avg_length': sum(short_lengths)/len(short_lengths),\n",
    "            'long_caption_avg_length': sum(long_lengths)/len(long_lengths)\n",
    "        },\n",
    "        'files': {\n",
    "            'original_llava_captions': os.path.basename(llava_captions_file),\n",
    "            'train_captions': 'train_captions.json',\n",
    "            'test_captions': 'test_captions.json'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    metadata_file = os.path.join(output_dir, \"split_metadata.json\")\n",
    "    with open(metadata_file, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nFiles created:\")\n",
    "    print(f\"  Training data: {train_file}\")\n",
    "    print(f\"  Test data: {test_file}\")\n",
    "    print(f\"  Metadata: {metadata_file}\")\n",
    "    \n",
    "    return {\n",
    "        'train_file': train_file,\n",
    "        'test_file': test_file,\n",
    "        'metadata_file': metadata_file,\n",
    "        'train_size': train_size,\n",
    "        'test_size': test_size\n",
    "    }\n",
    "\n",
    "def check_data_availability():\n",
    "    \"\"\"Check what data is available and create splits if needed\"\"\"\n",
    "    print(\"Checking data availability...\")\n",
    "    \n",
    "    # Check images\n",
    "    images_exist = os.path.exists(config.IMAGES_DIR)\n",
    "    if images_exist:\n",
    "        image_files = [f for f in os.listdir(config.IMAGES_DIR) \n",
    "                      if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]\n",
    "        image_count = len(image_files)\n",
    "    else:\n",
    "        image_count = 0\n",
    "    \n",
    "    # Check for different caption file formats\n",
    "    llava_captions = os.path.join(config.BASE_DATA_DIR, \"captions_database.json\")\n",
    "    train_captions = os.path.join(config.BASE_DATA_DIR, \"train_captions.json\")\n",
    "    test_captions = os.path.join(config.BASE_DATA_DIR, \"test_captions.json\")\n",
    "    \n",
    "    files_status = {\n",
    "        'images_dir': images_exist,\n",
    "        'image_count': image_count,\n",
    "        'llava_captions': os.path.exists(llava_captions),\n",
    "        'train_captions': os.path.exists(train_captions),\n",
    "        'test_captions': os.path.exists(test_captions)\n",
    "    }\n",
    "    \n",
    "    print(f\"Data Status:\")\n",
    "    print(f\"  Images directory: {config.IMAGES_DIR} ({'✓' if images_exist else '✗'})\")\n",
    "    print(f\"  Image count: {image_count}\")\n",
    "    print(f\"  LLaVA captions: {'✓' if files_status['llava_captions'] else '✗'}\")\n",
    "    print(f\"  Training split: {'✓' if files_status['train_captions'] else '✗'}\")\n",
    "    print(f\"  Test split: {'✓' if files_status['test_captions'] else '✗'}\")\n",
    "    \n",
    "    # Create splits if LLaVA data exists but splits don't\n",
    "    if (files_status['llava_captions'] and \n",
    "        not (files_status['train_captions'] and files_status['test_captions'])):\n",
    "        \n",
    "        print(f\"\\nLLaVA captions found but splits missing. Creating train/test split...\")\n",
    "        split_result = create_train_test_split(\n",
    "            llava_captions_file=llava_captions,\n",
    "            images_dir=config.IMAGES_DIR,\n",
    "            output_dir=config.BASE_DATA_DIR,\n",
    "            test_ratio=0.125  # 1k test, 7k train\n",
    "        )\n",
    "        \n",
    "        # Update status\n",
    "        files_status['train_captions'] = True\n",
    "        files_status['test_captions'] = True\n",
    "        files_status['split_result'] = split_result\n",
    "    \n",
    "    return files_status\n",
    "\n",
    "# Check data availability and create splits if needed\n",
    "data_status = check_data_availability()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf433a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 9: Loss Functions and Training Utilities\n",
    "# ============================================================================\n",
    "\n",
    "def PCA(input_tensor, PCA_dim=config.PCA_DIM):\n",
    "    \"\"\"Apply PCA for dimensionality reduction\"\"\"\n",
    "    # Calculate mean\n",
    "    mean = torch.mean(input_tensor, dim=0)\n",
    "    # Center the data\n",
    "    X_centered = input_tensor - mean.unsqueeze(0)\n",
    "    X_centered = X_centered.float()\n",
    "    \n",
    "    # Use SVD for numerical stability\n",
    "    U, S, Vt = torch.linalg.svd(X_centered, full_matrices=False)\n",
    "    principal_components = Vt.T[:, :PCA_dim]\n",
    "    \n",
    "    # Transform and reconstruct\n",
    "    X_transformed = torch.mm(X_centered, principal_components)\n",
    "    X_reversed = torch.mm(X_transformed, principal_components.T)\n",
    "    X_reversed += mean\n",
    "    \n",
    "    return X_reversed\n",
    "\n",
    "def single_loss(image_embeds, text_embeds, temperature=config.TEMPERATURE):\n",
    "    \"\"\"Standard contrastive loss function\"\"\"\n",
    "    # Normalize embeddings\n",
    "    image_embeds = F.normalize(image_embeds, dim=1)\n",
    "    text_embeds = F.normalize(text_embeds, dim=1)\n",
    "    \n",
    "    # Compute similarity matrix\n",
    "    logits = torch.matmul(image_embeds, text_embeds.T) / temperature\n",
    "    \n",
    "    # Labels are the positions of the positive pairs\n",
    "    labels = torch.arange(logits.size(0), device=logits.device)\n",
    "    \n",
    "    # Compute loss in both directions\n",
    "    loss_i2t = F.cross_entropy(logits, labels)\n",
    "    loss_t2i = F.cross_entropy(logits.T, labels)\n",
    "    \n",
    "    return (loss_i2t + loss_t2i) / 2\n",
    "\n",
    "def long_clip_loss(image_embedding, long_embedding, short_embedding):\n",
    "    \"\"\"LongCLIP-style loss with PCA-based dual alignment\"\"\"\n",
    "    # Normalize features\n",
    "    image_features_long = F.normalize(image_embedding, dim=1)\n",
    "    text_features_long = F.normalize(long_embedding, dim=1)\n",
    "    text_features_short = F.normalize(short_embedding, dim=1)\n",
    "    \n",
    "    # Apply PCA to get compressed image features\n",
    "    image_features_short = PCA(image_features_long, config.PCA_DIM)\n",
    "    image_features_short = F.normalize(image_features_short, dim=1)\n",
    "    \n",
    "    # Calculate similarity matrices\n",
    "    sim_i2tl = torch.matmul(image_features_long, text_features_long.T)\n",
    "    sim_tl2i = sim_i2tl.T\n",
    "    sim_i2ts = torch.matmul(image_features_short, text_features_short.T)\n",
    "    sim_ts2i = sim_i2ts.T\n",
    "    \n",
    "    # Apply temperature scaling\n",
    "    if hasattr(model, 'logit_scale'):\n",
    "        logit_scale = model.logit_scale.exp()\n",
    "        sim_i2tl = logit_scale * sim_i2tl\n",
    "        sim_tl2i = logit_scale * sim_tl2i\n",
    "        sim_i2ts = logit_scale * sim_i2ts\n",
    "        sim_ts2i = logit_scale * sim_ts2i\n",
    "    \n",
    "    # Create targets\n",
    "    bs = image_embedding.size(0)\n",
    "    targets = torch.arange(bs, device=image_embedding.device)\n",
    "    \n",
    "    # Calculate losses\n",
    "    loss_itcl = (\n",
    "        F.cross_entropy(sim_i2tl, targets, label_smoothing=0.1) +\n",
    "        F.cross_entropy(sim_tl2i, targets, label_smoothing=0.1)\n",
    "    ) / 2\n",
    "    \n",
    "    loss_itcs = (\n",
    "        F.cross_entropy(sim_i2ts, targets, label_smoothing=0.1) +\n",
    "        F.cross_entropy(sim_ts2i, targets, label_smoothing=0.1)\n",
    "    ) / 2\n",
    "    \n",
    "    total_loss = (loss_itcl + loss_itcs) / 2\n",
    "    return total_loss\n",
    "\n",
    "print(\"Loss functions defined successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37cbdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 10: Training Function\n",
    "# ============================================================================\n",
    "\n",
    "def train_model(config, images_dir, captions_file,\n",
    "                long_clip_loss_fn=None, single_loss_fn=None):\n",
    "    \"\"\"Train the model with given parameters\"\"\"\n",
    "    \n",
    "    print(f\"Starting training with {config.NUM_EPOCHS} epochs...\")\n",
    "    \n",
    "    # Check data availability\n",
    "    if not os.path.exists(images_dir):\n",
    "        raise FileNotFoundError(f\"Images directory not found: {images_dir}\")\n",
    "    if not os.path.exists(captions_file):\n",
    "        print(f\"Captions file not found: {captions_file}\")\n",
    "        pull_from_json = False\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = Flickr8kCaptionedDataset(images_dir, captions_file, preprocess, pull_from_json)\n",
    "    dataloader = DataLoader(dataset, batch_size=config.BATCH_SIZE, shuffle=True, \n",
    "                          num_workers=config.NUM_WORKERS, drop_last=True)\n",
    "    \n",
    "    # Setup optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.LEARNING_RATE)\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(config.NUM_EPOCHS):\n",
    "        total_loss = 0.0\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{config.NUM_EPOCHS}\")\n",
    "        \n",
    "        for batch_idx, (images, captions, caption_types) in enumerate(progress_bar):\n",
    "            images = images.to(config.DEVICE)\n",
    "            tokenized_captions = tokenizer(captions).to(config.DEVICE)\n",
    "            \n",
    "            # Forward pass\n",
    "            with torch.cuda.amp.autocast():\n",
    "                image_features = model.encode_image(images)\n",
    "                text_features = model.encode_text(tokenized_captions)\n",
    "                \n",
    "                # Choose loss function\n",
    "                if (long_clip_loss_fn is not None and \n",
    "                    any(ct != 'standard' for ct in caption_types)):\n",
    "                    # Use long_clip_loss if available and appropriate\n",
    "                    loss = long_clip_loss_fn(image_features, text_features, text_features)\n",
    "                else:\n",
    "                    # Use single loss\n",
    "                    if single_loss_fn is None:\n",
    "                        raise ValueError(\"Single loss function must be provided\")\n",
    "                    loss = single_loss_fn(image_features, text_features)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update progress\n",
    "            total_loss += loss.item()\n",
    "            avg_loss = total_loss / (batch_idx + 1)\n",
    "            progress_bar.set_postfix(loss=f\"{avg_loss:.4f}\")\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{config.NUM_EPOCHS}, Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        checkpoint_path = os.path.join(config.CHECKPOINT_DIR, f\"mobileclip_finetuned_epoch{epoch+1}_last.pt\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"Training function ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca8f522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# UPDATED CELL 11: Run Training with Proper Train Split\n",
    "# ============================================================================\n",
    "\n",
    "# Check if we have proper training data\n",
    "if data_status['train_captions'] and data_status['image_count'] > 0:\n",
    "    print(\"=\"*60)\n",
    "    print(\"TRAINING WITH PROPER TRAIN/TEST SPLIT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Use training split for training\n",
    "    train_captions_file = os.path.join(config.BASE_DATA_DIR, \"train_captions.json\")\n",
    "    \n",
    "    print(f\"Training with: {train_captions_file}\")\n",
    "    \n",
    "    # Load training metadata to show split info\n",
    "    metadata_file = os.path.join(config.BASE_DATA_DIR, \"split_metadata.json\")\n",
    "    if os.path.exists(metadata_file):\n",
    "        with open(metadata_file, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        print(f\"Training set size: {metadata['split_info']['train_size']} images\")\n",
    "        print(f\"Test set size: {metadata['split_info']['test_size']} images\")\n",
    "        print(f\"Test ratio: {metadata['split_info']['test_ratio']:.1%}\")\n",
    "    \n",
    "    # Train the model on training split only\n",
    "    trained_model = train_model(\n",
    "        config=config,\n",
    "        images_dir=config.IMAGES_DIR,\n",
    "        captions_file=train_captions_file,  # Use training split\n",
    "        pull_from_json=True,\n",
    "        long_clip_loss_fn=long_clip_loss,\n",
    "        single_loss_fn=single_loss\n",
    "    )\n",
    "    \n",
    "    print(\"✓ Training completed on training split!\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Training data not available. Please ensure you have:\")\n",
    "    print(\"1. LLaVA captions file (captions_database.json)\")\n",
    "    print(\"2. Images in the data/Images/ directory\")\n",
    "    if not data_status['llava_captions']:\n",
    "        print(\"\\nTo generate LLaVA captions, run your caption generation script first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ec28ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 12: Load Trained Model (if available)\n",
    "# ============================================================================\n",
    "\n",
    "def load_trained_model(checkpoint_path=config.FINETUNED_MODEL_PATH):\n",
    "    \"\"\"Load the trained model from checkpoint\"\"\"\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(f\"Loading trained model from: {checkpoint_path}\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=config.DEVICE)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(\"Trained model loaded successfully!\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"No trained model found at: {checkpoint_path}\")\n",
    "        print(\"Using base model for testing...\")\n",
    "        return False\n",
    "\n",
    "# Try to load trained model\n",
    "model_loaded = load_trained_model()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b058bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# UPDATED CELL 13: Model Testing on Test Set (Not Training Set)\n",
    "# ============================================================================\n",
    "\n",
    "def evaluate_on_test_set(model, preprocess, tokenizer, sample_size=5):\n",
    "    \"\"\"Evaluate model on the reserved test set\"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"EVALUATION ON TEST SET\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    test_captions_file = os.path.join(config.BASE_DATA_DIR, \"test_captions.json\")\n",
    "    \n",
    "    if not os.path.exists(test_captions_file):\n",
    "        print(\"❌ Test set not found. Please ensure train/test split was created.\")\n",
    "        return None\n",
    "    \n",
    "    # Load test data\n",
    "    with open(test_captions_file, 'r') as f:\n",
    "        test_data = json.load(f)\n",
    "    \n",
    "    print(f\"Test set contains {len(test_data)} images\")\n",
    "    print(f\"Running detailed evaluation on {sample_size} sample images...\")\n",
    "    \n",
    "    # Sample images for evaluation\n",
    "    test_images = list(test_data.keys())\n",
    "    random.seed(42)  # For reproducible sampling\n",
    "    sample_images = random.sample(test_images, min(sample_size, len(test_images)))\n",
    "    \n",
    "    results = []\n",
    "    model.eval()\n",
    "    \n",
    "    for i, image_name in enumerate(sample_images): # Iterate through the sampled images\n",
    "        try:\n",
    "            image_path = os.path.join(config.IMAGES_DIR, image_name) # Construct the full image path\n",
    "            image = preprocess(Image.open(image_path).convert('RGB')).unsqueeze(0).to(config.DEVICE) # Open, convert to RGB, preprocess, add batch dimension, and move image to device\n",
    "            \n",
    "            captions = test_data[image_name] # Retrieve captions for the current image\n",
    "            short_caption = captions['short_caption'] # Extract the short caption\n",
    "            long_caption = captions['long_caption'] # Extract the long caption\n",
    "            \n",
    "            # Test both caption types\n",
    "            test_texts = [short_caption, long_caption] # Combine short and long captions into a list\n",
    "            text_tokens = tokenizer(test_texts).to(config.DEVICE) # Tokenize the captions and move them to the device\n",
    "            \n",
    "            with torch.no_grad(), torch.cuda.amp.autocast(): # Disable gradient calculation and enable mixed precision for inference\n",
    "                image_features = model.encode_image(image) # Encode the image into a feature vector\n",
    "                text_features = model.encode_text(text_tokens) # Encode the text into a feature vector\n",
    "                \n",
    "                image_features = F.normalize(image_features, dim=-1) # Normalize the image features\n",
    "                text_features = F.normalize(text_features, dim=-1) # Normalize the text features\n",
    "                \n",
    "                #TODO: Here the improvement is calculated as the difference in cosine similarity between the long and short captions,\n",
    "                # but the same number of tokens considered for both captions and rest is truncated. Longer captions must be captured with the proposed method for the improvement to be meaningful. \n",
    "                similarities = (100.0 * image_features @ text_features.T).squeeze() # Calculate cosine similarity between image and text features\n",
    "            \n",
    "            result = {\n",
    "                'image_name': image_name, # Store the image name\n",
    "                'short_caption': short_caption, # Store the short caption\n",
    "                'long_caption': long_caption, # Store the long caption\n",
    "                'short_similarity': float(similarities[0]), # Store the similarity score for the short caption\n",
    "                'long_similarity': float(similarities[1]), # Store the similarity score for the long caption\n",
    "                'short_words': len(short_caption.split()), # Count the number of words in the short caption\n",
    "                'long_words': len(long_caption.split()), # Count the number of words in the long caption\n",
    "                'improvement': float(similarities[1] - similarities[0]) # Calculate the improvement in similarity from short to long caption\n",
    "            }\n",
    "            \n",
    "            results.append(result) # Append the result to the list of results\n",
    "            \n",
    "            print(f\"\\nTest {i+1}/{len(sample_images)}: {image_name}\") # Print the test image number and name\n",
    "            print(f\"  Short caption ({result['short_words']} words): {result['short_similarity']:.4f}\") # Print the short caption similarity and word count\n",
    "            print(f\"  Long caption ({result['long_words']} words): {result['long_similarity']:.4f}\") # Print the long caption similarity and word count\n",
    "            print(f\"  Improvement: {result['improvement']:+.4f}\") # Print the improvement in similarity\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image_name}: {e}\") # Handle any errors during processing\n",
    "    \n",
    "    # Calculate aggregate statistics\n",
    "    if results:\n",
    "        print(f\"\\n\" + \"=\"*50)\n",
    "        print(\"TEST SET EVALUATION SUMMARY\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        short_similarities = [r['short_similarity'] for r in results]\n",
    "        long_similarities = [r['long_similarity'] for r in results]\n",
    "        improvements = [r['improvement'] for r in results]\n",
    "        short_word_counts = [r['short_words'] for r in results]\n",
    "        long_word_counts = [r['long_words'] for r in results]\n",
    "        \n",
    "        print(f\"Results based on {len(results)} test images:\")\n",
    "        print(f\"\\nShort Captions:\")\n",
    "        print(f\"  Average similarity: {sum(short_similarities)/len(short_similarities):.4f}\")\n",
    "        print(f\"  Average length: {sum(short_word_counts)/len(short_word_counts):.1f} words\")\n",
    "        \n",
    "        print(f\"\\nLong Captions:\")\n",
    "        print(f\"  Average similarity: {sum(long_similarities)/len(long_similarities):.4f}\")\n",
    "        print(f\"  Average length: {sum(long_word_counts)/len(long_word_counts):.1f} words\")\n",
    "        \n",
    "        print(f\"\\nLong vs Short Analysis:\")\n",
    "        avg_improvement = sum(improvements) / len(improvements)\n",
    "        positive_improvements = sum(1 for imp in improvements if imp > 0)\n",
    "        print(f\"  Average improvement: {avg_improvement:+.4f}\")\n",
    "        print(f\"  Long captions better: {positive_improvements}/{len(improvements)} ({positive_improvements/len(improvements)*100:.1f}%)\")\n",
    "        \n",
    "        if avg_improvement > 0:\n",
    "            print(f\"  ✓ Model shows improvement on longer, detailed captions\")\n",
    "        else:\n",
    "            print(f\"  ⚠ Model performance decreased on longer captions\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def compare_base_vs_finetuned_on_test():\n",
    "    \"\"\"Compare base model vs fine-tuned model on test set\"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"BASE vs FINE-TUNED COMPARISON ON TEST SET\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    test_captions_file = os.path.join(config.BASE_DATA_DIR, \"test_captions.json\")\n",
    "    if not os.path.exists(test_captions_file):\n",
    "        print(\"❌ Test set not found.\")\n",
    "        return None\n",
    "    \n",
    "    # Test base model\n",
    "    print(\"1. Evaluating Base Model...\")\n",
    "    base_model, base_preprocess, base_tokenizer = load_base_model(checkpoint_path=None)\n",
    "    # Nothing is applied to the base model, so we just load it\n",
    "    base_results = evaluate_on_test_set(base_model, base_preprocess, base_tokenizer, sample_size=10)\n",
    "    \n",
    "    # Test fine-tuned model if available\n",
    "    if os.path.exists(config.FINETUNED_MODEL_PATH):\n",
    "        print(f\"\\n2. Evaluating Fine-tuned Model...\")\n",
    "        ft_model, ft_preprocess, ft_tokenizer = load_base_model(checkpoint_path=config.FINETUNED_MODEL_PATH)\n",
    "        # No positional embedding interpolation is applied to the fine-tuned model, because it was already done during training\n",
    "        # ft_model = apply_positional_embedding_interpolation(ft_model)\n",
    "        ft_results = evaluate_on_test_set(ft_model, ft_preprocess, ft_tokenizer, sample_size=10)\n",
    "        \n",
    "        # Compare results\n",
    "        if base_results and ft_results:\n",
    "            print(f\"\\n\" + \"=\"*50)\n",
    "            print(\"COMPARISON SUMMARY\")\n",
    "            print(\"=\"*50)\n",
    "            \n",
    "            base_short_avg = sum(r['short_similarity'] for r in base_results) / len(base_results)\n",
    "            base_long_avg = sum(r['long_similarity'] for r in base_results) / len(base_results)\n",
    "            ft_short_avg = sum(r['short_similarity'] for r in ft_results) / len(ft_results)\n",
    "            ft_long_avg = sum(r['long_similarity'] for r in ft_results) / len(ft_results)\n",
    "            \n",
    "            short_improvement = ((ft_short_avg - base_short_avg) / base_short_avg * 100) if base_short_avg > 0 else 0\n",
    "            long_improvement = ((ft_long_avg - base_long_avg) / base_long_avg * 100) if base_long_avg > 0 else 0\n",
    "            \n",
    "            print(f\"Short Caption Performance:\")\n",
    "            print(f\"  Base model: {base_short_avg:.4f}\")\n",
    "            print(f\"  Fine-tuned: {ft_short_avg:.4f}\")\n",
    "            print(f\"  Improvement: {short_improvement:+.2f}%\")\n",
    "            \n",
    "            print(f\"\\nLong Caption Performance:\")\n",
    "            print(f\"  Base model: {base_long_avg:.4f}\")\n",
    "            print(f\"  Fine-tuned: {ft_long_avg:.4f}\")\n",
    "            print(f\"  Improvement: {long_improvement:+.2f}%\")\n",
    "            \n",
    "            print(f\"\\nKey Findings:\")\n",
    "            if long_improvement > short_improvement:\n",
    "                print(f\"  ✓ Fine-tuning particularly helps with long captions (+{long_improvement-short_improvement:.2f}% extra benefit)\")\n",
    "            if ft_long_avg > ft_short_avg:\n",
    "                print(f\"  ✓ Fine-tuned model handles detailed descriptions better\")\n",
    "            \n",
    "        return {'base': base_results, 'finetuned': ft_results}\n",
    "    else:\n",
    "        print(f\"❌ Fine-tuned model not found at: {config.FINETUNED_MODEL_PATH}\")\n",
    "        return {'base': base_results}\n",
    "\n",
    "# Run evaluation on test set (not training set!)\n",
    "print(\"Test set evaluation functions ready.\")\n",
    "print(\"Run evaluation with:\")\n",
    "print(\"  test_results = evaluate_on_test_set(model, preprocess, tokenizer)\")\n",
    "print(\"  comparison = compare_base_vs_finetuned_on_test()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc804d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 14: Performance Comparison Summary\n",
    "# ============================================================================\n",
    "\n",
    "def print_performance_summary():\n",
    "    \"\"\"Print a summary of model performance\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"LIGHTVISION PERFORMANCE SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"Model Configuration:\")\n",
    "    print(f\"  Base Model: {config.MODEL_NAME}\")\n",
    "    print(f\"  Device: {config.DEVICE}\")\n",
    "    print(f\"  Positional Embedding Interpolation: λ={config.LAMBDA2}\")\n",
    "    print(f\"  PCA Dimension: {config.PCA_DIM}\")\n",
    "    \n",
    "    if model_loaded:\n",
    "        print(f\"  Status: ✓ Fine-tuned model loaded\")\n",
    "        print(f\"  Checkpoint: {config.FINETUNED_MODEL_PATH}\")\n",
    "    else:\n",
    "        print(f\"  Status: ⚠ Base model (no fine-tuning)\")\n",
    "    \n",
    "    print(f\"\\nData Configuration:\")\n",
    "    _, image_count, caption_files = check_data_availability()\n",
    "    print(f\"  Images available: {image_count}\")\n",
    "    print(f\"  Caption files: {sum(caption_files.values())} available\")\n",
    "    \n",
    "    print(f\"\\nNext Steps:\")\n",
    "    if not model_loaded:\n",
    "        print(\"  1. ⚠ Train the model using available data\")\n",
    "        print(\"  2. Run comprehensive evaluation\")\n",
    "        print(\"  3. Set up retrieval framework\")\n",
    "    else:\n",
    "        print(\"  1. ✓ Model is ready for deployment\")\n",
    "        print(\"  2. Run comprehensive evaluation\")\n",
    "        print(\"  3. Set up retrieval framework with FAISS\")\n",
    "    \n",
    "    print(f\"\\nFor comprehensive evaluation and retrieval setup:\")\n",
    "    print(f\"  - Use the evaluation_system.py module\")\n",
    "    print(f\"  - Use the retrieval_framework.py module\")\n",
    "\n",
    "# Print summary\n",
    "print_performance_summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1302ebf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 15: Quick Interactive Test\n",
    "# ============================================================================\n",
    "\n",
    "def interactive_test():\n",
    "    \"\"\"Run an interactive test session\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"INTERACTIVE MODEL TEST\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Enter text queries to test the model (type 'quit' to exit)\")\n",
    "    \n",
    "    while True:\n",
    "        query = input(\"\\nEnter your query: \").strip()\n",
    "        \n",
    "        if query.lower() in ['quit', 'exit', 'q']:\n",
    "            break\n",
    "        \n",
    "        if not query:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Encode the query\n",
    "            text_tokens = tokenizer([query]).to(config.DEVICE)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                text_features = model.encode_text(text_tokens)\n",
    "                text_features = F.normalize(text_features, dim=-1)\n",
    "            \n",
    "            print(f\"Query encoded successfully!\")\n",
    "            print(f\"Text feature shape: {text_features.shape}\")\n",
    "            print(f\"Feature norm: {text_features.norm():.4f}\")\n",
    "            \n",
    "            # If you have a test image, you can compare similarities here\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing query: {e}\")\n",
    "\n",
    "# Uncomment the line below to run interactive test\n",
    "# interactive_test()\n",
    "\n",
    "print(\"Interactive test function ready. Uncomment the line above to run it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa8fd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 16: Export Model and Prepare for Deployment\n",
    "# ============================================================================\n",
    "\n",
    "def export_model_for_deployment():\n",
    "    \"\"\"Prepare model for deployment in retrieval framework\"\"\"\n",
    "    print(\"Preparing model for deployment...\")\n",
    "    \n",
    "    # Save model in a format ready for retrieval framework\n",
    "    deployment_config = {\n",
    "        'model_name': config.MODEL_NAME,\n",
    "        'checkpoint_path': config.FINETUNED_MODEL_PATH if model_loaded else None,\n",
    "        'device': str(config.DEVICE),\n",
    "        'embedding_dim': config.EMBEDDING_DIM,\n",
    "        'positional_interpolation': {\n",
    "            'applied': True,\n",
    "            'lambda': config.LAMBDA2\n",
    "        },\n",
    "        'training_config': {\n",
    "            'batch_size': config.BATCH_SIZE,\n",
    "            'learning_rate': config.LEARNING_RATE,\n",
    "            'epochs_trained': config.NUM_EPOCHS if model_loaded else 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save deployment configuration\n",
    "    config_path = os.path.join(config.CHECKPOINT_DIR, \"deployment_config.json\")\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(deployment_config, f, indent=2)\n",
    "    \n",
    "    print(f\"Deployment configuration saved to: {config_path}\")\n",
    "    \n",
    "    # Save current model state for retrieval framework\n",
    "    if model_loaded:\n",
    "        retrieval_model_path = os.path.join(config.CHECKPOINT_DIR, \"model_for_retrieval.pt\")\n",
    "        torch.save(model.state_dict(), retrieval_model_path)\n",
    "        print(f\"Model state saved for retrieval: {retrieval_model_path}\")\n",
    "    \n",
    "    return deployment_config\n",
    "\n",
    "# Export model\n",
    "deployment_config = export_model_for_deployment()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ee3dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 17: Notebook Summary and Next Steps\n",
    "# ============================================================================\n",
    "\n",
    "def print_notebook_summary():\n",
    "    \"\"\"Print a comprehensive summary of what was accomplished\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"LIGHTVISION NOTEBOOK EXECUTION SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"✓ COMPLETED TASKS:\")\n",
    "    print(\"  1. ✓ Environment setup and dependency checking\")\n",
    "    print(\"  2. ✓ Base MobileCLIP model loading\")\n",
    "    print(\"  3. ✓ Positional embedding interpolation applied\")\n",
    "    print(\"  4. ✓ Loss functions implemented (single_loss, long_clip_loss)\")\n",
    "    print(\"  5. ✓ Training framework set up\")\n",
    "    \n",
    "    if model_loaded:\n",
    "        print(\"  6. ✓ Model fine-tuning completed\")\n",
    "        print(\"  7. ✓ Trained model loaded and tested\")\n",
    "    else:\n",
    "        print(\"  6. ⚠ Model fine-tuning skipped (no training data)\")\n",
    "        print(\"  7. ⚠ Using base model for testing\")\n",
    "    \n",
    "    print(\"  8. ✓ Model testing functions implemented\")\n",
    "    print(\"  9. ✓ Deployment configuration prepared\")\n",
    "    \n",
    "    print(f\"\\n📊 MODEL CONFIGURATION:\")\n",
    "    print(f\"  • Base Model: {config.MODEL_NAME}\")\n",
    "    print(f\"  • Device: {config.DEVICE}\")\n",
    "    print(f\"  • Embedding Dimension: {config.EMBEDDING_DIM}\")\n",
    "    print(f\"  • Positional Interpolation: λ={config.LAMBDA2}\")\n",
    "    print(f\"  • PCA Dimension: {config.PCA_DIM}\")\n",
    "    \n",
    "    print(f\"\\n📁 GENERATED FILES:\")\n",
    "    print(f\"  • Base model: {config.BASE_MODEL_PATH}\")\n",
    "    if model_loaded:\n",
    "        print(f\"  • Fine-tuned model: {config.FINETUNED_MODEL_PATH}\")\n",
    "    print(f\"  • Deployment config: {os.path.join(config.CHECKPOINT_DIR, 'deployment_config.json')}\")\n",
    "    \n",
    "    print(f\"\\n🚀 NEXT STEPS:\")\n",
    "    print(f\"  1. Run comprehensive evaluation:\")\n",
    "    print(f\"     python evaluation_system.py --mode full\")\n",
    "    print(f\"  2. Set up retrieval framework:\")\n",
    "    print(f\"     python retrieval_framework.py\")\n",
    "    print(f\"  3. Compare model performance:\")\n",
    "    print(f\"     python example_usage.py --example 2\")\n",
    "    print(f\"  4. Deploy to production environment\")\n",
    "    \n",
    "    print(f\"\\n🔬 RESEARCH CONTRIBUTIONS:\")\n",
    "    print(f\"  • Lightweight mobile-compatible CLIP model\")\n",
    "    print(f\"  • Extended context length handling (77+ tokens)\")\n",
    "    print(f\"  • Principal Component Matching for multi-granularity alignment\")\n",
    "    print(f\"  • Knowledge-preserving positional embedding interpolation\")\n",
    "    \n",
    "    print(f\"\\n📈 EXPECTED IMPROVEMENTS:\")\n",
    "    print(f\"  • Better handling of detailed, long-form queries\")\n",
    "    print(f\"  • Improved semantic understanding beyond 20 tokens\")\n",
    "    print(f\"  • Maintained efficiency for mobile deployment\")\n",
    "    print(f\"  • Enhanced retrieval accuracy for complex descriptions\")\n",
    "\n",
    "# Print final summary\n",
    "print_notebook_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ffba23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 18: Cleanup and Memory Management\n",
    "# ============================================================================\n",
    "\n",
    "def cleanup_memory():\n",
    "    \"\"\"Clean up GPU memory and prepare for next steps\"\"\"\n",
    "    import gc\n",
    "    \n",
    "    print(\"Cleaning up memory...\")\n",
    "    \n",
    "    # Clear cache\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Garbage collection\n",
    "    gc.collect()\n",
    "    \n",
    "    # Print memory usage\n",
    "    if torch.cuda.is_available():\n",
    "        memory_allocated = torch.cuda.memory_allocated() / 1024**3  # GB\n",
    "        memory_reserved = torch.cuda.memory_reserved() / 1024**3   # GB\n",
    "        print(f\"GPU Memory - Allocated: {memory_allocated:.2f} GB, Reserved: {memory_reserved:.2f} GB\")\n",
    "    \n",
    "    print(\"Memory cleanup completed!\")\n",
    "\n",
    "# Optional cleanup\n",
    "# cleanup_memory()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"LIGHTVISION NOTEBOOK EXECUTION COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)\n",
    "print(\"The model is now ready for deployment in the retrieval framework.\")\n",
    "print(\"Check the generated files in the checkpoints/ directory.\")\n",
    "print(\"Run the evaluation and retrieval modules for comprehensive testing.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
