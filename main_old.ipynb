{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HKneQM-_e5DT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKneQM-_e5DT",
        "outputId": "77204dea-017a-4910-8104-20c7a9b4e81c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LightVision'...\n",
            "remote: Enumerating objects: 72, done.\u001b[K\n",
            "remote: Counting objects: 100% (72/72), done.\u001b[K\n",
            "remote: Compressing objects: 100% (58/58), done.\u001b[K\n",
            "remote: Total 72 (delta 22), reused 57 (delta 12), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (72/72), 5.71 MiB | 19.44 MiB/s, done.\n",
            "Resolving deltas: 100% (22/22), done.\n"
          ]
        }
      ],
      "source": [
        "# prompt: pull this repo: \"https://github.com/erenyavuz02/LightVision.git\"\n",
        "\n",
        "!git clone https://github.com/erenyavuz02/LightVision.git\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89daaaba",
      "metadata": {
        "id": "89daaaba"
      },
      "source": [
        "# Get Pretrained Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dae603b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dae603b",
        "outputId": "b02b5c67-5ff9-4bee-f007-d7860d50891f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-11 08:35:18--  https://docs-assets.developer.apple.com/ml-research/datasets/mobileclip/mobileclip_s0.pt\n",
            "Resolving docs-assets.developer.apple.com (docs-assets.developer.apple.com)... 17.253.118.201, 17.253.118.202, 2403:300:a32:f000::1, ...\n",
            "Connecting to docs-assets.developer.apple.com (docs-assets.developer.apple.com)|17.253.118.201|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 215934653 (206M) [application/octet-stream]\n",
            "Saving to: ‘checkpoints/mobileclip_s0.pt’\n",
            "\n",
            "mobileclip_s0.pt    100%[===================>] 205.93M  75.6MB/s    in 2.7s    \n",
            "\n",
            "2025-05-11 08:35:22 (75.6 MB/s) - ‘checkpoints/mobileclip_s0.pt’ saved [215934653/215934653]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p checkpoints/\n",
        "\n",
        "!wget https://docs-assets.developer.apple.com/ml-research/datasets/mobileclip/mobileclip_s0.pt -P checkpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b42eac2b",
      "metadata": {
        "id": "b42eac2b"
      },
      "source": [
        "# dowload libraries if necessary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60043773",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60043773",
        "outputId": "9ad51461-2c46-458c-8dbc-7bc289c283d8",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m101.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m97.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m107.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.6.0+cu124)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.15)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from timm) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm) (0.21.0+cu124)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.30.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->timm) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2025.4.26)\n",
            "Collecting open-clip-torch\n",
            "  Downloading open_clip_torch-2.32.0-py3-none-any.whl.metadata (31 kB)\n",
            "Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (0.21.0+cu124)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (2024.11.6)\n",
            "Collecting ftfy (from open-clip-torch)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (4.67.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (0.30.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (0.5.3)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (1.0.15)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.9.0->open-clip-torch) (1.3.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->open-clip-torch) (0.2.13)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->open-clip-torch) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->open-clip-torch) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->open-clip-torch) (2.32.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->open-clip-torch) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->open-clip-torch) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.9.0->open-clip-torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open-clip-torch) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open-clip-torch) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open-clip-torch) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open-clip-torch) (2025.4.26)\n",
            "Downloading open_clip_torch-2.32.0-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ftfy, open-clip-torch\n",
            "Successfully installed ftfy-6.3.1 open-clip-torch-2.32.0\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.6.0 dill-0.3.8 fsspec-2025.3.0 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Collecting clip-benchmark\n",
            "  Downloading clip_benchmark-1.6.1-py2.py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from clip-benchmark) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from clip-benchmark) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm>=2 in /usr/local/lib/python3.11/dist-packages (from clip-benchmark) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.0 in /usr/local/lib/python3.11/dist-packages (from clip-benchmark) (1.6.1)\n",
            "Requirement already satisfied: open-clip-torch>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from clip-benchmark) (2.32.0)\n",
            "Collecting pycocoevalcap (from clip-benchmark)\n",
            "  Downloading pycocoevalcap-1.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting webdataset>=0.2.31 (from clip-benchmark)\n",
            "  Downloading webdataset-0.2.111-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from clip-benchmark) (4.51.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from open-clip-torch>=0.2.1->clip-benchmark) (2024.11.6)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from open-clip-torch>=0.2.1->clip-benchmark) (6.3.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from open-clip-torch>=0.2.1->clip-benchmark) (0.30.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from open-clip-torch>=0.2.1->clip-benchmark) (0.5.3)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (from open-clip-torch>=0.2.1->clip-benchmark) (1.0.15)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2,>=1.0->clip-benchmark) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2,>=1.0->clip-benchmark) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2,>=1.0->clip-benchmark) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2,>=1.0->clip-benchmark) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->clip-benchmark) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->clip-benchmark) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->clip-benchmark) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->clip-benchmark) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->clip-benchmark) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->clip-benchmark) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->clip-benchmark) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->clip-benchmark) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->clip-benchmark) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->clip-benchmark) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->clip-benchmark) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->clip-benchmark) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->clip-benchmark) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->clip-benchmark) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->clip-benchmark) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->clip-benchmark) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->clip-benchmark) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->clip-benchmark) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->clip-benchmark) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->clip-benchmark) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.1->clip-benchmark) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision>=0.8.9->clip-benchmark) (11.2.1)\n",
            "Collecting braceexpand (from webdataset>=0.2.31->clip-benchmark)\n",
            "  Downloading braceexpand-0.1.7-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from webdataset>=0.2.31->clip-benchmark) (6.0.2)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from pycocoevalcap->clip-benchmark) (2.0.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers->clip-benchmark) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers->clip-benchmark) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->clip-benchmark) (0.21.1)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pycocotools>=2.0.2->pycocoevalcap->clip-benchmark) (3.10.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->open-clip-torch>=0.2.1->clip-benchmark) (0.2.13)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.1->clip-benchmark) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->clip-benchmark) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->clip-benchmark) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->clip-benchmark) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->clip-benchmark) (2025.4.26)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap->clip-benchmark) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap->clip-benchmark) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap->clip-benchmark) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap->clip-benchmark) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap->clip-benchmark) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap->clip-benchmark) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap->clip-benchmark) (1.17.0)\n",
            "Downloading clip_benchmark-1.6.1-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading webdataset-0.2.111-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycocoevalcap-1.2-py3-none-any.whl (104.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading braceexpand-0.1.7-py2.py3-none-any.whl (5.9 kB)\n",
            "Installing collected packages: braceexpand, webdataset, pycocoevalcap, clip-benchmark\n",
            "Successfully installed braceexpand-0.1.7 clip-benchmark-1.6.1 pycocoevalcap-1.2 webdataset-0.2.111\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install torchvision\n",
        "!pip install timm\n",
        "!pip install open-clip-torch\n",
        "!pip install datasets\n",
        "!pip install clip-benchmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yqZeLYRgfcb_",
      "metadata": {
        "id": "yqZeLYRgfcb_"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/LightVision')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87ffff17",
      "metadata": {
        "id": "87ffff17"
      },
      "source": [
        "# Libaries, Parameters and Model Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c56856b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c56856b",
        "outputId": "d24631d3-f9fb-4207-a7c6-7b7541732ccf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "import json\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import mobileclip\n",
        "import random\n",
        "import zipfile\n",
        "import requests\n",
        "import io\n",
        "\n",
        "# --- Configuration ---\n",
        "# Using relative paths for better portability\n",
        "BASE_DATA_DESTINATION = os.path.join(os.getcwd(), \"/content/LightVision/data\")\n",
        "FLICKR8K_IMAGES_FOLDER_NAME = \"Images\"\n",
        "CAPTIONS_JSON_FILENAME = \"all_captions2.json\"\n",
        "CHECKPOINT_DIR = os.path.join(os.getcwd(), \"checkpoints\")\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "g0WZrMuRojwX",
      "metadata": {
        "id": "g0WZrMuRojwX"
      },
      "source": [
        "Load the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LpvZNWaAogXm",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpvZNWaAogXm",
        "outputId": "00d5503c-ad18-4ef0-cf79-7471807dd012"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CLIP(\n",
              "  (image_encoder): MCi(\n",
              "    (model): FastViT(\n",
              "      (patch_embed): Sequential(\n",
              "        (0): MobileOneBlock(\n",
              "          (se): Identity()\n",
              "          (activation): GELU(approximate='none')\n",
              "          (reparam_conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "        )\n",
              "        (1): MobileOneBlock(\n",
              "          (se): Identity()\n",
              "          (activation): GELU(approximate='none')\n",
              "          (reparam_conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64)\n",
              "        )\n",
              "        (2): MobileOneBlock(\n",
              "          (se): Identity()\n",
              "          (activation): GELU(approximate='none')\n",
              "          (reparam_conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (network): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): RepMixerBlock(\n",
              "            (token_mixer): RepMixer(\n",
              "              (reparam_conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
              "            )\n",
              "            (convffn): ConvFFN(\n",
              "              (conv): Sequential(\n",
              "                (conv): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=64, bias=False)\n",
              "                (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "              (fc1): Conv2d(64, 192, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "          (1): RepMixerBlock(\n",
              "            (token_mixer): RepMixer(\n",
              "              (reparam_conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
              "            )\n",
              "            (convffn): ConvFFN(\n",
              "              (conv): Sequential(\n",
              "                (conv): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=64, bias=False)\n",
              "                (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "              (fc1): Conv2d(64, 192, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "        )\n",
              "        (1): PatchEmbed(\n",
              "          (proj): Sequential(\n",
              "            (0): ReparamLargeKernelConv(\n",
              "              (activation): GELU(approximate='none')\n",
              "              (se): Identity()\n",
              "              (lkb_reparam): Conv2d(64, 128, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=64)\n",
              "            )\n",
              "            (1): MobileOneBlock(\n",
              "              (se): Identity()\n",
              "              (activation): GELU(approximate='none')\n",
              "              (reparam_conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (2): Sequential(\n",
              "          (0): RepMixerBlock(\n",
              "            (token_mixer): RepMixer(\n",
              "              (reparam_conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
              "            )\n",
              "            (convffn): ConvFFN(\n",
              "              (conv): Sequential(\n",
              "                (conv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128, bias=False)\n",
              "                (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "              (fc1): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "          (1): RepMixerBlock(\n",
              "            (token_mixer): RepMixer(\n",
              "              (reparam_conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
              "            )\n",
              "            (convffn): ConvFFN(\n",
              "              (conv): Sequential(\n",
              "                (conv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128, bias=False)\n",
              "                (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "              (fc1): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "          (2): RepMixerBlock(\n",
              "            (token_mixer): RepMixer(\n",
              "              (reparam_conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
              "            )\n",
              "            (convffn): ConvFFN(\n",
              "              (conv): Sequential(\n",
              "                (conv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128, bias=False)\n",
              "                (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "              (fc1): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "          (3): RepMixerBlock(\n",
              "            (token_mixer): RepMixer(\n",
              "              (reparam_conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
              "            )\n",
              "            (convffn): ConvFFN(\n",
              "              (conv): Sequential(\n",
              "                (conv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128, bias=False)\n",
              "                (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "              (fc1): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "          (4): RepMixerBlock(\n",
              "            (token_mixer): RepMixer(\n",
              "              (reparam_conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
              "            )\n",
              "            (convffn): ConvFFN(\n",
              "              (conv): Sequential(\n",
              "                (conv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128, bias=False)\n",
              "                (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "              (fc1): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "          (5): RepMixerBlock(\n",
              "            (token_mixer): RepMixer(\n",
              "              (reparam_conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
              "            )\n",
              "            (convffn): ConvFFN(\n",
              "              (conv): Sequential(\n",
              "                (conv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128, bias=False)\n",
              "                (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "              (fc1): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "        )\n",
              "        (3): PatchEmbed(\n",
              "          (proj): Sequential(\n",
              "            (0): ReparamLargeKernelConv(\n",
              "              (activation): GELU(approximate='none')\n",
              "              (se): SEModule(\n",
              "                (fc1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "                (bn): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "                (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "                (gate): Sigmoid()\n",
              "              )\n",
              "              (lkb_reparam): Conv2d(128, 256, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=128)\n",
              "            )\n",
              "            (1): MobileOneBlock(\n",
              "              (se): Identity()\n",
              "              (activation): GELU(approximate='none')\n",
              "              (reparam_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (4): Sequential(\n",
              "          (0): RepMixerBlock(\n",
              "            (token_mixer): RepMixer(\n",
              "              (reparam_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
              "            )\n",
              "            (convffn): ConvFFN(\n",
              "              (conv): Sequential(\n",
              "                (conv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256, bias=False)\n",
              "                (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "              (fc1): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "          (1): RepMixerBlock(\n",
              "            (token_mixer): RepMixer(\n",
              "              (reparam_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
              "            )\n",
              "            (convffn): ConvFFN(\n",
              "              (conv): Sequential(\n",
              "                (conv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256, bias=False)\n",
              "                (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "              (fc1): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "          (2): RepMixerBlock(\n",
              "            (token_mixer): RepMixer(\n",
              "              (reparam_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
              "            )\n",
              "            (convffn): ConvFFN(\n",
              "              (conv): Sequential(\n",
              "                (conv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256, bias=False)\n",
              "                (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "              (fc1): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "          (3): RepMixerBlock(\n",
              "            (token_mixer): RepMixer(\n",
              "              (reparam_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
              "            )\n",
              "            (convffn): ConvFFN(\n",
              "              (conv): Sequential(\n",
              "                (conv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256, bias=False)\n",
              "                (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "              (fc1): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "          (4): RepMixerBlock(\n",
              "            (token_mixer): RepMixer(\n",
              "              (reparam_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
              "            )\n",
              "            (convffn): ConvFFN(\n",
              "              (conv): Sequential(\n",
              "                (conv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256, bias=False)\n",
              "                (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "              (fc1): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "          (5): RepMixerBlock(\n",
              "            (token_mixer): RepMixer(\n",
              "              (reparam_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
              "            )\n",
              "            (convffn): ConvFFN(\n",
              "              (conv): Sequential(\n",
              "                (conv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256, bias=False)\n",
              "                (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "              (fc1): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "          (6): RepMixerBlock(\n",
              "            (token_mixer): RepMixer(\n",
              "              (reparam_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
              "            )\n",
              "            (convffn): ConvFFN(\n",
              "              (conv): Sequential(\n",
              "                (conv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256, bias=False)\n",
              "                (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "              (fc1): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "          (7): RepMixerBlock(\n",
              "            (token_mixer): RepMixer(\n",
              "              (reparam_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
              "            )\n",
              "            (convffn): ConvFFN(\n",
              "              (conv): Sequential(\n",
              "                (conv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256, bias=False)\n",
              "                (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "              (fc1): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "          (8): RepMixerBlock(\n",
              "            (token_mixer): RepMixer(\n",
              "              (reparam_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
              "            )\n",
              "            (convffn): ConvFFN(\n",
              "              (conv): Sequential(\n",
              "                (conv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256, bias=False)\n",
              "                (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "              (fc1): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "          (9): RepMixerBlock(\n",
              "            (token_mixer): RepMixer(\n",
              "              (reparam_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
              "            )\n",
              "            (convffn): ConvFFN(\n",
              "              (conv): Sequential(\n",
              "                (conv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256, bias=False)\n",
              "                (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "              (fc1): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "        )\n",
              "        (5): PatchEmbed(\n",
              "          (proj): Sequential(\n",
              "            (0): ReparamLargeKernelConv(\n",
              "              (activation): GELU(approximate='none')\n",
              "              (se): SEModule(\n",
              "                (fc1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "                (bn): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "                (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "                (gate): Sigmoid()\n",
              "              )\n",
              "              (lkb_reparam): Conv2d(256, 512, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=256)\n",
              "            )\n",
              "            (1): MobileOneBlock(\n",
              "              (se): Identity()\n",
              "              (activation): GELU(approximate='none')\n",
              "              (reparam_conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (6): RepCPE(\n",
              "          (reparam_conv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "        )\n",
              "        (7): Sequential(\n",
              "          (0): AttentionBlock(\n",
              "            (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (token_mixer): MHSA(\n",
              "              (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (convffn): ConvFFN(\n",
              "              (conv): Sequential(\n",
              "                (conv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512, bias=False)\n",
              "                (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "              (fc1): Conv2d(512, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Conv2d(1536, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "          (1): AttentionBlock(\n",
              "            (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (token_mixer): MHSA(\n",
              "              (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (convffn): ConvFFN(\n",
              "              (conv): Sequential(\n",
              "                (conv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512, bias=False)\n",
              "                (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "              (fc1): Conv2d(512, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Conv2d(1536, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (conv_exp): MobileOneBlock(\n",
              "        (se): SEBlock(\n",
              "          (reduce): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (expand): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (activation): GELU(approximate='none')\n",
              "        (reparam_conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
              "      )\n",
              "      (head): GlobalPool2D(\n",
              "        (pool): GlobalPool()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (text_encoder): TextTransformer(\n",
              "    (embedding_layer): Embedding(49408, 512)\n",
              "    (positional_embedding): LearnablePositionalEmbedding(num_embeddings=77, embedding_dim=512, padding_idx=None)\n",
              "    (embedding_dropout): Dropout(p=0.0, inplace=False)\n",
              "    (transformer): ModuleList(\n",
              "      (0): RepMixerBlock(\n",
              "        (token_mixer): RepMixer(\n",
              "          (reparam_conv): Conv2d(512, 512, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5), groups=512)\n",
              "        )\n",
              "        (convffn): ConvFFN(\n",
              "          (conv): Sequential(\n",
              "            (conv): Conv2d(512, 512, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5), groups=512, bias=False)\n",
              "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (fc1): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act): GELU(approximate='none')\n",
              "          (fc2): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (1-4): 4 x TransformerEncoder(embed_dim=512, ffn_dim=2048, dropout=0.0, ffn_dropout=0.0, stochastic_dropout=0.0, attn_fn=MultiHeadAttention, act_fn=GELU, norm_fn=layer_norm_fp32)\n",
              "      (5): RepMixerBlock(\n",
              "        (token_mixer): RepMixer(\n",
              "          (reparam_conv): Conv2d(512, 512, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5), groups=512)\n",
              "        )\n",
              "        (convffn): ConvFFN(\n",
              "          (conv): Sequential(\n",
              "            (conv): Conv2d(512, 512, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5), groups=512, bias=False)\n",
              "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (fc1): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act): GELU(approximate='none')\n",
              "          (fc2): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): LayerNormFP32((512,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Load the MobileCLIP model\n",
        "model_path = os.path.join(CHECKPOINT_DIR, 'mobileclip_s0.pt')\n",
        "model, _, preprocess = mobileclip.create_model_and_transforms(\n",
        "    'mobileclip_s0',\n",
        "    pretrained=model_path\n",
        ")\n",
        "model.to(DEVICE)\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = mobileclip.get_tokenizer('mobileclip_s0')\n",
        "\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qhlKSeyGolNA",
      "metadata": {
        "id": "qhlKSeyGolNA"
      },
      "source": [
        "Test the base model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"A man.\"\n",
        ", \"A man sitting on a bench.\"\n",
        ", \"A man sitting on a red bench in a park.\"\n",
        ", \"A man sitting on a red bench in a park holding a yellow umbrella.\"\n",
        ", \"A man sitting on a red bench in a park holding a yellow umbrella while feeding pigeons.\""
      ],
      "metadata": {
        "id": "oCvpYrL7TavT"
      },
      "id": "oCvpYrL7TavT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vCHVIpnsoiHQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "vCHVIpnsoiHQ",
        "outputId": "80baad50-8b24-431f-c67a-0800e91b32bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label probs: tensor([[0.2498, 0.2346, 0.2659, 0.2498]], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-066a6fd84805>:7: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.no_grad(), torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "\n",
        "image = preprocess(Image.open(\"/content/Screenshot 2025-05-11 at 01.34.57.png\").convert('RGB')).unsqueeze(0)\n",
        "text = tokenizer([\"The lemon on the left is yellow and the eggplant on the right is purple.\"\n",
        ", \"The lemon on the left is purple and the eggplant on the right is yellow.\"\n",
        ", \"The lemon on the right is yellow and the eggplant on the left is purple.\"\n",
        ", \"The lemon on the right is purple and the eggplant on the left is yellow\"])\n",
        "\n",
        "with torch.no_grad(), torch.cuda.amp.autocast():\n",
        "    image_features = model.encode_image(image.half().to(DEVICE))\n",
        "    text_features = model.encode_text(text.to(DEVICE))\n",
        "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "\n",
        "# Set the print options for PyTorch to avoid scientific notation and limit decimal places\n",
        "torch.set_printoptions(sci_mode=False, precision=4)\n",
        "\n",
        "print(\"Label probs:\", text_probs)\n",
        "\n",
        "\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer(\"At a train station, a group of people, including both young children and adults, are standing on a platform waiting for a train to arrive. The train is already present on the tracks, partially visible on the right side of the image. Some of the people watch the train closely, while others seem to be patiently anticipating its departure. There is a total of eight individuals  waiting \")# for the  train,  with one child in the middle of the platform and the others scattered around. A backpack can be found on the far left side of the platform, suggesting that someone may have set it down while waiting.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7U5EbF2ye2Oj",
        "outputId": "a364e3e9-fe6a-43f3-ec4e-9fd798b96cd8"
      },
      "id": "7U5EbF2ye2Oj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[49406,   536,   320,  3231,  2631,   267,   320,  1771,   539,  1047,\n",
              "           267,  2814,  2212,  1888,  2153,   537,  9391,   267,   631,  2862,\n",
              "           525,   320,  5549,  2680,   556,   320,  3231,   531,  8851,   269,\n",
              "           518,  3231,   533,  2426,  2881,   525,   518,  7579,   267, 21269,\n",
              "          8626,   525,   518,  1155,  1145,   539,   518,  2867,   269,   836,\n",
              "           539,   518,  1047,  1239,   518,  3231, 13478,   267,  1519,  3326,\n",
              "          7523,   531,   655, 22980, 48067,   902, 17850,   269,   997,   533,\n",
              "           320,  4445,   539,  7910, 11990,  2680, 49407]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "before : [0.2498, 0.2346, 0.2659, 0.2498]\n",
        "after : [0.2783, 0.1797, 0.2456, 0.2963]"
      ],
      "metadata": {
        "id": "xfwEAJ2GWSST"
      },
      "id": "xfwEAJ2GWSST"
    },
    {
      "cell_type": "code",
      "source": [
        "\"A man in a red hat standing next to a yellow car in front of a green grocery store.\"\n",
        "→ \"A man wearing a bright red hat is casually standing next to a yellow car, which appears to be parked in front of a green grocery store on a sunny afternoon with some people walking nearby.\"\n",
        "\n",
        "\"A man in a red hat standing next to a yellow car in front of a store.\"\n",
        "→ \"A man, possibly in his mid-thirties, is seen in a red hat while standing next to a yellow car, which is parked near what seems to be a local store with large windows and some decorative signage.\"\n",
        "\n",
        "\"A man standing next to a yellow car in front of a green grocery store.\"\n",
        "→ \"There is a man, casually dressed, standing next to a yellow car that is parked right in front of a green-painted grocery store, where a bicycle rack and some flower pots are also visible.\"\n",
        "\n",
        "\"A man in a red hat standing next to a car in front of a green grocery store.\"\n",
        "→ \"Wearing a red hat and dark trousers, a man is standing next to a parked car, which is located near a green grocery store that has various posters and sale signs displayed in the window.\""
      ],
      "metadata": {
        "id": "dNyfCt4mYBXH"
      },
      "id": "dNyfCt4mYBXH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80cc7e55-37ae-4f72-d36b-4f536cff683c",
        "id": "LyfyD2oWYBvz"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label probs: tensor([[    0.6294,     0.3700,     0.0001,     0.0005]], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-101-ee8bb013a1db>:7: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.no_grad(), torch.cuda.amp.autocast():\n"
          ]
        }
      ],
      "source": [
        "\n",
        "image = preprocess(Image.open(\"/content/test_image.png\").convert('RGB')).unsqueeze(0)\n",
        "text = tokenizer([ \"A man in a red hat standing next to a yellow car in front of a grocery store with green signboard.\"\n",
        ", \"A man in a red hat standing next to a yellow car in front of a store.\"\n",
        ", \"A man standing next to a yellow car in front of a grocery store with green signboard.\"\n",
        ", \"A man in a red hat standing next to a car in front of a grocery store with green signboard.\"])\n",
        "\n",
        "with torch.no_grad(), torch.cuda.amp.autocast():\n",
        "    image_features = model.encode_image(image.half().to(DEVICE))\n",
        "    text_features = model.encode_text(text.to(DEVICE))\n",
        "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "\n",
        "# Set the print options for PyTorch to avoid scientific notation and limit decimal places\n",
        "torch.set_printoptions(sci_mode=False, precision=4)\n",
        "\n",
        "print(\"Label probs:\", text_probs)\n",
        "\n"
      ],
      "id": "LyfyD2oWYBvz"
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer(\"place at the harbor where two people are looking out towards ferries in the distance. They stand next to each other near a metal railing with one person hugging the other, possibly sharing a loving moment as they look into the ocean together.\")\n",
        "\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZ8VJbBaW28b",
        "outputId": "5b107f59-b748-442f-b7b7-bb66bee74ef8"
      },
      "id": "cZ8VJbBaW28b",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[49406,  1445,   536,   518, 10202,  1234,  1237,  1047,   631,  1312,\n",
            "           620,  4447, 28489,   530,   518,  7964,   269,   889,  2087,  1131,\n",
            "           531,  2416,  1010,  2252,   320,  4044,   559,  3299,   593,   637,\n",
            "          2533, 27058,   518,  1010,   267,  8601,  3567,   320,  3721,  2495,\n",
            "           601,   889,  1012,  1095,   518,  4918,  1952,   269, 49407,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a86efc59",
      "metadata": {
        "id": "a86efc59"
      },
      "source": [
        "# Changing the Positional Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97b92180",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97b92180",
        "outputId": "e02952ed-407e-456a-ed24-7e0ffa926363"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LearnablePositionalEmbedding(num_embeddings=77, embedding_dim=512, padding_idx=None)\n",
            "Modified Positional Embedding: Parameter containing:\n",
            "tensor([[[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "            0.0000e+00,  0.0000e+00],\n",
            "          [ 4.0536e-03,  1.6300e-03, -7.1365e-04,  ...,  7.1379e-04,\n",
            "            3.5601e-03, -7.5971e-03],\n",
            "          [ 7.7365e-03,  2.6204e-03,  1.1954e-03,  ...,  1.4502e-04,\n",
            "            1.3051e-03, -4.3484e-03],\n",
            "          ...,\n",
            "          [ 7.5012e-03,  5.1169e-03,  4.3844e-06,  ...,  1.0989e-03,\n",
            "            9.3555e-05, -3.0923e-04],\n",
            "          [ 3.1206e-03,  6.8456e-03, -7.9795e-04,  ...,  2.3707e-03,\n",
            "           -1.6804e-04, -2.4519e-03],\n",
            "          [-1.2599e-03,  8.5743e-03, -1.6003e-03,  ...,  3.6424e-03,\n",
            "           -4.2963e-04, -4.5946e-03]]]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "print(model.get_positional_embedding() )\n",
        "\n",
        "def get_positional_embedding(self, lambda2: int = 4):\n",
        "    \"\"\"\n",
        "    Get modified positional embedding for text encoder based on the given formula.\n",
        "    \"\"\"\n",
        "    pos_embed = self.text_encoder.get_positional_embedding().pos_embed.pos_embed\n",
        "    if pos_embed is None:\n",
        "        raise ValueError(\"Positional embedding not found in text encoder.\")\n",
        "\n",
        "    max_pos, embed_dim = pos_embed.shape[2], pos_embed.shape[3]\n",
        "    modified_pos_embed = torch.zeros((1, 1, max_pos, embed_dim), device=pos_embed.device)\n",
        "\n",
        "    for pos in range(max_pos):\n",
        "        if pos <= 20:\n",
        "            modified_pos_embed[:, :, pos, :] = pos_embed[:, :, pos, :]\n",
        "        else:\n",
        "            lower_idx = pos // lambda2\n",
        "            upper_idx = min(lower_idx + 1, max_pos - 1)  # Ensure upper_idx is within bounds\n",
        "            alpha = (pos % lambda2) / lambda2\n",
        "            modified_pos_embed[:, :, pos, :] = (1 - alpha) * pos_embed[:, :, lower_idx, :] + alpha * pos_embed[:, :, upper_idx, :]\n",
        "    # turn the torch tensor into nn parameter\n",
        "    modified_pos_embed = torch.nn.Parameter(modified_pos_embed, requires_grad=False)\n",
        "    return modified_pos_embed\n",
        "\n",
        "# Example usage\n",
        "lambda2 = 4\n",
        "new_pos_embed = get_positional_embedding(model, lambda2)\n",
        "print(\"Modified Positional Embedding:\", new_pos_embed)\n",
        "\n",
        "# set the models pos embedding to the new one\n",
        "model.text_encoder.get_positional_embedding().pos_embed.pos_embed = new_pos_embed\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7444910a",
      "metadata": {
        "id": "7444910a"
      },
      "source": [
        "# Testing the model after changing the positional embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1d0bbec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1d0bbec",
        "outputId": "0edeecd7-d98f-441f-935c-22c19138df24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label probs: tensor([[19.4844, 19.2188, 19.0000]], device='cuda:0', dtype=torch.float16)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-33764fa1e9ec>:6: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.no_grad(), torch.cuda.amp.autocast():\n"
          ]
        }
      ],
      "source": [
        "image = preprocess(Image.open(\"/content/LightVision/pngwing.com.png\").convert('RGB')).unsqueeze(0)\n",
        "image = image.to(DEVICE)\n",
        "text = tokenizer([\"a brown dog\", \"a white dog\", \"a black dog\"])\n",
        "text = text.to(DEVICE)\n",
        "\n",
        "with torch.no_grad(), torch.cuda.amp.autocast():\n",
        "    image_features = model.encode_image(image)\n",
        "    text_features = model.encode_text(text)\n",
        "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    text_probs = (100.0 * image_features @ text_features.T)\n",
        "\n",
        "# Set the print options for PyTorch to avoid scientific notation and limit decimal places\n",
        "torch.set_printoptions(sci_mode=False, precision=4)\n",
        "\n",
        "print(\"Label probs:\", text_probs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03590a4c",
      "metadata": {
        "id": "03590a4c"
      },
      "source": [
        "# Downloading the captioned images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "284b0522",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "284b0522",
        "outputId": "d39d44f1-b2c9-45a3-dacf-106990642e96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up data directories in: /content/LightVision/data\n",
            "Images already exist at /content/LightVision/data/Images.\n"
          ]
        }
      ],
      "source": [
        "# --- Configuration ---\n",
        "import os\n",
        "\n",
        "# Using relative paths for better portability\n",
        "# This creates a 'data' directory in the project folder\n",
        "\n",
        "KAGGLE_FLICKR8K_URL = \"https://www.kaggle.com/api/v1/datasets/download/adityajn105/flickr8k\"\n",
        "FLICKR8K_ZIP_FILENAME = \"flickr8k.zip\"\n",
        "FLICKR8K_IMAGES_FOLDER_NAME = \"Images\"\n",
        "CAPTIONS_CSV_FILENAME = \"captions.txt\"\n",
        "OUTPUT_FOLDER_NAME = \"output\"\n",
        "\n",
        "\n",
        "def download_file(url: str, destination_path: str):\n",
        "    print(f\"Downloading from {url} to {destination_path}...\")\n",
        "    try:\n",
        "        response = requests.get(url, stream=True)\n",
        "        response.raise_for_status()\n",
        "        with open(destination_path, \"wb\") as file:\n",
        "            for chunk in response.iter_content(chunk_size=1024):\n",
        "                if chunk:\n",
        "                    file.write(chunk)\n",
        "        print(\"Download complete.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        raise\n",
        "\n",
        "def extract_zip_file(zip_path: str, destination_folder: str):\n",
        "    print(f\"Extracting {zip_path} to {destination_folder}...\")\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
        "            zip_ref.extractall(destination_folder)\n",
        "        print(\"Extraction complete.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Extraction error: {e}\")\n",
        "        raise\n",
        "\n",
        "def setup_data_directory(base_data_path: str):\n",
        "    images_path = os.path.join(base_data_path, FLICKR8K_IMAGES_FOLDER_NAME)\n",
        "    output_path = os.path.join(base_data_path, OUTPUT_FOLDER_NAME)\n",
        "    os.makedirs(base_data_path, exist_ok=True)\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "    return base_data_path, images_path, output_path\n",
        "\n",
        "\n",
        "print(f\"Setting up data directories in: {BASE_DATA_DESTINATION}\")\n",
        "base_dir, images_dir, output_dir = setup_data_directory(BASE_DATA_DESTINATION)\n",
        "\n",
        "zip_file_path = os.path.join(base_dir, FLICKR8K_ZIP_FILENAME)\n",
        "\n",
        "if not os.path.exists(images_dir):\n",
        "    print(\"Images not found. Attempting download...\")\n",
        "    try:\n",
        "        download_file(KAGGLE_FLICKR8K_URL, zip_file_path)\n",
        "        extract_zip_file(zip_file_path, base_dir)\n",
        "        os.remove(zip_file_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to set up dataset: {e}\")\n",
        "        raise FileNotFoundError(f\"Please manually download and extract to: {base_dir}\")\n",
        "else:\n",
        "    print(f\"Images already exist at {images_dir}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc27e2b8",
      "metadata": {
        "id": "cc27e2b8"
      },
      "source": [
        "# Train the model using the downloaded images and custom captions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba672ab3",
      "metadata": {
        "id": "ba672ab3"
      },
      "source": [
        "Importing the required libraries\n",
        "setting parameters and lookups\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11b7d2ac",
      "metadata": {
        "id": "11b7d2ac"
      },
      "source": [
        "## Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22fa04af",
      "metadata": {
        "id": "22fa04af"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Custom Dataset for Flickr8k with the specific JSON caption format\n",
        "class Flickr8kCaptionedDataset(Dataset):\n",
        "    def __init__(self, image_dir, captions_file, preprocess_fn, pull_from_json=True):\n",
        "        self.image_dir = image_dir\n",
        "        self.preprocess_fn = preprocess_fn\n",
        "\n",
        "        self.num_samples = 0\n",
        "        # Create list of samples\n",
        "        self.samples = []\n",
        "\n",
        "        if pull_from_json:\n",
        "            # Load captions from JSON file\n",
        "            with open(captions_file, 'r') as f:\n",
        "                self.captions_data = json.load(f)\n",
        "\n",
        "            # Process JSON with format {\"image.jpg\": {\"long_caption\": \"...\", \"short_caption\": \"...\"}, ...}\n",
        "            for image_name, captions in self.captions_data.items():\n",
        "                if \"long_caption\" in captions and \"short_caption\" in captions:\n",
        "                    #if image is not in the image directory, skip\n",
        "                    image_path = os.path.join(self.image_dir, image_name)\n",
        "                    if not os.path.exists(image_path):\n",
        "                        print(f\"Image {image_path} not found, skipping.\")\n",
        "                        continue\n",
        "                    # Add both caption types for each image\n",
        "                    self.samples.append((image_name, captions[\"short_caption\"], captions[\"long_caption\"]))\n",
        "        else:\n",
        "            # Use the default Flickr8k captions file\n",
        "            captions_file = \"/content/LightVision/data/captions.txt\"\n",
        "            with open(captions_file, 'r') as f:\n",
        "                lines = f.readlines()\n",
        "\n",
        "            # Process the standard Flickr8k format\n",
        "            # Typically each line has format: \"image_name#caption\" or \"image_name,caption\"\n",
        "            for line in lines:\n",
        "                line = line.strip()\n",
        "                if line:\n",
        "                    # Try to split by common delimiters\n",
        "                    if '#' in line:\n",
        "                        parts = line.split('#', 1)\n",
        "                    else:\n",
        "                        parts = line.split(',', 1)\n",
        "\n",
        "                    if len(parts) == 2:\n",
        "                        image_name, caption = parts\n",
        "                        #print(f\"Image name: {image_name.strip()}, Caption: {caption.strip()}\")\n",
        "                        # Add this check before appending to self.samples\n",
        "                        image_path = os.path.join(self.image_dir, image_name.strip())\n",
        "                        if not os.path.exists(image_path):\n",
        "                            continue  # Skip this caption if image doesn't exist\n",
        "                        self.samples.append((image_name.strip(), caption.strip(), \"standard\"))\n",
        "                        self.num_samples += 1\n",
        "\n",
        "        print(f\"Loaded {len(self.samples)} samples from {captions_file}.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_name, caption, caption_type = self.samples[idx]\n",
        "        image_path = os.path.join(self.image_dir, image_name)\n",
        "\n",
        "        # Load and preprocess the image\n",
        "        try:\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "            image = self.preprocess_fn(image)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {image_path}: {e}\")\n",
        "            # Return a random valid sample instead\n",
        "            return self.__getitem__(random.randint(0, len(self) - 1))\n",
        "\n",
        "        return image, caption, caption_type\n",
        "\n",
        "    def __reduce__(self):\n",
        "        return (self.__class__, (self.image_dir, self.captions_file, self.preprocess_fn, True))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ca6364d",
      "metadata": {
        "id": "6ca6364d"
      },
      "source": [
        "# PCA of LongCLIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb52f8c0",
      "metadata": {
        "id": "cb52f8c0"
      },
      "outputs": [],
      "source": [
        "#rewrite PCA to avoid inf\n",
        "def PCA(input_tensor, PCA_dim):\n",
        "    # 计算均值\n",
        "    mean = torch.mean(input_tensor, dim=0)\n",
        "    # 去均值\n",
        "    X_centered = input_tensor - mean.unsqueeze(0)\n",
        "    X_centered = X_centered.float()\n",
        "\n",
        "    # 使用SVD而不是eig来计算主成分\n",
        "    U, S, Vt = torch.linalg.svd(X_centered, full_matrices=False)\n",
        "    principal_components = Vt.T[:, :PCA_dim]\n",
        "\n",
        "    # 转换到新的维度\n",
        "    X_transformed = torch.mm(X_centered, principal_components)\n",
        "    # 恢复到原始空间\n",
        "    X_reversed = torch.mm(X_transformed, principal_components.T)\n",
        "    X_reversed += mean\n",
        "\n",
        "    return X_reversed"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb56133f",
      "metadata": {
        "id": "bb56133f"
      },
      "source": [
        "## Loss functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1e2117a",
      "metadata": {
        "id": "c1e2117a"
      },
      "outputs": [],
      "source": [
        "# Contrastive Loss Function\n",
        "def single_loss(image_embeds, text_embeds, temperature=0.07):\n",
        "    # Normalize embeddings\n",
        "    image_embeds = F.normalize(image_embeds, dim=1)\n",
        "    text_embeds = F.normalize(text_embeds, dim=1)\n",
        "\n",
        "    # Compute similarity matrix\n",
        "    logits = torch.matmul(image_embeds, text_embeds.T) / temperature\n",
        "\n",
        "    # Labels are the positions of the positive pairs\n",
        "    labels = torch.arange(logits.size(0), device=logits.device)\n",
        "\n",
        "    # Compute loss in both directions (image->text and text->image)\n",
        "    loss_i2t = F.cross_entropy(logits, labels)\n",
        "    loss_t2i = F.cross_entropy(logits.T, labels)\n",
        "\n",
        "    return (loss_i2t + loss_t2i) / 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9ed8cf6",
      "metadata": {
        "id": "c9ed8cf6"
      },
      "outputs": [],
      "source": [
        "def long_clip_loss(image_embedding, long_embedding, short_embedding):\n",
        "    image_features_long = image_embedding\n",
        "    text_features_long = long_embedding\n",
        "    text_features_short = short_embedding\n",
        "\n",
        "    # Normalize features\n",
        "    image_features_long = image_features_long / image_features_long.norm(dim=1, keepdim=True)\n",
        "    text_features_long = text_features_long / text_features_long.norm(dim=1, keepdim=True)\n",
        "    text_features_short = text_features_short / text_features_short.norm(dim=1, keepdim=True)\n",
        "\n",
        "    # Apply PCA to get compressed image features\n",
        "    image_features_short = PCA(image_features_long, 32)\n",
        "    image_features_short = image_features_short / image_features_short.norm(dim=1, keepdim=True)\n",
        "\n",
        "    # Since we're not using distributed training, simplify this part\n",
        "    image_feat_all_long = image_features_long\n",
        "    image_features_all_short = image_features_short\n",
        "    text_feat_all_long = text_features_long\n",
        "    text_feat_all_short = text_features_short\n",
        "\n",
        "    # Calculate similarity matrices\n",
        "    sim_i2tl = torch.matmul(image_features_long, text_feat_all_long.T)\n",
        "    sim_tl2i = torch.matmul(image_feat_all_long, text_features_long.T)\n",
        "    sim_tl2i = sim_tl2i.T\n",
        "\n",
        "    sim_i2ts = torch.matmul(image_features_short, text_feat_all_short.T)\n",
        "    sim_ts2i = torch.matmul(image_features_all_short, text_features_short.T)\n",
        "    sim_ts2i = sim_ts2i.T\n",
        "\n",
        "    # Apply temperature scaling\n",
        "    logit_scale = model.logit_scale if hasattr(model, 'logit_scale') else 1.0\n",
        "\n",
        "    if isinstance(logit_scale, torch.nn.Parameter):\n",
        "        sim_i2tl = logit_scale.exp() * sim_i2tl\n",
        "        sim_tl2i = logit_scale.exp() * sim_tl2i\n",
        "        sim_i2ts = logit_scale.exp() * sim_i2ts\n",
        "        sim_ts2i = logit_scale.exp() * sim_ts2i\n",
        "\n",
        "    # Create targets for loss calculation\n",
        "    bs = image_embedding.size(0)\n",
        "    targets = torch.arange(bs, device=image_embedding.device)\n",
        "\n",
        "    # Calculate losses\n",
        "    loss_itcl = (\n",
        "        F.cross_entropy(sim_i2tl, targets, label_smoothing=0.1)\n",
        "        + F.cross_entropy(sim_tl2i, targets, label_smoothing=0.1)\n",
        "    ) / 2\n",
        "\n",
        "    loss_itcs = (\n",
        "        F.cross_entropy(sim_i2ts, targets, label_smoothing=0.1)\n",
        "        + F.cross_entropy(sim_ts2i, targets, label_smoothing=0.1)\n",
        "    ) / 2\n",
        "\n",
        "    # single loss by combining the two\n",
        "    total_loss = (loss_itcl + loss_itcs) / 2\n",
        "\n",
        "    return total_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf0e2873",
      "metadata": {
        "id": "cf0e2873"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7353ab8b",
      "metadata": {
        "id": "7353ab8b"
      },
      "outputs": [],
      "source": [
        "def train_model(\n",
        "    images_dir,\n",
        "    captions_file,\n",
        "    checkpoint_dir,\n",
        "    device='cuda',\n",
        "    batch_size=128,\n",
        "    learning_rate=1e-4,\n",
        "    num_epochs=10,\n",
        "    num_workers=0,\n",
        "    pull_from_json=False,\n",
        "    long_clip_loss_fn=None,\n",
        "    single_loss_fn=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Train a CLIP model with the given parameters.\n",
        "\n",
        "    Args:\n",
        "        images_dir: Directory containing images\n",
        "        captions_file: Path to captions JSON file\n",
        "        checkpoint_dir: Directory to save checkpoints\n",
        "        device: Device to train on ('cuda' or 'cpu')\n",
        "        batch_size: Batch size for training\n",
        "        learning_rate: Learning rate for optimizer\n",
        "        num_epochs: Number of training epochs\n",
        "        num_workers: Number of dataloader workers\n",
        "        pull_from_json: Whether to pull captions from JSON\n",
        "        long_clip_loss_fn: Loss function for long captions\n",
        "        single_loss_fn: Loss function for single captions\n",
        "    \"\"\"\n",
        "\n",
        "    # Check if files exist\n",
        "    if not os.path.exists(images_dir):\n",
        "        raise FileNotFoundError(f\"Images directory not found: {images_dir}\")\n",
        "    if not os.path.exists(captions_file):\n",
        "        print(f\"Captions file not found: {captions_file}\")\n",
        "        pull_from_json = False\n",
        "\n",
        "\n",
        "    dataset = Flickr8kCaptionedDataset(images_dir, captions_file, preprocess, pull_from_json=pull_from_json)\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        drop_last=True\n",
        "    )\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Make sure model is in training mode\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0.0\n",
        "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        for batch_idx, (images, captions, long_captions) in enumerate(progress_bar):\n",
        "            images = images.to(device)\n",
        "\n",
        "            # Tokenize the captions\n",
        "            tokenized_captions = tokenizer(captions).to(device)\n",
        "\n",
        "            # Forward pass with mixed precision\n",
        "            with torch.cuda.amp.autocast():\n",
        "                image_features = model.encode_image(images)\n",
        "                text_features = model.encode_text(tokenized_captions)\n",
        "\n",
        "                # Compute contrastive loss\n",
        "                if long_captions is not None and long_clip_loss_fn is not None:\n",
        "                    long_captions = tokenizer(long_captions).to(device)\n",
        "                    long_text_features = model.encode_text(long_captions)\n",
        "                    loss = long_clip_loss_fn(image_features, text_features, long_text_features)\n",
        "                else:\n",
        "                    # Use single loss if long captions are not available\n",
        "                    if single_loss_fn is None:\n",
        "                        raise ValueError(\"Single loss function must be provided\")\n",
        "                    loss = single_loss_fn(image_features, text_features)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update progress bar\n",
        "            total_loss += loss.item()\n",
        "            avg_loss = total_loss / (batch_idx + 1)\n",
        "            progress_bar.set_postfix(loss=f\"{avg_loss:.4f}\")\n",
        "\n",
        "        # Print average loss for the epoch\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # Save checkpoint\n",
        "        checkpoint_path = os.path.join(checkpoint_dir, f\"mobileclip_finetuned_epoch{epoch+1}_last.pt\")\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': avg_loss,\n",
        "        }, checkpoint_path)\n",
        "        print(f\"Checkpoint saved: {checkpoint_path}\")\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7e5d0e0",
      "metadata": {
        "id": "d7e5d0e0"
      },
      "source": [
        "## Running the training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd246bbd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd246bbd",
        "outputId": "f44441e3-301b-47cd-a21b-36ada27ba889"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 1074 samples from /content/LightVision/data/all_captions.json.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/1:   0%|          | 0/8 [00:00<?, ?it/s]<ipython-input-16-d5a1d3a0fd7e>:65: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "Epoch 1/1: 100%|██████████| 8/8 [00:10<00:00,  1.29s/it, loss=1.0873]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, Loss: 1.0873\n",
            "Checkpoint saved: checkpoints/mobileclip_finetuned_epoch1_last.pt\n",
            "Loaded 5840 samples from /content/LightVision/data/new_file.json.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1: 100%|██████████| 45/45 [00:57<00:00,  1.29s/it, loss=1.0028]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, Loss: 1.0028\n",
            "Checkpoint saved: checkpoints/mobileclip_finetuned_epoch1_last.pt\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CLIP(\n",
              "  (image_encoder): MCi(\n",
              "    (model): FastViT(\n",
              "      (patch_embed): Sequential(\n",
              "        (0): MobileOneBlock(\n",
              "          (se): Identity()\n",
              "          (activation): GELU(approximate='none')\n",
              "          (reparam_conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "        )\n",
              "        (1): MobileOneBlock(\n",
              "          (se): Identity()\n",
              "          (activation): GELU(approximate='none')\n",
              "          (reparam_conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64)\n",
              "        )\n",
              "        (2): MobileOneBlock(\n",
              "          (se): Identity()\n",
              "          (activation): GELU(approximate='none')\n",
              "          (reparam_conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (network): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): RepMixerBlock(\n",
              "            (token_mixer): RepMixer(\n",
              "              (reparam_conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
              "            )\n",
              "            (convffn): ConvFFN(\n",
              "              (conv): Sequential(\n",
              "                (conv): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=64, bias=False)\n",
              "                (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "              (fc1): Conv2d(64, 192, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "          (1): RepMixerBlock(\n",
              "            (token_mixer): RepMixer(\n",
              "              (reparam_conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
              "            )\n",
              "            (convffn): ConvFFN(\n",
              "              (conv): Sequential(\n",
              "                (conv): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=64, bias=False)\n",
              "                (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "              (fc1): Conv2d(64, 192, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "        )\n",
              "        (1): PatchEmbed(\n",
              "          (proj): Sequential(\n",
              "            (0): ReparamLargeKernelConv(\n",
              "              (activation): GELU(approximate='none')\n",
              "              (se): Identity()\n",
              "              (lkb_reparam): Conv2d(64, 128, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=64)\n",
              "            )\n",
              "            (1): MobileOneBlock(\n",
              "              (se): Identity()\n",
              "              (activation): GELU(approximate='none')\n",
              "              (reparam_conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (2): Sequential(\n",
              "          (0): RepMixerBlock(\n",
              "            (token_mixer): RepMixer(\n",
              "              (reparam_conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
              "            )\n",
              "            (convffn): ConvFFN(\n",
              "              (conv): Sequential(\n",
              "                (conv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128, bias=False)\n",
              "                (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "              (fc1): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "          (1): RepMixerBlock(\n",
              "            (token_mixer): RepMixer(\n",
              "              (reparam_conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
              "            )\n",
              "            (convffn): ConvFFN(\n",
              "              (conv): Sequential(\n",
              "                (conv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128, bias=False)\n",
              "                (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "              (fc1): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "          (2): RepMixerBlock(\n",
              "            (token_mixer): RepMixer(\n",
              "              (reparam_conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
              "            )\n",
              "            (convffn): ConvFFN(\n",
              "              (conv): Sequential(\n",
              "                (conv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128, bias=False)\n",
              "                (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "              (fc1): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "          (3): RepMixerBlock(\n",
              "            (token_mixer): RepMixer(\n",
              "              (reparam_conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
              "            )\n",
              "            (convffn): ConvFFN(\n",
              "              (conv): Sequential(\n",
              "                (conv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128, bias=False)\n",
              "                (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "              (fc1): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "          (4): RepMixerBlock(\n",
              "            (token_mixer): RepMixer(\n",
              "              (reparam_conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
              "            )\n",
              "            (convffn): ConvFFN(\n",
              "              (conv): Sequential(\n",
              "                (conv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128, bias=False)\n",
              "                (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "              (fc1): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "          (5): RepMixerBlock(\n",
              "            (token_mixer): RepMixer(\n",
              "              (reparam_conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
              "            )\n",
              "            (convffn): ConvFFN(\n",
              "              (conv): Sequential(\n",
              "                (conv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128, bias=False)\n",
              "                (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "              (fc1): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "        )\n",
              "        (3): PatchEmbed(\n",
              "          (proj): Sequential(\n",
              "            (0): ReparamLargeKernelConv(\n",
              "              (activation): GELU(approximate='none')\n",
              "              (se): SEModule(\n",
              "                (fc1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "                (bn): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "                (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "                (gate): Sigmoid()\n",
              "              )\n",
              "              (lkb_reparam): Conv2d(128, 256, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=128)\n",
              "            )\n",
              "            (1): MobileOneBlock(\n",
              "              (se): Identity()\n",
              "              (activation): GELU(approximate='none')\n",
              "              (reparam_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (4): Sequential(\n",
              "          (0): RepMixerBlock(\n",
              "            (token_mixer): RepMixer(\n",
              "              (reparam_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
              "            )\n",
              "            (convffn): ConvFFN(\n",
              "              (conv): Sequential(\n",
              "                (conv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256, bias=False)\n",
              "                (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "              (fc1): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "          (1): RepMixerBlock(\n",
              "            (token_mixer): RepMixer(\n",
              "              (reparam_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
              "            )\n",
              "            (convffn): ConvFFN(\n",
              "              (conv): Sequential(\n",
              "                (conv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256, bias=False)\n",
              "                (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "              (fc1): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "          (2): RepMixerBlock(\n",
              "            (token_mixer): RepMixer(\n",
              "              (reparam_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
              "            )\n",
              "            (convffn): ConvFFN(\n",
              "              (conv): Sequential(\n",
              "                (conv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256, bias=False)\n",
              "                (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "              (fc1): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "          (3): RepMixerBlock(\n",
              "            (token_mixer): RepMixer(\n",
              "              (reparam_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
              "            )\n",
              "            (convffn): ConvFFN(\n",
              "              (conv): Sequential(\n",
              "                (conv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256, bias=False)\n",
              "                (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "              (fc1): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "          (4): RepMixerBlock(\n",
              "            (token_mixer): RepMixer(\n",
              "              (reparam_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
              "            )\n",
              "            (convffn): ConvFFN(\n",
              "              (conv): Sequential(\n",
              "                (conv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256, bias=False)\n",
              "                (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "              (fc1): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "          (5): RepMixerBlock(\n",
              "            (token_mixer): RepMixer(\n",
              "              (reparam_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
              "            )\n",
              "            (convffn): ConvFFN(\n",
              "              (conv): Sequential(\n",
              "                (conv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256, bias=False)\n",
              "                (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "              (fc1): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "          (6): RepMixerBlock(\n",
              "            (token_mixer): RepMixer(\n",
              "              (reparam_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
              "            )\n",
              "            (convffn): ConvFFN(\n",
              "              (conv): Sequential(\n",
              "                (conv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256, bias=False)\n",
              "                (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "              (fc1): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "          (7): RepMixerBlock(\n",
              "            (token_mixer): RepMixer(\n",
              "              (reparam_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
              "            )\n",
              "            (convffn): ConvFFN(\n",
              "              (conv): Sequential(\n",
              "                (conv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256, bias=False)\n",
              "                (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "              (fc1): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "          (8): RepMixerBlock(\n",
              "            (token_mixer): RepMixer(\n",
              "              (reparam_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
              "            )\n",
              "            (convffn): ConvFFN(\n",
              "              (conv): Sequential(\n",
              "                (conv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256, bias=False)\n",
              "                (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "              (fc1): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "          (9): RepMixerBlock(\n",
              "            (token_mixer): RepMixer(\n",
              "              (reparam_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
              "            )\n",
              "            (convffn): ConvFFN(\n",
              "              (conv): Sequential(\n",
              "                (conv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256, bias=False)\n",
              "                (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "              (fc1): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "        )\n",
              "        (5): PatchEmbed(\n",
              "          (proj): Sequential(\n",
              "            (0): ReparamLargeKernelConv(\n",
              "              (activation): GELU(approximate='none')\n",
              "              (se): SEModule(\n",
              "                (fc1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "                (bn): Identity()\n",
              "                (act): ReLU(inplace=True)\n",
              "                (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "                (gate): Sigmoid()\n",
              "              )\n",
              "              (lkb_reparam): Conv2d(256, 512, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=256)\n",
              "            )\n",
              "            (1): MobileOneBlock(\n",
              "              (se): Identity()\n",
              "              (activation): GELU(approximate='none')\n",
              "              (reparam_conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (6): RepCPE(\n",
              "          (reparam_conv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "        )\n",
              "        (7): Sequential(\n",
              "          (0): AttentionBlock(\n",
              "            (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (token_mixer): MHSA(\n",
              "              (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (convffn): ConvFFN(\n",
              "              (conv): Sequential(\n",
              "                (conv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512, bias=False)\n",
              "                (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "              (fc1): Conv2d(512, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Conv2d(1536, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "          (1): AttentionBlock(\n",
              "            (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (token_mixer): MHSA(\n",
              "              (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (convffn): ConvFFN(\n",
              "              (conv): Sequential(\n",
              "                (conv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512, bias=False)\n",
              "                (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "              (fc1): Conv2d(512, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Conv2d(1536, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (conv_exp): MobileOneBlock(\n",
              "        (se): SEBlock(\n",
              "          (reduce): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (expand): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (activation): GELU(approximate='none')\n",
              "        (reparam_conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
              "      )\n",
              "      (head): GlobalPool2D(\n",
              "        (pool): GlobalPool()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (text_encoder): TextTransformer(\n",
              "    (embedding_layer): Embedding(49408, 512)\n",
              "    (positional_embedding): LearnablePositionalEmbedding(num_embeddings=77, embedding_dim=512, padding_idx=None)\n",
              "    (embedding_dropout): Dropout(p=0.0, inplace=False)\n",
              "    (transformer): ModuleList(\n",
              "      (0): RepMixerBlock(\n",
              "        (token_mixer): RepMixer(\n",
              "          (reparam_conv): Conv2d(512, 512, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5), groups=512)\n",
              "        )\n",
              "        (convffn): ConvFFN(\n",
              "          (conv): Sequential(\n",
              "            (conv): Conv2d(512, 512, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5), groups=512, bias=False)\n",
              "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (fc1): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act): GELU(approximate='none')\n",
              "          (fc2): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (1-4): 4 x TransformerEncoder(embed_dim=512, ffn_dim=2048, dropout=0.0, ffn_dropout=0.0, stochastic_dropout=0.0, attn_fn=MultiHeadAttention, act_fn=GELU, norm_fn=layer_norm_fp32)\n",
              "      (5): RepMixerBlock(\n",
              "        (token_mixer): RepMixer(\n",
              "          (reparam_conv): Conv2d(512, 512, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5), groups=512)\n",
              "        )\n",
              "        (convffn): ConvFFN(\n",
              "          (conv): Sequential(\n",
              "            (conv): Conv2d(512, 512, kernel_size=(1, 11), stride=(1, 1), padding=(0, 5), groups=512, bias=False)\n",
              "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (fc1): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act): GELU(approximate='none')\n",
              "          (fc2): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): LayerNormFP32((512,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "images_dir = os.path.join(BASE_DATA_DESTINATION, FLICKR8K_IMAGES_FOLDER_NAME)\n",
        "captions_file = os.path.join(BASE_DATA_DESTINATION, CAPTIONS_JSON_FILENAME)\n",
        "checkpoint_dir = \"checkpoints\"\n",
        "\"\"\"\n",
        "# Train the model\n",
        "trained_model = train_model(\n",
        "    images_dir=images_dir,\n",
        "    captions_file=\"/content/LightVision/data/captions.txt\",\n",
        "    checkpoint_dir=checkpoint_dir,\n",
        "    device=DEVICE,\n",
        "    batch_size=256,\n",
        "    learning_rate=1e-4,\n",
        "    num_epochs=0,\n",
        "    num_workers=0,\n",
        "    pull_from_json=False,\n",
        "    long_clip_loss_fn=long_clip_loss,\n",
        "    single_loss_fn=single_loss\n",
        ")\n",
        "\"\"\"\n",
        "# Train the model\n",
        "trained_model = train_model(\n",
        "    images_dir=images_dir,\n",
        "    captions_file=\"/content/LightVision/data/all_captions.json\",\n",
        "    checkpoint_dir=checkpoint_dir,\n",
        "    device=DEVICE,\n",
        "    batch_size=128,\n",
        "    learning_rate=1e-4,\n",
        "    num_epochs=1,\n",
        "    num_workers=0,\n",
        "    pull_from_json=True,\n",
        "    long_clip_loss_fn=long_clip_loss,\n",
        "    single_loss_fn=single_loss\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trained_model = train_model(\n",
        "    images_dir=images_dir,\n",
        "    captions_file=\"/content/LightVision/data/new_file.json\",\n",
        "    checkpoint_dir=checkpoint_dir,\n",
        "    device=DEVICE,\n",
        "    batch_size=128,\n",
        "    learning_rate=1e-4,\n",
        "    num_epochs=1,\n",
        "    num_workers=0,\n",
        "    pull_from_json=True,\n",
        "    long_clip_loss_fn=long_clip_loss,\n",
        "    single_loss_fn=single_loss\n",
        ")\n",
        "\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c49b57a8",
      "metadata": {
        "id": "c49b57a8"
      },
      "source": [
        "Load the trained checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af73e6a5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af73e6a5",
        "outputId": "dbb41215-b8a5-4c47-866e-926ab0314d27"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "# If not trained, load the trained checkpoint /content/checkpoints/mobileclip_finetuned_epoch4.pt\n",
        "checkpoint_path = os.path.join(CHECKPOINT_DIR, 'mobileclip_finetuned_epoch1_last.pt')\n",
        "checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
        "\n",
        "model.load_state_dict(checkpoint['model_state_dict'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97b7e2b7",
      "metadata": {
        "id": "97b7e2b7"
      },
      "source": [
        "# Evaluate the model with short captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2832abfe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "2832abfe",
        "outputId": "1aba169f-a281-4244-f55d-6f0d6a065966"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label probabilities after training: tensor([[    0.0001,     0.0145,     0.0701,     0.4362,     0.4791]],\n",
            "       device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-27-2e1db02336be>:13: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.no_grad(), torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[0.5692, 0.4296, 0.0012]]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "# Example evaluation\n",
        "test_image_path = \"/content/ChatGPT Image 11 May 2025 01_24_53.png\"\n",
        "test_texts = [\"A man.\"\n",
        ", \"A man sitting on a bench.\"\n",
        ", \"A man sitting on a red bench in a park.\"\n",
        ", \"A man sitting on a red bench in a park holding a yellow umbrella.\"\n",
        ", \"A man sitting on a red bench in a park holding a yellow umbrella while feeding pigeons.\"]\n",
        "\n",
        "\n",
        "test_image = preprocess(Image.open(test_image_path).convert('RGB')).unsqueeze(0).to(DEVICE)\n",
        "test_text = tokenizer(test_texts).to(DEVICE)\n",
        "\n",
        "with torch.no_grad(), torch.cuda.amp.autocast():\n",
        "    image_features = model.encode_image(test_image)\n",
        "    text_features = model.encode_text(test_text)\n",
        "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "\n",
        "torch.set_printoptions(sci_mode=False, precision=4)\n",
        "print(\"Label probabilities after training:\", text_probs)\n",
        "\n",
        "\"[0.5692, 0.4296, 0.0012]]\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1K29WbRsmsG",
      "metadata": {
        "id": "d1K29WbRsmsG"
      },
      "source": [
        "Base: [[0.9789,0.0004,  0.0206]]\n",
        "pos embed: [[0.4075, 0.3124, 0.2801]\n",
        "after: ([[0.7447, 0.1197, 0.1356]]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69ad6790",
      "metadata": {
        "id": "69ad6790"
      },
      "source": [
        "# Evalute the model with long captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Mks38R3znFxs",
      "metadata": {
        "id": "Mks38R3znFxs"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def remove_first_n_words(text, n=3):\n",
        "    return ' '.join(text.split()[n:])\n",
        "\n",
        "with open(\"/content/captions_database-10.json\", \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "for v in data.values():\n",
        "    v[\"long_caption\"] = remove_first_n_words(v.pop(\"long_detailed\"))\n",
        "\n",
        "with open(\"new_file.json\", \"w\") as f:\n",
        "    json.dump(data, f, indent=2)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}